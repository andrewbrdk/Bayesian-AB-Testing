{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059d7c01",
   "metadata": {},
   "source": [
    "# Bayesian A/B Testing: Connection to $p$-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4a7a6",
   "metadata": {},
   "source": [
    "*The $p$-values from the $t$-test, $\\chi^2$-test, and Mann-Whitney $U$ test in A/B experiments are numerically close to the Bayesian best group probabilities. This relation holds despite differences in the underlying definitions.*\n",
    "\n",
    "*- [$P$-Values](#$P$-Values)*  \n",
    "*- [$T$-Test](#$T$-Test)*  \n",
    "*- [$\\chi^2$-Test](#$\\chi^2$-Test)*  \n",
    "*- [Mann-Whitney $U$ Test](#Mann-Whitney-$U$-Test)*  \n",
    "*- [References](#References)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6b5af",
   "metadata": {},
   "source": [
    "## $P$-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a519a4",
   "metadata": {},
   "source": [
    "$P$-values are used in null hypothesis significance testing [[HT](https://en.wikipedia.org/wiki/Statistical_hypothesis_test)]. A null hypothesis $H_0$ is specified, and a statistical test $T$—a random variable with known distribution $P_T(x \\mid H_0)$ under $H_0$—is selected. The observed value $x_0$ is the test statistic [[TestStat](https://en.wikipedia.org/wiki/Test_statistic)]. The one-sided $p$-value is defined as $p = P_T(x \\ge x_0 \\mid H_0)$ [[PVal](https://en.wikipedia.org/wiki/P-value), [TailedTests](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests)]. Small $p$-values lead to rejection of $H_0$; otherwise, it is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d41335",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../figs/null_hypothesis.png\" alt=\"null_hypothesis\" width=\"800\"/>\n",
    "<em> Under the null hypothesis $H_0$ the test statistic follows the distribution $P_T(x \\mid H_0)$, and the $p$-value is $p = P_T(x \\ge x_0 \\mid H_0)$. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46499a53",
   "metadata": {},
   "source": [
    "Decisions regarding the null hypothesis $H_0$ are based on the $p$-value, $p = P_T(x \\ge x_0 \\mid H_0)$, whereas the probability of the hypothesis given the observed data is expressed as $P(H_0 \\mid x_0)$. According to Bayes’ theorem, $P(H_0 \\mid x_0) \\propto P_T(x = x_0 \\mid H_0) P(H_0)$. In other words, selecting a hypothesis requires computing the likelihood of the observed data under competing hypotheses and comparing these values while accounting for prior probabilities.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "p = P_{T}(x \\ge x_{0} | H_0)\n",
    "\\\\\n",
    "\\,\n",
    "\\\\\n",
    "P(H_0 | x_0) = \\frac{P_{T}(x = x_{0} | H_0) P(H_0)}{P_{T}(x = x_{0} | H_0) P(H_0) + P_{T}(x = x_{0} | {\\sim}H_0) P({\\sim}H_0)}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cba49",
   "metadata": {},
   "source": [
    "In A/B testing, the most commonly used methods are the $t$-test for means, the $\\chi^2$-test for proportions, and the Mann-Whitney $U$ test. It is shown below that, under certain conditions, the $p$-values of these tests are numerically close to the Bayesian probabilities that the parameter of one group exceeds that of the other. These relationships hold despite differences in the underlying definitions of the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e63c00",
   "metadata": {},
   "source": [
    "## $T$-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a120f",
   "metadata": {},
   "source": [
    "Means are compared using $t$-tests [[TTest](https://en.wikipedia.org/wiki/Student%27s_t-test)]. Consider two samples of sizes $N_A$ and $N_B$ drawn from random variables $A$ and $B$. Under the null hypothesis of equal population means, $H_0: E[A] = E[B]$, the ratio of the difference in sample means to its standard error, $X = \\overline{\\Delta} / s_{\\Delta}$, is expected to follow a $t$-distribution [[WelchT](https://en.wikipedia.org/wiki/Welch%27s_t-test)]. For sufficiently large sample sizes, this distribution approaches the standard normal, $\\text{Norm}(0,1)$ [[TDist](https://en.wikipedia.org/wiki/Student%27s_t-distribution)]. From the observed data, the test statistic is $x_0 = \\overline{\\Delta} / s_{\\Delta}$, and the probability of obtaining a value equal to or greater than $x_0$ defines the one-sided $p$-value, $P_X(x \\ge x_0 \\mid H_0)$. If this $p$-value is below the chosen significance level, the means of the two groups are considered significantly different.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\overline{A} = \\frac{1}{N_{A}} \\sum_{i=1}^{N_{A}} A_i,\n",
    "\\quad\n",
    "s_A^2 = \\frac{1}{N_A} \\sum_{i=1}^{N_A} (A_i - \\overline{A})^2,\n",
    "\\quad\n",
    "\\text{similar for } B\n",
    "\\\\\n",
    "X = \\frac{\\overline{\\Delta}}{s_{\\Delta}},\n",
    "\\quad\n",
    "\\overline{\\Delta} = \\overline{B} - \\overline{A},\n",
    "\\quad\n",
    "s^2_{\\Delta} = \\frac{s_A^2}{N_A} + \\frac{s_B^2}{N_B}\n",
    "\\\\\n",
    "H_0: E[A] = E[B],\n",
    "\\quad\n",
    "P_{X}(x | H_0) \\approx \\text{Norm}(x; 0, 1)\n",
    "\\\\\n",
    "x_0 - \\text{realized value of } X, \\quad\n",
    "p = P_{X}(x \\ge x_0 | H_0)\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e3130",
   "metadata": {},
   "source": [
    "The A/B testing goal is to select the group with the higher mean. Instead of the $p$-value $P_X(x \\ge x_0 \\mid H_0)$, the quantity of interest is the probability that the mean of group $B$ exceeds that of group $A$ given the observed data $P(\\mu_B > \\mu_A \\mid A_i, B_j) = P(\\mu_\\Delta > 0 \\mid A_i, B_j)$, where $\\mu_A$, $\\mu_B$, and $\\mu_\\Delta$ denote the true means of the corresponding distributions. This probability can be estimated using Bayesian modeling by constructing a posterior distribution for the mean difference $\\mu_\\Delta$. By the central limit theorem, the distribution of sample means is approximately normal. Therefore, the likelihood is chosen as a normal distribution, $P(\\overline{\\Delta} \\mid \\mu_\\Delta) = \\text{Norm}(\\overline{\\Delta} \\mid \\mu_\\Delta, s_\\Delta^2)$, where $s_\\Delta^2$ is the variance estimated from the data. The model contains a single random parameter, the mean difference $\\mu_\\Delta$. Modeling is applied to the sample means rather than the raw observations. Thus, parameter updates are based on a single observed value, $\\overline{\\Delta}$. For conjugacy, the prior distribution is also chosen to be normal, $P(\\mu_\\Delta) = \\text{Norm}(\\mu_\\Delta \\mid \\mu_0, \\sigma_0^2)$. The posterior distribution is then normal with updated mean and variance, $P(\\mu_\\Delta \\mid \\overline{\\Delta}) = \\text{Norm}(\\mu_\\Delta \\mid \\mu_N, \\sigma_N^2)$ [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution)]. For a sufficiently broad prior centered at zero, $\\mu_0 = 0$ and $\\sigma_0^2 \\gg s_\\Delta^2$, the posterior distribution approximately $P(\\mu_\\Delta \\mid \\overline{\\Delta}) \\approx \\text{Norm}(\\mu_\\Delta \\mid \\overline{\\Delta}, s_\\Delta^2)$. The probability that the mean of one group exceeds the other can then be expressed as the upper tail of a normal distribution with mean $x_0 = \\overline{\\Delta}/s_\\Delta$ and unit variance: $P(\\mu_B > \\mu_A \\mid A_i, B_j) = P(\\mu_\\Delta > 0 \\mid \\overline{\\Delta}) \\approx P(\\text{Norm}(x > 0 \\mid x_0, 1))$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\overline{\\Delta} | \\mu_{\\Delta}) & =\n",
    "\\text{Norm}(\\overline{\\Delta} | \\mu_{\\Delta}, s_{\\Delta}^2),\n",
    "\\quad\n",
    "P(\\mu_{\\Delta}) =\n",
    "\\text{Norm}(\\mu_{\\Delta} | \\mu_0, \\sigma_0^2) \n",
    "\\\\\n",
    "P(\\mu_{\\Delta} | \\overline{\\Delta}) \n",
    "& = \\text{Norm}(\\mu_{\\Delta} | \\mu_{N}, \\sigma_{N}^2),\n",
    "\\quad\n",
    "\\sigma_{N}^2 = \\frac{\\sigma_{0}^2 s_{\\Delta}^2}{s_{\\Delta}^2 + \\sigma_{0}^2},\n",
    "\\quad\n",
    "\\mu_{N} = \\mu_{0} \\frac{\\sigma_{N}^2}{\\sigma_{0}^2} + \\frac{\\sigma_{N}^2}{s_{\\Delta}^2} \\overline{\\Delta}\n",
    "\\\\\n",
    "\\mu_0 = 0, & \\, \\sigma_{0}^2 \\gg s^2_{\\Delta}: \n",
    "\\, \n",
    "\\sigma_N^2 \\approx s^2_{\\Delta}, \\, \\mu_N \\approx \\overline{\\Delta}, \n",
    "\\\\\n",
    "P(\\mu_{\\Delta} | \\overline{\\Delta}) & \\approx \n",
    "\\text{Norm}(\\mu_{\\Delta} | \\overline{\\Delta}, s^2_{\\Delta})\n",
    "\\\\\n",
    "P(\\mu_B > \\mu_A | A_i, B_j ) &= P(\\mu_{\\Delta} > 0 | \\overline{\\Delta})  \\approx P(\\text{Norm}(\\mu_{\\Delta} > 0 | \\overline{\\Delta}, s^2_{\\Delta})) = P(\\text{Norm}(x > 0 | x_0, 1))\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04425d",
   "metadata": {},
   "source": [
    "By the symmetry of the normal distribution, $P(\\text{Norm}(x > x_0 \\mid 0, 1)) = P(\\text{Norm}(x < 0 \\mid x_0, 1))$. Therefore, the one-sided $p$-value from a $t$-test is close to the probability that the mean of one group exceeds that of the other.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p = P_{X}(x > x_0 | H_0)\n",
    "& = P \\left( \\text{Norm}(x > x_0| 0, 1) \\right)\n",
    "\\\\\n",
    "& =  P \\left( \\text{Norm}(x < 0 | x_0, 1) \\right)\n",
    "\\\\\n",
    "& = 1 - P \\left( \\text{Norm}(x > 0 | x_0, 1) \\right)\n",
    "\\approx 1 - P(\\mu_B > \\mu_A | A_i, B_j )\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d8603",
   "metadata": {},
   "source": [
    "To illustrate, consider two normal distributions with different means. For a given sample, the Bayesian estimate of the probability $P(\\mu_B > \\mu_A \\mid A_i, B_j)$ is compared with the $p$-value from a $t$-test. A one-sided $t$-test with unequal variances (`equal_var=False`, `alternative`) is used [[ScipyTTestInd](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)]. The probability $p = P(x > x_0 \\mid H_0) = P(\\text{Norm}(x > x_0 \\mid 0, 1))$ is shaded dark, while $P(\\text{Norm}(x < 0 \\mid x_0, 1)) \\approx 1 - P(\\mu_B > \\mu_A \\mid A_i, B_j)$ is shaded light. By the properties of the normal distribution, the areas of these regions are equal. Consequently, the $p$-value is numerically close to the Bayesian estimate of the probability. It should be noted, however, that the two are not equivalent—they are defined differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_diff_scaled(sampA, sampB, mu0=None, s20=None):\n",
    "    delta = sampB.mean() - sampA.mean()\n",
    "    s2delta = sampA.var() / sampA.size + sampB.var() / sampB.size\n",
    "    mu0 = mu0 or 0\n",
    "    s20 = s20 or 30 * s2delta\n",
    "    s2n = s2delta * s20 / (s2delta + s20)\n",
    "    mun = mu0 * s2n / s20 + delta * s2n / s2delta\n",
    "    return stats.norm(loc=mun/np.sqrt(s2n), scale=1)\n",
    "\n",
    "muA = 0.1\n",
    "muB = 0.115\n",
    "sigma = 1.3\n",
    "\n",
    "exactA = stats.norm(muA, sigma)\n",
    "exactB = stats.norm(muB, sigma)\n",
    "\n",
    "N = 30000\n",
    "sampA = exactA.rvs(size=N)\n",
    "sampB = exactB.rvs(size=N)\n",
    "\n",
    "a = 'greater' if np.mean(sampA) > np.mean(sampB) else 'less'\n",
    "t_stat, p_value = stats.ttest_ind(sampA, sampB, equal_var=False, alternative=a)\n",
    "t_stat = np.abs(t_stat)\n",
    "\n",
    "post_dist = posterior_diff_scaled(sampA, sampB)\n",
    "mean_b_gt_a = 1 - post_dist.cdf(0)\n",
    "\n",
    "xaxis_min = -7\n",
    "xaxis_max = 8\n",
    "x = np.linspace(xaxis_min, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.norm.pdf(x, loc=0, scale=1), \n",
    "                         line_color='black', opacity=0.8, name='$\\mathrm{Norm}(0, 1)$'))\n",
    "fig.add_trace(go.Scatter(x=x[x>t_stat], y=stats.norm.pdf(x[x>t_stat], loc=0, scale=1), \n",
    "                         line_color=\"rgba(0, 0, 0, 0.7)\", name='$P(x>x_0 | H_0)$', fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.7)\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_dist.pdf(x), \n",
    "                         line_color='black', opacity=0.2, name='$\\mathrm{Norm}(x_0, 1)$'))\n",
    "fig.add_trace(go.Scatter(x=x[x<0], y=post_dist.pdf(x[x<0]), \n",
    "                         line_color=\"rgba(128, 128, 128, 0.2)\", name='$P(\\mu_{\\Delta} < 0 | \\overline{\\Delta})$', fill=\"tozeroy\", fillcolor=\"rgba(128, 128, 128, 0.2)\"))\n",
    "fig.add_trace(go.Scatter(x=[0, 0], y=[0, max(stats.norm.pdf(x, loc=0, scale=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '0'], textposition=\"top center\", \n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[t_stat, t_stat], y=[0, max(stats.norm.pdf(x, loc=0, scale=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$x_0$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.update_layout(title=r'$T\\text{-распределение и апостериорное среднего разности}$',\n",
    "                  xaxis_title='$x$',\n",
    "                  yaxis_title='Плотность вероятности',\n",
    "                  xaxis_range=[xaxis_min, xaxis_max],\n",
    "                  hovermode=\"x\",\n",
    "                  template=\"plotly_white\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "\n",
    "print(f'p-value P(x>x0 | H0): {p_value:.4f}')\n",
    "print(f'1 - p: {1 - p_value:.4f}')\n",
    "print(f'Bayes P(mu_delta > 0): {mean_b_gt_a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c419d18",
   "metadata": {},
   "source": [
    "The numerical closeness between the $p$-value and the Bayesian probability $P(\\mu_\\Delta > 0 \\mid \\overline{\\Delta})$ can be assessed by the number of correctly identified superior groups across a series of experiments. In each experiment, two normal distributions are defined. For group $A$, the mean is fixed at `mu = 0.1`, while for group $B$, the mean is randomly chosen within $\\pm 5\\%$ of `mu`. Data points are added to each group in increments of `n_samp_step`. At each step, a $t$-test is performed. The experiment stops either when $1 - p$ reaches `prob_stop = 0.95` or when the maximum number of points `n_samp_max` has been used. The experiment duration is not fixed in advance, and the minimum sample size is `n_samp_min + n_samp_step`. Upon stopping, the Bayesian posterior distribution is computed, and the probability $P(\\mu_\\Delta > 0 \\mid \\overline{\\Delta})$ is compared with the $p$-value. This procedure is repeated `nexps` times. The proportion of correctly identified superior groups across all experiments is recorded. Out of `nexps = 1000` experiments, 880 were completed, with 818 correctly identified cases, yielding an accuracy of `0.93`, which is close to `prob_stop = 0.95`. In each experiment, the $p$-values are close to the Bayesian probabilities. If the minimum sample size `n_samp_min` is too small, the proportion of correctly identified groups falls below `prob_stop`. For the Bayesian model, instead of specifying a minimum sample size, a smaller prior variance $\\sigma_0^2$ can be used—the effect is similar to setting a minimum number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best_bayes', '1-pval'])\n",
    "\n",
    "mu = 0.1\n",
    "nexps = 1000\n",
    "cmp['A'] = [mu] * nexps\n",
    "cmp['B'] = mu * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 3_000_000\n",
    "n_samp_step = 10_000\n",
    "n_samp_min = 100_000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    muA = cmp.at[i, 'A']\n",
    "    muB = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.norm(loc=muA, scale=1)\n",
    "    exact_dist_B = stats.norm(loc=muB, scale=1)\n",
    "    n_samp_current = n_samp_min\n",
    "    sampA = exact_dist_A.rvs(n_samp_max)\n",
    "    sampB = exact_dist_B.rvs(n_samp_max)\n",
    "    post_dist = None\n",
    "    mean_b_gt_a_bayes = np.nan\n",
    "    while n_samp_current < n_samp_max:\n",
    "        n_samp_current += n_samp_step\n",
    "        a = 'greater' if np.mean(sampA[:n_samp_current]) > np.mean(sampB[:n_samp_current]) else 'less'\n",
    "        t_stat, p_value = stats.ttest_ind(sampA[:n_samp_current], sampB[:n_samp_current], equal_var=False, alternative=a)\n",
    "        p_best_t = 1 - p_value\n",
    "        best_gr = 'A' if p_best_t >= prob_stop and a == 'greater' else 'B' if p_best_t >= prob_stop and a == 'less' else None\n",
    "        if best_gr:\n",
    "            post_dist = posterior_diff_scaled(sampA[:n_samp_current], sampB[:n_samp_current])\n",
    "            mean_b_gt_a_bayes = 1 - post_dist.cdf(0)\n",
    "            cmp.at[i, 'A_exp'] = sampA[:n_samp_current].mean()\n",
    "            cmp.at[i, 'B_exp'] = sampB[:n_samp_current].mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_current\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best_bayes'] = max(mean_b_gt_a_bayes, 1 - mean_b_gt_a_bayes)\n",
    "            cmp.at[i, '1-pval'] = 1 - p_value\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_current}, best_gr {best_gr}, Bayes P(b>a) {mean_b_gt_a_bayes:.4f}, T-test p-val {p_value:.4f}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(30))\n",
    "finished = np.sum(cmp['best_exp'].notna())\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Finished: {finished}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / finished}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3ec00",
   "metadata": {},
   "source": [
    "## $\\chi^2$-Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06de92d",
   "metadata": {},
   "source": [
    "Conversion rates are commonly compared using the $\\chi^2$ test [[Chi2Test](https://en.wikipedia.org/wiki/Chi-squared_test)]. Pearson’s $\\chi^2$ statistic for multinomial distributions is defined as $\\chi^2 = \\sum_{i=1}^k (S_i - N p_i)^2 / N p_i$, where $N$ is the total number of observations, and $S_i$ and $N p_i$ are the observed and expected counts in category $i$ with proportion $p_i$ [[Chi2Pearson](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)]. For a binomial distribution, the statistic reduces to $\\chi^2 = (S - N p)^2 / N p (1 - p)$. By the central limit theorem, $(S - N p) / \\sqrt{N p (1 - p)}$ converges to a standard normal distribution. The sum of squares of $k$ independent standard normal variables defines the $\\chi^2$-distribution with $k$ degrees of freedom, $\\chi_k^2 = \\sum_{i=1}^k X_i^2$, where $X_i \\sim \\text{Norm}(0,1)$ [[Chi2Dist](https://en.wikipedia.org/wiki/Chi-squared_distribution)]. The $\\chi^2$ statistic converges to a $\\chi_1^2$ distribution.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\chi^2 & = \n",
    "\\sum_{i=1}^k \\frac{(S_i - Np_i)^2}{N p_i}\n",
    "\\\\\n",
    "& =\n",
    "\\frac{(S - N p)^2}{N p}\n",
    "+\n",
    "\\frac{((N - S) - N (1-p))^2}{N (1-p)}\n",
    "\\\\\n",
    "& =\n",
    "\\frac{(S - Np)^2}{N p (1-p)} \n",
    "\\to \\chi_1^2, \\quad N \\to \\infty\n",
    "\\\\\n",
    "\\chi^2_k & = \\sum_{i=1}^{k} X_i^2,\\, X_i \\sim \\text{Norm}(0,1)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b9d00",
   "metadata": {},
   "source": [
    "For a two groups conversion A/B test under the null hypothesis of equal expected conversion rates $p = (S_A + S_B)/(N_A + N_B)$ the test statistic is $\\chi^2 = \\Delta p^2 / s_\\Delta^2$, where $\\Delta p = S_B / N_B - S_A / N_A$ and $s_\\Delta^2 = p(1 - p)(1/N_A + 1/N_B)$. The distribution of $\\chi^2$ approaches a $\\chi_1^2$ distribution for sufficiently large sample sizes. Since the $\\chi_1^2$ distribution is obtained by squaring a standard normal variable, the $p$-value region $p = P_{\\chi_1^2}(x > \\chi^2 \\mid H_0)$ corresponds to the region $P(\\text{Norm}(x > \\chi \\cup x < -\\chi \\mid 0, 1))$. The areas of the regions $x > \\chi$ and $x < -\\chi$ are equal by symmetry of the normal distribution.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "A \\sim \\mathrm{Bernoulli}(p_A), \n",
    "\\,\n",
    "S_A = \\sum_{i=1}^{N_A} A_i,\n",
    "\\quad\n",
    "B \\sim \\mathrm{Bernoulli}(p_B), \n",
    "\\,\n",
    "S_B = \\sum_{i=1}^{N_B} B_i,\n",
    "\\\\\n",
    "p = \\frac{S_A + S_B}{N_A + N_B},\n",
    "\\quad\n",
    "\\Delta p = \\frac{S_B}{N_B} - \\frac{S_A}{N_A},\n",
    "\\quad\n",
    "s_{\\Delta}^2 = \\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_B}\n",
    "\\\\\n",
    "\\begin{split}\n",
    "H_0: E[A] = E[B], \\quad \\chi^2 & = \\frac{(S_A - N_A p)^2}{N_A p (1-p)} + \\frac{(S_B - N_B p)^2}{N_B p (1-p)} \n",
    "\\\\\n",
    "& = \\frac{N_A N_B \\Delta p^2}{(N_A + N_B) p (1-p)}\n",
    "\\\\\n",
    "& = \\frac{\\Delta p^2}{s_{\\Delta}^2} \\to \\chi_1^2, \\, n \\to \\infty\n",
    "\\end{split}\n",
    "\\\\\n",
    "\\,\n",
    "\\\\\n",
    "\\begin{split}\n",
    "\\text{p-val} & = P_{\\chi_1^2}(x > \\chi^2 | H_0)\n",
    "\\\\\n",
    "& = P \\left( \\text{Norm}(x > \\chi \\cup x < -\\chi; 0, 1) \\right) \n",
    "\\\\\n",
    "& = 2 P\\left( \\text{Norm}(x > \\chi; 0, 1) \\right)\n",
    "\\end{split}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaa6ef",
   "metadata": {},
   "source": [
    "For the Bayesian estimation of the probability that the conversion rate of one group exceeds that of the other, $P(\\theta_B > \\theta_A \\mid A_i, B_j)$, the likelihood in each group is modeled by a binomial distribution, $P(S \\mid \\theta, N) = \\text{Binom}(S \\mid \\theta, N)$, and the prior by a beta distribution, $P(\\theta) = \\text{Beta}(\\theta; \\alpha, \\beta)$. The posterior distribution is then also beta-distributed, with updated parameters $P(\\theta \\mid S, N) = \\text{Beta}(\\theta; \\alpha + S, \\beta + N - S)$. For typical values of $N$ and $S$, the beta distribution is well approximated by a normal distribution, and the distribution of the difference in conversion rates is therefore also approximately normal. Under uniform priors and small differences between groups, the distribution of the difference can be written as $P_{\\Delta \\theta}(x) \\approx \\text{Norm}(x; \\Delta p, s_\\Delta^2)$, with parameters $\\Delta p$ and $s_\\Delta^2$ identical to those used in the $\\chi^2$ test. The probability that the conversion rate of one group exceeds that of the other is given by the positive tail of this distribution, $P(\\theta_B > \\theta_A \\mid A_i, B_j) = P(\\Delta \\theta > 0 \\mid A_i, B_j) \\approx P(\\text{Norm}(x > 0; \\chi, 1))$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(S | \\theta, N) & = \\mbox{Binom}(S | \\theta, N)\n",
    "\\\\ \n",
    "P(\\theta) & = \\mbox{Beta}(\\theta; \\alpha, \\beta)\n",
    "\\\\\n",
    "P(\\theta | S, N) & = \\mbox{Beta}(\\theta; \\alpha + S, \\beta + N - S)\n",
    "\\\\\n",
    "& \\approx \\mbox{Norm}(\\theta; \\mu, \\sigma^2),\n",
    "\\quad\n",
    "\\mu = S / N, \n",
    "\\quad\n",
    "\\sigma^2 = \\mu (1 - \\mu) / N,\n",
    "\\quad \n",
    "S, N \\gg \\alpha, \\beta \\gg 1\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P_{\\theta_A}(x) & = \\mbox{Beta}(x; \\alpha_A + S_A, \\beta_A + N_A - S_A)\n",
    "\\approx \\mbox{Norm}(x; \\mu_A, \\sigma^2_A),\n",
    "\\\\\n",
    "P_{\\theta_B}(x) & = \\mbox{Beta}(x; \\alpha_B + S_B, \\beta_B + N_B - S_B)\n",
    "\\approx \\mbox{Norm}(x; \\mu_B, \\sigma^2_B)\n",
    "\\\\\n",
    "P_{\\Delta \\theta}(x) & \\approx \\mbox{Norm}\\left(x; \\mu_B - \\mu_A, \\sigma_A^2 + \\sigma_B^2\\right)\n",
    "\\approx \\mbox{Norm}\\left(x; \\Delta p, s_{\\Delta}^2 \\right),\n",
    "\\quad \\text{при } p \\approx S_A/N_A \\approx S_B/N_B\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\theta_B > \\theta_A | A_i, B_j) \n",
    "= P(\\Delta \\theta > 0 | A_i, B_j) \n",
    "\\approx P\\left(\\text{Norm}(x > 0; \\Delta p, s_{\\Delta}^2) \\right)\n",
    "= P\\left(\\text{Norm}(x > 0; \\chi, 1) \\right), \\quad \\chi = \\Delta p / s_{\\Delta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4bfc3",
   "metadata": {},
   "source": [
    "By symmetry of the normal distribution $P(\\text{Norm}(x > \\chi \\mid 0, 1)) = P(\\text{Norm}(x < 0 \\mid \\chi, 1))$. Therefore, $P(\\theta_B > \\theta_A \\mid A_i, B_j) \\approx 1 - \\text{p-val}/2$.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\begin{split}\n",
    "\\text{p-val} & = P_{\\chi_1^2}(x > \\chi^2 | H_0)\n",
    "\\\\\n",
    "& = 2 P\\left( \\text{Norm}(x > \\chi; 0, 1) \\right)\n",
    "\\\\\n",
    "& = 2 P\\left(\\text{Norm}(x < 0; \\chi, 1) \\right)\n",
    "\\\\\n",
    "& \\approx 2 \\left( 1 - P(\\theta_B > \\theta_A | A_i, B_j) \\right)\n",
    "\\end{split}\n",
    "\\\\\n",
    "\\,\n",
    "\\\\\n",
    "P(\\theta_B > \\theta_A | A_i, B_j) \\approx 1 - \\frac{\\text{p-val}}{2}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6110e2",
   "metadata": {},
   "source": [
    "The relationship $P(\\theta_B > \\theta_A \\mid A_i, B_j) \\approx 1 - \\mbox{p-val}/2$ can be tested using samples from two Bernoulli distributions with conversion rates $p_A = 0.1$ and $p_B = 0.103$. For the $\\chi^2$ test, the data are arranged in a contingency table with rows $S_A, N_A - S_A$ and $S_B, N_B - S_B$ [[ScipyChi2Con](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)]. Because the $\\chi^2$ $p$-value does not indicate the direction of the difference between $p_A$ and $p_B$, the sample means are compared as well. On the first figure, the $\\chi_1^2$ distribution is shown, with the shaded area corresponding to the $p$-value, $\\mbox{p-val} = P_{\\chi_1^2}(x > \\chi^2 \\mid H_0)$. On the second figure, the dark shaded regions $x > \\chi$ and $x < -\\chi$ correspond to the $p$-value when considering the squared normal variable. The gray curve represents the normal distribution $\\text{Norm}(\\chi, 1)$, and its shaded area approximately equals $1 - P(\\theta_B > \\theta_A \\mid A_i, B_j)$. By symmetry, the area of the shaded gray region coincides with each of the dark regions. This demonstrates that the $p$-value and the Bayesian probability estimate are numerically consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "def posterior_dist_binom(ns, ntotal, a_prior=1, b_prior=1):\n",
    "    a = a_prior + ns\n",
    "    b = b_prior + ntotal - ns \n",
    "    return stats.beta(a=a, b=b)\n",
    "\n",
    "def diff_ba_scaled(post_dist_A, post_dist_B):\n",
    "    m = post_dist_B.mean() - post_dist_A.mean()\n",
    "    v = post_dist_A.var() + post_dist_B.var()\n",
    "    return stats.norm(loc=m/np.sqrt(v), scale=1)\n",
    "\n",
    "\n",
    "pA = 0.1\n",
    "pB = pA * 1.03\n",
    "\n",
    "exactA = stats.bernoulli(pA)\n",
    "exactB = stats.bernoulli(pB)\n",
    "\n",
    "N = 10000\n",
    "sampA = exactA.rvs(size=N)\n",
    "sampB = exactB.rvs(size=N)\n",
    "SA = np.sum(sampA)\n",
    "SB = np.sum(sampB)\n",
    "\n",
    "t = np.array([\n",
    "    [SA,     N - SA],\n",
    "    [SB,     N - SB]\n",
    "])\n",
    "chi2_stat, p_value_chi2, dof, expected = stats.chi2_contingency(t, correction=False)\n",
    "chi = np.sqrt(chi2_stat)\n",
    "p_A_samp = SA / N\n",
    "p_B_samp = SB / N\n",
    "pb_gt_pa_chi = 1 - p_value_chi2 / 2\n",
    "pb_gt_pa_chi = pb_gt_pa_chi if p_B_samp > p_A_samp  else 1 - pb_gt_pa_chi\n",
    "\n",
    "post_dist_A = posterior_dist_binom(ns=SA, ntotal=N)\n",
    "post_dist_B = posterior_dist_binom(ns=SB, ntotal=N)\n",
    "diff_dist_scaled = diff_ba_scaled(post_dist_A, post_dist_B)\n",
    "pb_gt_pa_bayes = 1 - diff_dist_scaled.cdf(0)\n",
    "\n",
    "xaxis_min = 0\n",
    "xaxis_max = 5\n",
    "x = np.linspace(xaxis_min, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.chi.pdf(x, df=1), \n",
    "                         line_color='black', opacity=0.8, name=f'$\\chi^2_1$'))\n",
    "fig.add_trace(go.Scatter(x=[chi2_stat, chi2_stat], y=[0, max(stats.chi.pdf(x, df=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$\\chi^2$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x[x>chi2_stat], y=stats.chi.pdf(x[x>chi2_stat], df=1), \n",
    "                         line_color='black', opacity=0.8, name='$P_{\\chi_1^2}(x > \\chi^2)$', fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.7)\"))\n",
    "fig.update_layout(title='Хи-квадрат',\n",
    "                  xaxis_title='$x$',\n",
    "                  yaxis_title='Плотность вероятности',\n",
    "                  xaxis_range=[xaxis_min, xaxis_max],\n",
    "                  hovermode=\"x\",\n",
    "                  template=\"plotly_white\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "\n",
    "xaxis_min = -5\n",
    "xaxis_max = 5\n",
    "x = np.linspace(xaxis_min, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.norm.pdf(x, loc=0, scale=1), \n",
    "                         line_color='black', opacity=0.8, name='$\\mathrm{Norm}(0, 1)$'))\n",
    "fig.add_trace(go.Scatter(x=[0, 0], y=[0, max(stats.norm.pdf(x, loc=0, scale=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '0'], textposition=\"top center\", \n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[chi, chi], y=[0, max(stats.norm.pdf(x, loc=0, scale=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$\\chi$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[-chi, -chi], y=[0, max(stats.norm.pdf(x, loc=0, scale=1))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$-\\chi$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x[x>chi], y=stats.norm.pdf(x[x>chi], loc=0, scale=1), \n",
    "                         line_color='black', opacity=0.8, name='$P(\\mathrm{Norm}(x > \\chi \\cup x < -\\chi | 0, 1)$', fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.7)\"))\n",
    "fig.add_trace(go.Scatter(x=x[x<-chi], y=stats.norm.pdf(x[x<-chi], loc=0, scale=1), \n",
    "                         line_color='black', opacity=0.8, name='$P(\\mathrm{Norm}(x > \\chi | 0, 1)$', fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.7)\",\n",
    "                         showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x, y=diff_dist_scaled.pdf(x), \n",
    "                         line_color='black', opacity=0.2, name=r'$\\mathrm{Norm}(\\chi, 1)$'))\n",
    "fig.add_trace(go.Scatter(x=x[x<0], y=diff_dist_scaled.pdf(x[x<0]), \n",
    "                         line_color=\"rgba(128, 128, 128, 0.2)\", name=r'$P(\\mathrm{Norm}(x < 0 | \\chi, 1))$', fill=\"tozeroy\", fillcolor=\"rgba(128, 128, 128, 0.2)\"))\n",
    "fig.update_layout(title=r'$P\\text{-значение и байесовская модель}$',\n",
    "                  xaxis_title='$x$',\n",
    "                  yaxis_title='Плотность вероятности',\n",
    "                  xaxis_range=[xaxis_min, xaxis_max],\n",
    "                  hovermode=\"x\",\n",
    "                  template=\"plotly_white\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "\n",
    "print(f'Bayes P(theta_b > theta_a): {pb_gt_pa_bayes:.4f}')\n",
    "print(f\"Chi2: 1-pval/2:   {pb_gt_pa_chi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a081f",
   "metadata": {},
   "source": [
    "The $\\chi^2$ $p$-value is used to select the group with higher conversion in a series of experiments. In each experiment, there are two groups: the conversion in group $A$ is fixed at $p_A = 0.1$, while $p_B$ is randomly chosen within $\\pm 5\\%$ of $p_A$. Data are added to each group in increments of `n_samp_step` per step. At each step, both the $\\chi^2$ $p$-value and the Bayesian probability $P(\\theta_B > \\theta_A \\mid A_i, B_j)$ are computed. The experiment stops when the estimated probability that one group has a higher conversion exceeds `prob_stop` or when the maximum number of points, `n_samp_max`, is reached. Out of `nexps = 1000` experiments, 986 were completed, with 940 correctly identified groups, yielding a proportion of `0.953`, close to `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a849f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best_bayes', 'p_best_chi'])\n",
    "\n",
    "p = 0.1\n",
    "nexps = 1000\n",
    "cmp['A'] = [p] * nexps\n",
    "cmp['B'] = p * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 5_000_000\n",
    "n_samp_step = 10_000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    pA = cmp.at[i, 'A']\n",
    "    pB = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.bernoulli(p=pA)\n",
    "    exact_dist_B = stats.bernoulli(p=pB)\n",
    "    n_samp_total = 0\n",
    "    ns_A = 0\n",
    "    ns_B = 0\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA = exact_dist_A.rvs(n_samp_step)\n",
    "        dB = exact_dist_B.rvs(n_samp_step)\n",
    "        n_samp_total += n_samp_step\n",
    "        ns_A = ns_A + np.sum(dA)\n",
    "        ns_B = ns_B + np.sum(dB)\n",
    "        p_A_samp = ns_A / n_samp_total\n",
    "        p_B_samp = ns_B / n_samp_total\n",
    "        t = np.array([\n",
    "            [ns_A,     n_samp_total - ns_A],\n",
    "            [ns_B,     n_samp_total - ns_B]\n",
    "        ])\n",
    "        chi2_stat, p_value_chi, dof, expected = stats.chi2_contingency(t, correction=False)\n",
    "        pb_gt_pa_chi = 1 - p_value_chi / 2\n",
    "        pb_gt_pa_chi = pb_gt_pa_chi if p_B_samp > p_A_samp  else 1 - pb_gt_pa_chi\n",
    "        best_gr = 'B' if pb_gt_pa_chi >= prob_stop else 'A' if 1 - pb_gt_pa_chi >= prob_stop else None\n",
    "        if best_gr:\n",
    "            post_dist_A = posterior_dist_binom(ns=ns_A, ntotal=n_samp_total)\n",
    "            post_dist_B = posterior_dist_binom(ns=ns_B, ntotal=n_samp_total)\n",
    "            pb_gt_pa_bayes = 1 - diff_ba_scaled(post_dist_A, post_dist_B).cdf(0)\n",
    "            cmp.at[i, 'A_exp'] = p_A_samp\n",
    "            cmp.at[i, 'B_exp'] = p_B_samp\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best_bayes'] = max(pb_gt_pa_bayes, 1 - pb_gt_pa_bayes)\n",
    "            cmp.at[i, 'p_best_chi'] = max(pb_gt_pa_chi, 1 - pb_gt_pa_chi)\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P_best Bayes {max(pb_gt_pa_bayes, 1 - pb_gt_pa_bayes):.4f}, Chi (1-pval/2): {1 - p_value_chi/2:.4f}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(30))\n",
    "finished = np.sum(cmp['best_exp'].notna())\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Finished: {finished}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / finished}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9f73",
   "metadata": {},
   "source": [
    "## Mann-Whitney $U$ Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607868a8",
   "metadata": {},
   "source": [
    "The Mann-Whitney $U$ statistic for samples from two random variables $A$, $B$ of size $N_A$, $N_B$ is defined by pairwise comparisons of elements [[MannWhitneyU](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)]. For continuous distributions, the probability of ties is zero. In this case, the $U$ statistic is the number of pairs $(A_i, B_j)$ where $A_i > B_j$, $U_A = \\sum_{i=1}^{N_A} \\sum_{j=1}^{N_B} I(A_i > B_j)$, where $I$ is the indicator function. Equivalently, the statistic can be expressed as $U_A = R_A - N_A(N_A + 1)/2$, where $R_A$ is the sum of ranks of elements $A$ in the combined sample. The term $N_A(N_A + 1)/2$ corresponds to the minimal sum of ranks if all $A_i < B_j$, computed as an arithmetic series. If the largest $A_i$ exceeds $n$ elements of $B$, then $U_A = n$ and $R_A = N_A(N_A + 1)/2 + n$. For large sample sizes, the ratio $U_A / N_A N_B$ converges to the probability that a randomly selected value from $A$ exceeds one from $B$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "A, B & - \\text{continuous distributions}\n",
    "\\\\\n",
    "U_A & = \\sum_{i=1}^{N_A} \\sum_{j=1}^{N_B} I(A_i > B_j),\n",
    "\\quad\n",
    "I(\\cdot) = 1 \\text{ if the condition is true, else } 0 \n",
    "\\\\\n",
    "U_A & = R_A - N_A (N_A + 1)/2, \\quad R_A \\text{- sum of ranks of elements A in the combined sample } A_i, B_j\n",
    "\\\\\n",
    "\\frac{U_A}{N_A N_B} & \\to P(A > B),\n",
    "\\quad\n",
    "N_A, N_B \\to \\infty\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6f75f",
   "metadata": {},
   "source": [
    "The mean $E[U_A]$ and variance $\\text{Var}(U_A)$ of the $U$ statistic are known under the null hypothesis of identical distributions $A = B$. The standardized statistic $(U_A - E[U_A]) / \\sqrt{\\text{Var}(U_A)}$ converges to a normal distribution. It is convenient to use a probability $u_A = U_A / N_A N_B$. The $p$-value is then computed as $p = P(\\text{Norm}(x < u_A; 1/2, \\text{Var}(U_A) / (N_A N_B)^2))$ for $u_A < 0.5$, or as the upper-tail probability $x > u_A$ for $u_A > 0.5$.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "H_0: A = B, \n",
    "\\quad\n",
    "E[U_A] = \\frac{N_A N_B}{2}, \\quad\n",
    "\\text{Var}(U_A) = \\frac{N_A N_B (N_A + N_B + 1)}{12}\n",
    "\\\\\n",
    "\\frac{U_A - E[U_A]}{\\sqrt{\\text{Var}(U_A)}} \\to \\text{Norm}(0,1), \\quad N_A, N_B \\to \\infty\n",
    "\\\\\n",
    "u_A = \\frac{U_A}{N_A N_B},\n",
    "\\quad\n",
    "\\frac{u_A - 1/2}{\\sqrt{\\text{Var}(U_A)/(N_A N_B)^2}} \\to \\text{Norm}(0,1), \\quad N_A, N_B \\to \\infty\n",
    "\\\\\n",
    "p = P( x > u_A | H_0 ) = P \\left( \\mbox{Norm}\\left(x < u_A; 1/2, \\text{Var}(U_A)/(N_A N_B)^2 \\right) \\right),\n",
    "\\quad u_A < 0.5\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a827a",
   "metadata": {},
   "source": [
    "In the Bayesian approach, the probability $P(B > A)$ can be estimated by comparing the posterior predictive distributions of the two groups. This requires specifying models for the original distributions, constructing posterior distributions of the parameters, and deriving posterior predictive distributions to compute $P(B > A)$. An alternative approach, which does not require distributional assumptions, is to model the probability $\\theta = P(B > A)$ that a point from $B$ exceeds a point from $A$. Data consist of pairs $(A_i, B_j)$, where $B_j > A_i$ with probability $\\theta$. Given the total number of pairs $N$ and the number $S$ of pairs with $B_j > A_i$, $\\theta$ can be estimated. This problem is analogous to modeling conversion rates. To ensure independence, each $A_i$ is compared with only one $B_j$, avoiding repeated comparisons that would create dependencies. The likelihood is then binomial, $P(S \\mid \\theta) = \\text{Binom}(S \\mid \\theta, N)$, and the prior is a beta distribution. The posterior is also beta-distributed. The probability that a point from one group exceeds a point from the other is the posterior mass above $0.5$: $P(B > A) \\approx P(\\theta  > 0.5 | S, N)$.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\theta = P(B > A),\n",
    "\\quad\n",
    "S = \\sum_{i=1}^N I(B_i > A_i),\n",
    "\\quad N_A = N_B = N\n",
    "\\\\\n",
    "\\begin{split}\n",
    "P(S | \\theta) & = \\text{Binom}(S; \\theta, N)\n",
    "\\\\\n",
    "P(\\theta) & = \\text{Beta}(\\theta; \\alpha, \\beta)\n",
    "\\\\\n",
    "P(\\theta | S) & = \\text{Beta}(\\theta; S + \\alpha, N - S + \\beta)\n",
    "\\\\\n",
    "& \\approx \\text{Norm}(\\theta; \\mu, \\sigma^2),\n",
    "\\quad\n",
    "\\mu = (S + \\alpha ) / (N + \\alpha + \\beta), \n",
    "\\quad\n",
    "\\sigma^2 = \\mu (1 - \\mu) / N,\n",
    "\\quad\n",
    "N, S \\gg \\alpha, \\beta \\gg 1\n",
    "\\end{split}\n",
    "\\\\\n",
    "\\,\n",
    "\\\\\n",
    "\\begin{split}\n",
    "P(B > A) \\approx P(\\theta  > 0.5 | S, N) & = P(\\text{Norm}(x > 0.5; \\mu, \\sigma^2))\n",
    "\\end{split}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d883796",
   "metadata": {},
   "source": [
    "In the Mann-Whitney $U$ statistic, each observation $A_i$ is compared with every $B_j$, whereas in the Bayesian model each $A_i$ and $B_j$ participates in only one comparison. Because the Bayesian approach uses fewer effective comparisons, its posterior variance is larger than that of the $U$ statistic. As a result, the $p$-value is not numerically close to the Bayesian probability $P(\\theta > 0.5 \\mid S, N)$. As the sample size grows, $\\mathrm{Var}(U_A)/(N_A N_B)^2 \\to 1/6N$ for $N_A = N_B = N \\to \\infty$. In the Bayesian model, under small differences between groups, the posterior satisfies $\\mu \\approx 0.5$ and $\\sigma^2 \\approx 1/4N$. Consequently, the relationship $p \\lesssim 1 - P(B > A)$ holds asymptotically.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p & \\approx P \\left( \\mbox{Norm}\\left(x < u_A; 0.5, 1/6N \\right) \\right)\n",
    "\\\\\n",
    "& \\lesssim P(\\text{Norm}(x < 0.5; \\mu, 1/4N))\n",
    "\\\\\n",
    "& \\approx P(A > B)\n",
    "\\\\\n",
    "& = 1 - P(B > A)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef141e",
   "metadata": {},
   "source": [
    "The relationship between the $p$-value and the Bayesian probability is illustrated using normal distributions. The probability that a draw from distribution $B$ exceeds a draw from $A$ equals the probability that their difference is positive $P(B > A) = P(B - A > 0)$. The difference of two normally distributed random variables is itself normally distributed, with mean equal to the difference of the original means and variance equal to the sum of the variances [[NormalSum](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)].\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "A \\sim \\text{Norm}(\\mu_A, \\sigma_A^2),\n",
    "\\quad\n",
    "B \\sim \\text{Norm}(\\mu_B, \\sigma_B^2)\n",
    "\\\\\n",
    "B - A \\sim \\text{Norm}(\\mu_B - \\mu_A, \\sigma_A^2 + \\sigma_B^2)\n",
    "\\\\\n",
    "P(B > A) = P(B - A > 0) = 1 - F_{B-A}(0)\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74d305",
   "metadata": {},
   "source": [
    "The first panel shows the original normal distributions with means `mu1 = 0, mu2 = 0.1` and unit variance. In the second panel, the dark curve represents the distribution of $U/N_A N_B$ under the assumption of identical distributions. The dark shaded area corresponds to the $p$-value of the Mann-Whitney $U$ test [[ScipyMannWhitneyU](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html)]. The light curve shows the Bayesian model, and the light shaded area represents the Bayesian probability $P(A > B)$. The Bayesian distribution is wider than $U/N_A N_B$ due to its larger variance. As a result, the light shaded area is typically larger than the dark one, consistent with the relation $p \\lesssim P(A > B)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(34)\n",
    "\n",
    "mu1, sigma1 = 0.0, 1\n",
    "mu2, sigma2 = 0.1, 1\n",
    "exactA = stats.norm(loc=mu1, scale=sigma1)\n",
    "exactB = stats.norm(loc=mu2, scale=sigma2)\n",
    "p_b_gt_a_exact = 1 - stats.norm.cdf(0, loc=mu2-mu1, scale=np.sqrt(sigma1**2 + sigma2**2))\n",
    "\n",
    "nA = nB = 1000\n",
    "sampA = exactA.rvs(nA)\n",
    "sampB = exactB.rvs(nB)\n",
    "\n",
    "U, pval = stats.mannwhitneyu(sampA, sampB, alternative='greater')\n",
    "pval = pval if U / (nA * nB) > 0.5 else 1 - pval\n",
    "varu = nA * nB * (nA + nB + 1) / 12\n",
    "p_u = stats.norm(loc=0.5, scale=np.sqrt(varu/(nA*nA*nB*nB)))\n",
    "ua = U / (nA*nB)\n",
    "\n",
    "a0 = 1\n",
    "b0 = 1\n",
    "S = np.sum(sampB > sampA)\n",
    "post_theta = stats.beta(a0 + S, b0 + nB - S)\n",
    "\n",
    "\n",
    "x = np.linspace(-7, 7, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=exactA.pdf(x),\n",
    "    mode='lines', name='A', line_color='black'))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=exactB.pdf(x),\n",
    "    mode='lines', name='B', line_color='blue', opacity=0.7))\n",
    "fig.update_layout(\n",
    "    title=\"A, B\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "xaxis_min = 0.4\n",
    "xaxis_max = 0.6\n",
    "x = np.linspace(xaxis_min, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=p_u.pdf(x), \n",
    "                         line_color='black', opacity=0.8, name=f'$U_A/N_A N_B$'))\n",
    "fig.add_trace(go.Scatter(x=x[x<ua], y=p_u.pdf(x[x<ua]), \n",
    "                         line_color='black', opacity=0.8, name=r'$p = P(x < u_A | H_0)$', \n",
    "                         fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.7)\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_theta.pdf(x),\n",
    "                         line_color='black', opacity=0.3, name=r'$P(\\theta | S, N)$'))\n",
    "fig.add_trace(go.Scatter(x=x[x<0.5], y=post_theta.pdf(x[x<0.5]), \n",
    "                         name=r'$P(\\theta < 0.5 | S, N)$',\n",
    "                         line_color=\"rgba(128, 128, 128, 0.3)\",\n",
    "                         fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 0, 0.3)\"))\n",
    "fig.add_trace(go.Scatter(x=[1-ua, 1-ua], y=[0, max(p_u.pdf(x))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$1-u_A$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[ua, ua], y=[0, max(p_u.pdf(x))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$u_A$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[0.5, 0.5], y=[0, max(p_u.pdf(x))*1.1], \n",
    "                         line_color='black', \n",
    "                         mode='lines+text', text=['', '$0.5$'], textposition=\"top center\",\n",
    "                         line_dash='dash', showlegend=False))\n",
    "fig.update_layout(\n",
    "    title=r'$U\\text{-статистика и байесовская модель}$',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f'CDF(u_a): {p_u.cdf(ua):.5f}')\n",
    "print(f'scipy min(pval, 1-pval): {min(pval, 1-pval):.5f}')\n",
    "print(f'Bayes P(theta < 0.5) {post_theta.cdf(0.5):.5f}')\n",
    "print()\n",
    "print(f'P(B>A) exact: {p_b_gt_a_exact:.5f}')\n",
    "print(f'1 - U/(NA NB): {1-ua:.5f}')\n",
    "print(f'Bayes E[theta] {post_theta.mean():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bc75e",
   "metadata": {},
   "source": [
    "The accuracy of selecting the variant with higher probability $P(B > A)$ is evaluated for both the Bayesian model and the Mann-Whitney $U$ statistic in a series of experiments. In each experiment, two normal distributions are generated. The mean of group $A$ is fixed at `mua = 0.1`, while the mean of group $B$ is drawn uniformly within $\\pm 5\\%$ of $A$. The Bayesian model is evaluated first. A total of nexp experiments are conducted, with `n_samp_step` observations added to each group per step. At each step, the Bayesian probability $P(B > A) = P(\\theta > 0.5 \\mid S)$ is computed. An experiment stops when either $P(B > A)$ or $P(A > B)$ exceeds `prob_stop`. In both the Bayesian and $U$-test approaches, a minimum effective sample size is required to achieve the target accuracy. In the Bayesian model, this is enforced via the prior parameters `a0 = 100000` and `b0 = 100000` rather than an explicit minimum sample size. Out of 1000 experiments, 860 terminate, and 827 yield the correct decision. The resulting accuracy of 0.96 is close to the target `prob_stop = 0.95`. At stopping, the $U$ statistic is computed and is typically larger than the corresponding Bayesian probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best_bayes', 'p_best_u'])\n",
    "\n",
    "mua = 0.1\n",
    "nexps = 1000\n",
    "cmp['A'] = [mua] * nexps\n",
    "cmp['B'] = mua * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 5_000_000\n",
    "n_samp_step = 10000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    mua = cmp.at[i, 'A']\n",
    "    mub = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.norm(loc=mua)\n",
    "    exact_dist_B = stats.norm(loc=mub)\n",
    "    n_samp_total = 0\n",
    "    sampA = exact_dist_A.rvs(n_samp_max)\n",
    "    sampB = exact_dist_B.rvs(n_samp_max)\n",
    "    while n_samp_total < n_samp_max:\n",
    "        n_samp_total += n_samp_step\n",
    "        a0 = 100000\n",
    "        b0 = 100000\n",
    "        Ub = np.sum(sampB[:n_samp_total] > sampA[:n_samp_total])\n",
    "        post_u_ewise = stats.beta(a0 + Ub, b0 + n_samp_total - Ub)\n",
    "        pb_gt_pa_bayes = 1 - post_u_ewise.cdf(0.5)\n",
    "        best_gr = 'B' if pb_gt_pa_bayes >= prob_stop else 'A' if 1 - pb_gt_pa_bayes >= prob_stop else None\n",
    "        if best_gr:\n",
    "            U, pval = stats.mannwhitneyu(sampA[:n_samp_total], sampB[:n_samp_total], alternative='greater')\n",
    "            cmp.at[i, 'A_exp'] = sampA[:n_samp_total].mean()\n",
    "            cmp.at[i, 'B_exp'] = sampB[:n_samp_total].mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best_bayes'] = max(pb_gt_pa_bayes, 1 - pb_gt_pa_bayes)\n",
    "            cmp.at[i, 'p_best_u'] = max(pval, 1 - pval)\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P_best Bayes {pb_gt_pa_bayes:.4f}, U p-val: {pval:.4f}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(30))\n",
    "finished = np.sum(cmp['best_exp'].notna())\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Finished: {finished}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / finished}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca0a86",
   "metadata": {},
   "source": [
    "To evaluate the accuracy of decisions based on the Mann-Whitney $U$ statistic, a larger sampling step is used, with `n_samp_step = 200_000` observations added to each group per iteration. This step size implicitly enforces a minimum effective sample size. Out of 300 experiments, 262 terminate, and 249 correctly identify the superior group. The resulting accuracy of `0.95` matches the target stopping threshold `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best_bayes', 'p_best_u'])\n",
    "\n",
    "mua = 0.1\n",
    "nexps = 300\n",
    "cmp['A'] = [mua] * nexps\n",
    "cmp['B'] = mua * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 5_000_000\n",
    "n_samp_step = 200_000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    mua = cmp.at[i, 'A']\n",
    "    mub = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.norm(loc=mua)\n",
    "    exact_dist_B = stats.norm(loc=mub)\n",
    "    n_samp_total = 0\n",
    "    sampA = exact_dist_A.rvs(n_samp_max)\n",
    "    sampB = exact_dist_B.rvs(n_samp_max)\n",
    "    while n_samp_total < n_samp_max:\n",
    "        n_samp_total += n_samp_step\n",
    "        U, pval = stats.mannwhitneyu(sampA[:n_samp_total], sampB[:n_samp_total], alternative='greater')\n",
    "        pb_gt_pa_u = 1 - U / n_samp_total / n_samp_total\n",
    "        best_gr = 'B' if pval >= prob_stop else 'A' if 1 - pval >= prob_stop else None        \n",
    "        if best_gr:\n",
    "            a0 = 100000\n",
    "            b0 = 100000\n",
    "            Ub = np.sum(sampB[:n_samp_total] > sampA[:n_samp_total])\n",
    "            post_u_ewise = stats.beta(a0 + Ub, b0 + n_samp_total - Ub)\n",
    "            pb_gt_pa_bayes = 1 - post_u_ewise.cdf(0.5)\n",
    "            cmp.at[i, 'A_exp'] = sampA[:n_samp_total].mean()\n",
    "            cmp.at[i, 'B_exp'] = sampB[:n_samp_total].mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best_bayes'] = max(pb_gt_pa_bayes, 1 - pb_gt_pa_bayes)\n",
    "            cmp.at[i, 'p_best_u'] = max(pval, 1 - pval)\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P_best Bayes {pb_gt_pa_bayes:.4f}, U p-val: {pval:.4f}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(30))\n",
    "finished = np.sum(cmp['best_exp'].notna())\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Finished: {finished}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / finished}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6378",
   "metadata": {},
   "source": [
    "For the $t$-test, $\\chi^2$-test, and Mann-Whitney $U$ test, Bayesian models are presented in which the probability of the superior group is numerically close to the corresponding $p$-values. This correspondence holds despite differences in the definitions of $p$-values and Bayesian probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247790d6",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Chi2Dist] - [Chi-squared Distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution), *Wikipedia.*  \n",
    "[Chi2Pearson] - [Pearson’s Chi-squared Test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test), *Wikipedia.*  \n",
    "[Chi2Test] - [Chi-squared Test](https://en.wikipedia.org/wiki/Chi-squared_test), *Wikipedia.*  \n",
    "[ConjPrior] - [Conjugate Prior](https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution), *Wikipedia.*   \n",
    "[HT] - [Statistical Hypothesis Test](https://en.wikipedia.org/wiki/Statistical_hypothesis_test), *Wikipedia.*   \n",
    "[MannWhitneyU] - [Mann-Whitney U Test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), *Wikipedia.*  \n",
    "[NormalSum] - [Sum of Normally Distributed Random Variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables), *Wikipedia.*  \n",
    "[PVal] - [P-value](https://en.wikipedia.org/wiki/P-value), *Wikipedia.*  \n",
    "[ScipyChi2Con] - [scipy.stats.chi2_contingency](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html), *SciPy Reference.*  \n",
    "[ScipyMannWhitneyU] - [scipy.stats.mannwhitneyu](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html), *SciPy Reference.*  \n",
    "[ScipyTTestInd] - [scipy.stats.ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html), *SciPy Reference.*  \n",
    "[TailedTests] - [One- and Two-tailed Tests](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests), *Wikipedia.*  \n",
    "[TDist] - [Student’s t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution), *Wikipedia.*  \n",
    "[TestStat] - [Test Statistic](https://en.wikipedia.org/wiki/Test_statistic), *Wikipedia.*  \n",
    "[TTest] - [Student’s t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), *Wikipedia.*  \n",
    "[WelchT] - [Welch’s t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test), *Wikipedia.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
