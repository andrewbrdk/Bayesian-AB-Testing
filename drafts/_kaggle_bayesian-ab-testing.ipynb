{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d7ef80",
   "metadata": {
    "papermill": {
     "duration": 0.014131,
     "end_time": "2026-01-21T04:25:25.013710",
     "exception": false,
     "start_time": "2026-01-21T04:25:24.999579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99e36f",
   "metadata": {},
   "source": [
    "Kaggle: [https://www.kaggle.com/code/andrewbrdk/bayesian-a-b-testing](https://www.kaggle.com/code/andrewbrdk/bayesian-a-b-testing)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff972c",
   "metadata": {
    "papermill": {
     "duration": 0.012504,
     "end_time": "2026-01-21T04:25:25.038542",
     "exception": false,
     "start_time": "2026-01-21T04:25:25.026038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "GitHub: [https://github.com/andrewbrdk/Bayesian-AB-Testing](https://github.com/andrewbrdk/Bayesian-AB-Testing) \n",
    "\n",
    "*A/B testing is reviewed, and Bayesian modelling is discussed. Conversions, means, transactional revenue per user, and orders per visitor are compared in Bayesian approach.*\n",
    "\n",
    "&nbsp; &nbsp; *- [A/B Tests](#A/B-Tests)*  \n",
    "&nbsp; &nbsp; *- [Bayesian Modelling](#Bayesian-Modelling)*  \n",
    "&nbsp; &nbsp; *- [Conversions](#Conversions)*   \n",
    "&nbsp; &nbsp; *- [Means](#Means)*    \n",
    "&nbsp; &nbsp; *- [Transactional Revenue per User](#Transactional-Revenue-per-User)*  \n",
    "&nbsp; &nbsp; *- [Orders per Visitor](#Orders-per-Visitor)*  \n",
    "&nbsp; &nbsp; *- [Conclusion](#Conclusion)*  \n",
    "&nbsp; &nbsp; *- [References](#References)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec071e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:25.065575Z",
     "iopub.status.busy": "2026-01-21T04:25:25.065141Z",
     "iopub.status.idle": "2026-01-21T04:25:28.067541Z",
     "shell.execute_reply": "2026-01-21T04:25:28.066571Z"
    },
    "papermill": {
     "duration": 3.018377,
     "end_time": "2026-01-21T04:25:28.069875",
     "exception": false,
     "start_time": "2026-01-21T04:25:25.051498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.color\": \"whitesmoke\",\n",
    "    \"axes.axisbelow\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.titlepad\": 20,\n",
    "    \"axes.titlelocation\": \"left\",\n",
    "    \"lines.linewidth\": 1.3,\n",
    "    \"legend.frameon\": False,\n",
    "    \"figure.figsize\": (10, 5),\n",
    "    \"figure.dpi\": 200,\n",
    "    \"savefig.dpi\": 200,\n",
    "})\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c145d1c",
   "metadata": {
    "papermill": {
     "duration": 0.012265,
     "end_time": "2026-01-21T04:25:28.094246",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.081981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A/B Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2f5c3",
   "metadata": {
    "papermill": {
     "duration": 0.011912,
     "end_time": "2026-01-21T04:25:28.118062",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.106150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mobile apps and web services are constantly updated to drive revenue, conversions, engagement, and other key metrics. Some updates can hurt the product. It is essential to measure the impact of new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019b64b",
   "metadata": {
    "papermill": {
     "duration": 0.012685,
     "end_time": "2026-01-21T04:25:28.143087",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.130402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_experiment_versions.png?raw=true\" alt=\"experiment_versions\" width=\"400\"/>\n",
    "    \n",
    "<em>Raising prices (on the right) would increase average order value but drop conversion. The net effect on revenue is unpredictable and should be measured. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de1fcb",
   "metadata": {
    "papermill": {
     "duration": 0.012337,
     "end_time": "2026-01-21T04:25:28.167324",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.154987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Comparing metrics before and after a release doesn't show the true impact. Other factors, such as marketing campaigns, also affect them. Changes in metrics over time can't be attributed to a particular release unless the effect is strong enough to stand out over natural fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe7272",
   "metadata": {
    "papermill": {
     "duration": 0.011812,
     "end_time": "2026-01-21T04:25:28.190928",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.179116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_effect_size.png?raw=true\" alt=\"effect_size\"  width=\"900\"/>\n",
    "<em>Metrics dynamics is influenced by numerous factors. Unless the feature impact is strong enough (e.g., a sharp 30% drop), a change is hard to attribute to a particular release. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f37659",
   "metadata": {
    "papermill": {
     "duration": 0.011888,
     "end_time": "2026-01-21T04:25:28.214606",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.202718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A/B tests isolate the impact of the new feature. The original and modified versions are launched in parallel. Users are randomly split between the variants. Key metrics are compared in each group. The test is stopped when one version clearly outperforms the other or the experiment has been running long enough with no benefits to continue. The version to roll out for all users is decided upon the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c578464",
   "metadata": {
    "papermill": {
     "duration": 0.012072,
     "end_time": "2026-01-21T04:25:28.238968",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.226896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_ab_test.png?raw=true\" alt=\"ab_test\" width=\"800\"/>\n",
    "    \n",
    "<em>A/B experiment setup: the test versions run in parallel. Users are randomly split between the variants. Next steps are determined upon key metrics comparison between the groups. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b9e1d",
   "metadata": {
    "papermill": {
     "duration": 0.013895,
     "end_time": "2026-01-21T04:25:28.264811",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.250916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The A/B testing causal diagram [[CausalDAG](https://en.wikipedia.org/wiki/Causal_graph)] is as follows. Metrics are determined by users' actions in the service. The actions depend on the version of the website or app (e.g., available pricing plans), external factors (e.g., seasonality), and user segments (e.g., new vs. returning customers). In an A/B test the versions run simultaneously. External factors influence both groups equally. Users are split between the groups randomly, so the segment composition is identical. Differences in metrics between the groups are attributed to the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5d0f3",
   "metadata": {
    "papermill": {
     "duration": 0.012178,
     "end_time": "2026-01-21T04:25:28.290344",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.278166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_causal.png?raw=true\" alt=\"causal\" width=\"600\"/>\n",
    "    \n",
    "<em>Metrics are determined by users' actions in the service. The actions depend on the current version of the service, external factors, and user segments. In A/B test variants run simultaneously, so external factors impact metrics equally. Random user assignment ensures segments are comparable. As a result, differences in metrics between groups are attributed to the tested functionality. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66122561",
   "metadata": {
    "papermill": {
     "duration": 0.011834,
     "end_time": "2026-01-21T04:25:28.314029",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.302195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In A/B tests it is necessary to estimate metrics in each group, the impact of the feature, and select the \"best\" variant. The exact metric values are unknown and estimated from the collected data. Metrics can be treated as random variables. Probability distributions for these variables should fit the experimental data. Comparing the distributions allows to assess the effect. It’s convenient to present point estimates and highest probability density intervals. For example, a metric in Group A is $p_A = 7.1 \\pm 0.2$, and in Group B is $p_B = 7.4 \\pm 0.3$. The relative difference shows the effect $(p_B - p_A) / p_A = 4.2 \\pm 0.2\\%$. To select the \"best\" group the probability metric in B is higher than A is evaluated $P(p_B > p_A) = 95\\%$. Probability is understood in the subjective sense, as a measure of confidence in a particular outcome among several possibilities [[SubjProb](https://en.wikipedia.org/wiki/Probability_interpretations#Subjectivism)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ef56c",
   "metadata": {
    "papermill": {
     "duration": 0.012515,
     "end_time": "2026-01-21T04:25:28.338569",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.326054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_ab_metric_random.png?raw=true\" alt=\"ab_metric_random\" width=\"500\"/>\n",
    "<br/>   \n",
    "<em>\n",
    "In A/B tests it is necessary to evaluate metrics, effects, and choose the \"best\" group. Exact metric values are unknown and estimated from the collected data. The estimates are treated as random variables. The probability distributions for these variables should fit the experimental data. Comparing the distributions allows to asses the effect.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54fbb9",
   "metadata": {
    "papermill": {
     "duration": 0.011974,
     "end_time": "2026-01-21T04:25:28.362538",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.350564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Metric estimates are imprecise when data is limited. Estimates improve as more data is gathered. Confidence in better group also grows. The experiment can be stopped when confidence reaches a sufficient level. However, different stopping criteria may be employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ff16a",
   "metadata": {
    "papermill": {
     "duration": 0.011714,
     "end_time": "2026-01-21T04:25:28.386005",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.374291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_ab_dynamics.png?raw=true\" alt=\"ab_dynamics\" width=\"430\"/>\n",
    "<br/>\n",
    "<em>\n",
    "As data accumulates, the accuracy of metric estimates improves, and confidence in identifying the better group grows. The experiment can be stopped once confidence reaches a sufficient level.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a850106",
   "metadata": {
    "papermill": {
     "duration": 0.012281,
     "end_time": "2026-01-21T04:25:28.410160",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.397879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Distributions of metrics based on experimental data are estimated with Bayesian modelling [[SR](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919), [SGBS](https://www.amazon.co.uk/Students-Guide-Bayesian-Statistics/dp/1473916364)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f343ff",
   "metadata": {
    "papermill": {
     "duration": 0.011876,
     "end_time": "2026-01-21T04:25:28.433960",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.422084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bayesian Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707cf39",
   "metadata": {
    "papermill": {
     "duration": 0.011827,
     "end_time": "2026-01-21T04:25:28.457629",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.445802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The first example. Will it rain during the day if it's cloudy in the morning? To answer this, we can calculate the probability of rain given that it's cloudy, $P(\\mbox{Rain | Clouds}) = (\\mbox{Clouds, Rain}) / (\\mbox{Clouds})$. The total number of cloudy days is the sum of cloudy days with rain and cloudy days without rain, $(\\mbox{Clouds}) = \\mbox{(Clouds, Rain)} + \\mbox{(Clouds, No Rain)}$. Let’s assume 20% of days are rainy, $P(\\text{Rain}) = 20\\%$, the probability of morning cloudiness on a rainy day is $P(\\mbox{Clouds | Rain}) = 70\\%$, and on a dry day, $P(\\mbox{Clouds | No Rain}) = 10\\%$. The number of cloudy days with rain can be expressed using these probabilities: $\\mbox{(Clouds, Rain)} = (\\mbox{Total Days}) P(\\mbox{Rain}) P(\\mbox{Clouds | Rain})$. Similarly, the number of cloudy days without rain can be calculated. After substituting values, we get $P(\\mbox{Rain | Clouds}) = (0.7 \\cdot 0.2) / (0.7 \\cdot 0.2 + 0.1 \\cdot 0.8) = 63.6\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdd2cc",
   "metadata": {
    "papermill": {
     "duration": 0.011957,
     "end_time": "2026-01-21T04:25:28.481905",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.469948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_bayes_rain.png?raw=true\" alt=\"bayes_rain\" width=\"600\"/>\n",
    "<br/>\n",
    "<em>\n",
    "The probability of a rainy day given a cloudy morning is estimated by the ratio of rainy, cloudy days to all cloudy days — both with and without rain.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa29d90",
   "metadata": {
    "papermill": {
     "duration": 0.012037,
     "end_time": "2026-01-21T04:25:28.505873",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.493836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "P(\\mbox{Rain | Clouds}) & = \\frac{\\mbox{Clouds, Rain}}{\\mbox{Clouds, Rain + Clouds, No Rain}} \n",
    "\\\\\n",
    "\\\\\n",
    "& = \\frac{P(\\mbox{Clouds | Rain})P(\\mbox{Rain})}{P(\\mbox{Clouds | Rain})P(\\mbox{Rain}) + P(\\mbox{Clouds | No Rain})P(\\mbox{No Rain})}\n",
    "\\\\\n",
    "\\\\\n",
    "& = \\frac{0.7 \\cdot 0.2}{0.7 \\cdot 0.2 + 0.1 \\cdot 0.8} = 63.6 \\%\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca03e22",
   "metadata": {
    "papermill": {
     "duration": 0.011829,
     "end_time": "2026-01-21T04:25:28.529635",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.517806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In estimating the probability of rain given cloudy weather, $P(\\text{Rain | Clouds})$, it's important to consider not only the probability of cloudiness on a rainy day, $P(\\text{Clouds | Rain})$, but also the proportion of rainy days, $P(\\text{Rain})$, and the probability of cloudiness on a dry day, $P(\\text{Clouds | No Rain})$. Ignoring these factors can lead to a base rate fallacy [[BaseFal](https://en.wikipedia.org/wiki/Base_rate_fallacy)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445baf09",
   "metadata": {
    "papermill": {
     "duration": 0.012521,
     "end_time": "2026-01-21T04:25:28.553893",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.541372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another example. In the evening, you see an unfamiliar object in the park. From a distance, only its outline is visible. You try to guess what it is. Formally, this is a Bayes’ rule problem. You list possible options — leaves, a lost hat, a bird, a puddle. For each, you estimate how likely it is to match the observed shape: $P(\\mbox{Shape | Leaves})P(\\mbox{Shape | Leaves})$, $P(\\mbox{Shape | Hat})P(\\mbox{Shape | Hat})$, etc. You also factor in how common each option is — leaves are much more likely than a lost hat: $P(\\mbox{Leaves})>P(\\mbox{Hat})$. Bayes’ rule combines this information to estimate the probability of each option given what you see: $P(\\mbox{Leaves | Shape}) \\propto P(\\mbox{Shape | Leaves}) P(\\mbox{Leaves})$. As you get closer, the object glances at you and quickly climbs a tree — it turns out to be a squirrel you haven’t seen in the park for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed2e29",
   "metadata": {
    "papermill": {
     "duration": 0.011819,
     "end_time": "2026-01-21T04:25:28.578143",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.566324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_bayes_park.png?raw=true\" alt=\"park\" width=\"600\"/>\n",
    "<em>\n",
    "<br/>\n",
    "To choose between options using Bayes' rule, you need to account for two things: how common each option is (the width of the vertical bars) and how likely it is to produce the observed shape (the height of the shaded area within the bar). The probability of each option is proportional to the area of its shaded region relative to the total shaded area across all options.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2234b",
   "metadata": {
    "papermill": {
     "duration": 0.012374,
     "end_time": "2026-01-21T04:25:28.603151",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.590777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This example illustrates the key elements of Bayesian modeling. We have data or observations $\\mathcal{D}$ and possible explanations or hypotheses $\\mathcal{H}_i$. To choose between them, we evaluate how well each hypothesis explains the data — this is done by calculating likelihoods $P(\\mathcal{D}|\\mathcal{H}_i)$. Prior knowledge or experience is reflected in the prior probabilities $P(\\mathcal{H}_i)$. Bayes' rule combines the likelihood and the prior to compute the posterior probability $P(\\mathcal{H}_i|\\mathcal{D})$ — our updated belief in each hypothesis given the data. Based on these posteriors, we select the most suitable model. It's important to validate models: even if one hypothesis fits the data better than the others, none of them may fully reflect reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e4c0f",
   "metadata": {
    "papermill": {
     "duration": 0.011964,
     "end_time": "2026-01-21T04:25:28.627028",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.615064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{H}_i | \\mathcal{D}) &= \\frac{ P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }{P(\\mathcal{D})}\n",
    "= \\frac{ P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }{\\sum \\limits_i P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }\n",
    "\\\\\n",
    "P(\\mathcal{H}_i | \\mathcal{D}) &\\mbox{ - posterior probability distribution} \n",
    "\\\\\n",
    "P(\\mathcal{D} | \\mathcal{H}_i) &\\mbox{ - likelihood function}\n",
    "\\\\\n",
    "P(\\mathcal{H}_i) &\\mbox{ - prior probability distribution}\n",
    "\\\\\n",
    "P(\\mathcal{D}) &\\mbox{ - normalization constant}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e5e10",
   "metadata": {
    "papermill": {
     "duration": 0.012047,
     "end_time": "2026-01-21T04:25:28.651571",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.639524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_bayes_hypotheses_square.png?raw=true\" alt=\"bayes_hypotheses_square\" width=\"900\"/>\n",
    "<em>\n",
    "<br/>\n",
    "A set of models is selected to explain the data. For each model, we specify prior belief — its probability relative to other models. We then calculate the likelihood — the probability of observing the data assuming the model is true. Using Bayes' rule, we update our belief in each model given the data — this gives the posterior probability.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ee898",
   "metadata": {
    "papermill": {
     "duration": 0.011826,
     "end_time": "2026-01-21T04:25:28.675578",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.663752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next example. Suppose $N=1000$ users visited a webpage, and $n_s=100$ of them clicked the “Continue” button. What does the distribution of the possible conversion rate $p$ look like? We assume each user has the same probability of converting, and — before seeing the data — all possible values of $p$ are considered equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112adea5",
   "metadata": {
    "papermill": {
     "duration": 0.011728,
     "end_time": "2026-01-21T04:25:28.699082",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.687354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We need to estimate the probability $P(\\mathcal{H} | \\mathcal{D}) = P(p | n_s, N)$ for given $n_s$ и $N$. Using Bayes’ rule, we have: $P(p | n_s, N) \\propto P(n_s, N | p) P(p)$. Each user clicks with probability $p$ and does not click with probability $1-p$. The clicks of $N$ users can be modeled as a sequence of Bernoulli random variables [[BernProc](https://en.wikipedia.org/wiki/Bernoulli_process), [SciPyBern](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html)]. The probability of $n_s$ conversions out of $N$ with success probability $p$ is given by a binomial distribution $P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) = \\mbox{Binom}(n_s, N; p)$ [[BinomDist](https://en.wikipedia.org/wiki/Binomial_distribution), [SciPyBinom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html)]. Since all possible prior values of the conversion rate are equally likely, the prior distribution is uniform $P(\\mathcal{H}) = P(p) = \\mbox{Unif}(0, 1) = 1$. The posterior distribution $P(p | n_s, N)$ will be a Beta distribution [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution), [SciPyBeta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html)].\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) & = \\mbox{Binom}(n_s, N; p) = C^{n_s}_{N} p^{n_s} (1 - p)^{N-n_s}\n",
    "\\\\\n",
    "\\\\\n",
    "P(\\mathcal{H}) = P(p) & = \\mbox{Unif}(0, 1) = 1\n",
    "\\\\\n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) = P(p | n_s, N) \n",
    "& = \\frac{P(n_s, N | p) P(p)}{P(n_s, N)}\n",
    "= \\frac{P(n_s, N | p) P(p)}{\\int_0^1 d p P(n_s, N | p) P(p)}\n",
    "\\\\\n",
    "& = \\frac{p^{n_s} (1 - p)^{N-n_s}}{\\int_0^1 d p (1 - p)^{N-n_s} p^{n_s} }\n",
    "= \\mbox{Beta}(p; n_s + 1, N - n_s + 1)\n",
    "\\\\\n",
    "\\\\\n",
    "\\mbox{Beta}(x; \\alpha, \\beta) & \\equiv \\frac{x^{\\alpha-1} (1 - x)^{\\beta-1}}{\\int_0^1 dx x^{\\alpha-1} (1 - x)^{\\beta-1}}\n",
    " = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1 - x)^{\\beta-1}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026fc1f",
   "metadata": {
    "papermill": {
     "duration": 0.012391,
     "end_time": "2026-01-21T04:25:28.723313",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.710922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The graph of the posterior distribution $ P(p | n_s, N) $ is shown below. The mode coincides with the sample mean $n_s/N$, and the most likely values of $p$ are near this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac485e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:28.749982Z",
     "iopub.status.busy": "2026-01-21T04:25:28.748801Z",
     "iopub.status.idle": "2026-01-21T04:25:29.247988Z",
     "shell.execute_reply": "2026-01-21T04:25:29.246974Z"
    },
    "papermill": {
     "duration": 0.515854,
     "end_time": "2026-01-21T04:25:29.250994",
     "exception": false,
     "start_time": "2026-01-21T04:25:28.735140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ns = 100\n",
    "ntotal = 1000\n",
    "\n",
    "p_samp = ns / ntotal\n",
    "p_dist = stats.beta(a=ns+1, b=ntotal-ns+1)\n",
    "\n",
    "xaxis_max = 0.2\n",
    "x = np.linspace(0, xaxis_max, 1000)\n",
    "y = p_dist.pdf(x) \n",
    "plt.figure()\n",
    "plt.plot(x, y, color=\"black\", label=\"Distribution\")\n",
    "plt.vlines(p_samp, ymin=0, ymax=y.max(),\n",
    "    colors=\"black\", linestyles=\"dashed\", label=\"Sample mean\"\n",
    ")\n",
    "plt.title(r\"Posterior Distribution $P(p \\mid n_s, N)$\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, xaxis_max)\n",
    "plt.gca().xaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494286c6",
   "metadata": {
    "papermill": {
     "duration": 0.015729,
     "end_time": "2026-01-21T04:25:29.280577",
     "exception": false,
     "start_time": "2026-01-21T04:25:29.264848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another example. For version A of a webpage, $N_A=1000$ users visited, and $n_{s_A}=100$ clicked the \"Continue\" button. For version B, $N_B=1000$ users visited, and $n_{s_B}=110$ clicked the \"Continue\" button. What is the probability that the conversion rate for page B is higher than for page A?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a4d69",
   "metadata": {
    "papermill": {
     "duration": 0.01278,
     "end_time": "2026-01-21T04:25:29.306343",
     "exception": false,
     "start_time": "2026-01-21T04:25:29.293563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You need the probability $P(p_B > p_A)$. The posterior distribution of the conversion rates for each group is calculated as in the previous example: $P(p; n_s, N) = \\mbox{Beta}(p; n_s + 1, N - n_s + 1)$. To estimate $P(p_B > p_A)$, we can calculate the distribution of $p_B - p_A$ and find the probability $P(p_B - p_A > 0)$. The distribution of $p_B - p_A$ in general is given by the convolution of the two distributions [[ProbConv](https://en.wikipedia.org/wiki/Convolution_of_probability_distributions)]. In this case, we can use an approximation. Given the parameters, the posterior Beta distributions are close to normal distributions [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution#Special_and_limiting_cases), [NormDist](https://en.wikipedia.org/wiki/Normal_distribution), [SciPyNorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=norm)]. The difference of random variables with normal distributions is also a random variable with a normal distribution [[NormSum](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)]. Therefore, the distribution of $p_B - p_A$ is approximately normal, with a mean equal to the difference of the means of the posterior Beta distributions and a variance equal to the sum of the variances. The desired probability is expressed using the cumulative distribution function $P(p_B - p_A > 0) = 1 - F(0)$ [[CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function)]. Instead of analytical transformations, a numerical estimate can be performed. For this, samples are generated from the posterior distributions and compared. The first graph shows the posterior distributions in the groups. The second shows the approximate analytical distribution of $p_B - p_A$ and the distribution of the difference between samples from the posterior distributions. Both calculations yield a similar result: $P(p_B > p_A) = 77 \\%$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(p_B > p_A) & = P(p_B - p_A > 0)\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_A}(x) = \\mbox{Beta}(x; n_{s_A} + 1, N_A - n_{s_A} + 1)\n",
    "& \\approx \\mbox{Norm}(x; \\mu_A, \\sigma_A^2),\n",
    "\\quad\n",
    "\\mu_A = n_{s_A}/N_A, \n",
    "\\,\n",
    "\\sigma_A^2 = \\mu_A (1 - \\mu_A) / N_A,\n",
    "\\quad\n",
    "N_A \\gg n_{s_A} \\gg 1\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_B}(x) = \\mbox{Beta}(x; n_{s_B} + 1, N_B - n_{s_B} + 1)\n",
    "& \\approx \\mbox{Norm}(x; \\mu_B, \\sigma_B^2),\n",
    "\\quad\n",
    "\\mu_B = n_{s_B}/N_B, \n",
    "\\,\n",
    "\\sigma_B^2 = \\mu_B (1 - \\mu_B) / N_B,\n",
    "\\quad\n",
    "N_B \\gg n_{s_B} \\gg 1\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_B - p_A}(x) = \n",
    "\\int_{-\\infty}^{\\infty} dy P_{p_B}(y) P_{p_A}(y-x)\n",
    "& \\approx \\mbox{Norm}\\left(x; \\mu_B - \\mu_A, \\sigma_A^2 + \\sigma_B^2\\right),\n",
    "\\quad\n",
    "\\mbox{Norm}(x ; \\mu, \\sigma^2) \\equiv \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\tfrac{(x-\\mu)^2}{2 \\sigma^2} }\n",
    "\\\\\n",
    "\\\\\n",
    "P(p_B - p_A > 0) & = 1 - F_{p_B - p_A}(0)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e5122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:29.335826Z",
     "iopub.status.busy": "2026-01-21T04:25:29.335501Z",
     "iopub.status.idle": "2026-01-21T04:25:30.744129Z",
     "shell.execute_reply": "2026-01-21T04:25:30.743125Z"
    },
    "papermill": {
     "duration": 1.427341,
     "end_time": "2026-01-21T04:25:30.746484",
     "exception": false,
     "start_time": "2026-01-21T04:25:29.319143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "na = 1000\n",
    "sa = 100\n",
    "nb = 1000\n",
    "sb = 110\n",
    "\n",
    "p_dist_a = stats.beta(a=sa+1, b=na-sa+1)\n",
    "p_dist_b = stats.beta(a=sb+1, b=nb-sb+1)\n",
    "\n",
    "approx_diff_dist = stats.norm(loc=p_dist_b.mean() - p_dist_a.mean(), \n",
    "                              scale=np.sqrt(p_dist_b.std()**2 + p_dist_a.std()**2))\n",
    "dist_p_b_gt_a = 1 - approx_diff_dist.cdf(0)\n",
    "\n",
    "npost = 50000\n",
    "samp_a = p_dist_a.rvs(size=npost)\n",
    "samp_b = p_dist_b.rvs(size=npost)\n",
    "samp_p_b_gt_a = np.sum(samp_b > samp_a) / npost\n",
    "\n",
    "xaxis_max = 0.2\n",
    "x = np.linspace(0, xaxis_max, 1000)\n",
    "plt.figure()\n",
    "plt.plot(x, p_dist_a.pdf(x), color=\"black\", label=\"A\")\n",
    "plt.plot(x, p_dist_b.pdf(x), color=\"black\", alpha=0.3, label=\"B\")\n",
    "plt.title(\"Posterior Distributions\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, xaxis_max)\n",
    "plt.gca().xaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(-0.3, 0.3, 1000)\n",
    "plt.figure()\n",
    "plt.plot(x, approx_diff_dist.pdf(x), color=\"black\", label=\"Analytical approximation\")\n",
    "plt.hist(samp_b - samp_a, bins=500, density=True, color=\"black\", alpha=0.3, label=\"Posterior samples difference\")\n",
    "plt.vlines(0, 0, approx_diff_dist.pdf(x).max() * 1.05, colors=\"black\", linestyles=\"dashed\")\n",
    "plt.title(\"$p_B - p_A$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(-0.1, 0.1)\n",
    "plt.gca().xaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(p_b > p_a) diff dist: {dist_p_b_gt_a:.4f}\")\n",
    "print(f\"P(p_b > p_a) post samples: {samp_p_b_gt_a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9479b",
   "metadata": {
    "papermill": {
     "duration": 0.015624,
     "end_time": "2026-01-21T04:25:30.778586",
     "exception": false,
     "start_time": "2026-01-21T04:25:30.762962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbaef6",
   "metadata": {
    "papermill": {
     "duration": 0.015415,
     "end_time": "2026-01-21T04:25:30.809648",
     "exception": false,
     "start_time": "2026-01-21T04:25:30.794233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In A/B tests, conversions to orders, clicks on buttons, and other actions are often compared. If a user's behavior does not affect others, a Bernoulli process can be used for modeling. With a conversion rate $p$, the probability that $n_s$ out of $N$ users will perform the target action follows a binomial distribution $P(n_s, N | p) = \\mbox{Binom}(n_s, N | p)$. The prior distribution for conversions is conveniently modeled with a Beta distribution $P(p) = \\mbox{Beta}(p; \\alpha, \\beta)$. The Beta distribution's dependency on $p$, without normalization constants, is $\\mbox{Beta}(p; \\alpha, \\beta) \\propto p^{\\alpha-1}(1-p)^{\\beta-1}$. This form remains valid for the product of the likelihood and the prior distribution $P(p | n_s, N) \\propto \\mbox{Binom}(p, N) \\mbox{Beta}(p; \\alpha, \\beta) \\propto p^{n_s + \\alpha - 1} (1-p)^{N - n_s + \\beta - 1}$. The only important part is the dependency on $p$; the other factors will be included in the normalization constant. Therefore, the posterior distribution will also be a Beta distribution, but with different parameters $P(p | n_s, N) = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)$. Prior distributions with this property are called conjugate priors [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcf1bf",
   "metadata": {
    "papermill": {
     "duration": 0.015045,
     "end_time": "2026-01-21T04:25:30.839952",
     "exception": false,
     "start_time": "2026-01-21T04:25:30.824907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "P(\\mathcal{H} | \\mathcal{D}) \\propto P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) = \\mbox{Binom}(n_s, N | p) = C_{N}^{n_s} p^{n_s} (1-p)^{N-n_s}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathcal{H}) = P(p) = \\mbox{Beta}(p; \\alpha, \\beta) = \n",
    "\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{H} | \\mathcal{D}) & = P(p | n_s, N) \n",
    "\\\\\n",
    "& \\propto \\mbox{Binom}(n_s, N | p) \\mbox{Beta}(p; \\alpha, \\beta)\n",
    "\\\\\n",
    "& \\propto C_{N}^{n_s} p^{n_s} (1-p)^{N-n_s}\n",
    "\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "\\\\\n",
    "& \\propto p^{n_s + \\alpha - 1} (1-p)^{N - n_s + \\beta - 1}\n",
    "\\\\\n",
    "& = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69613a8",
   "metadata": {
    "papermill": {
     "duration": 0.015007,
     "end_time": "2026-01-21T04:25:30.870310",
     "exception": false,
     "start_time": "2026-01-21T04:25:30.855303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Beta distributions for different parameters are shown in the graph below [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution), [SciPyBeta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html)]. When $\\alpha = 1, \\beta=1$, the distribution is uniform—these values are convenient for use as prior distributions. In other cases, the maximum of the distribution occurs at $p = (\\alpha-1) / (\\alpha + \\beta - 2)$. As $\\alpha$ and $\\beta$ increase, the distribution narrows and approaches a normal distribution. Instead of using $\\alpha = 1, \\beta=1$ for the prior, the initial values of $\\alpha$ and $\\beta$ can be chosen based on historical data to make the prior distribution of conversions match the historical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2c524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:30.902628Z",
     "iopub.status.busy": "2026-01-21T04:25:30.902181Z",
     "iopub.status.idle": "2026-01-21T04:25:31.169440Z",
     "shell.execute_reply": "2026-01-21T04:25:31.168311Z"
    },
    "papermill": {
     "duration": 0.286401,
     "end_time": "2026-01-21T04:25:31.171989",
     "exception": false,
     "start_time": "2026-01-21T04:25:30.885588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, stats.beta.pdf(x, a=1, b=1), color='black', linestyle='dashed')\n",
    "plt.plot(x, stats.beta.pdf(x, a=1, b=5), color='black', linestyle='solid', linewidth=1.2)\n",
    "plt.plot(x, stats.beta.pdf(x, a=7, b=12), color='black', linestyle='solid', linewidth=1.2)\n",
    "plt.plot(x, stats.beta.pdf(x, a=150, b=50), color='black', linestyle='solid', linewidth=1.2)\n",
    "\n",
    "text_x = [0.93, 0.08, 0.37, 0.84]\n",
    "text_y = [1.35, 5.00, 4.10, 12.0]\n",
    "labels = [\"a=1, b=1\", \"a=1, b=5\", \"a=7, b=12\", \"a=150, b=50\"]\n",
    "for tx, ty, label in zip(text_x, text_y, labels):\n",
    "    plt.text(tx, ty, label, ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.title(\"Beta Distribution Beta(a, b)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74059752",
   "metadata": {
    "papermill": {
     "duration": 0.016743,
     "end_time": "2026-01-21T04:25:31.206361",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.189618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To verify the conversion calculation using the data, we set an exact value for the conversion `p`, then generate `nsample` binary random variables. From this sample, we build the posterior distribution of possible conversion values, denoted as `post_dist`. On the graph, the mode of the posterior distribution coincides with the sample mean and is close to the true value of `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70031457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:31.241168Z",
     "iopub.status.busy": "2026-01-21T04:25:31.240833Z",
     "iopub.status.idle": "2026-01-21T04:25:31.520422Z",
     "shell.execute_reply": "2026-01-21T04:25:31.519421Z"
    },
    "papermill": {
     "duration": 0.300997,
     "end_time": "2026-01-21T04:25:31.523889",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.222892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def posterior_dist_binom(ns, ntotal, a_prior=1, b_prior=1):\n",
    "    a = a_prior + ns\n",
    "    b = b_prior + ntotal - ns \n",
    "    return stats.beta(a=a, b=b)\n",
    "    \n",
    "p = 0.1\n",
    "nsample = 1010\n",
    "\n",
    "exact_dist = stats.bernoulli(p=p)\n",
    "data = exact_dist.rvs(nsample)\n",
    "post_dist = posterior_dist_binom(ns=np.sum(data), ntotal=len(data))\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.figure()\n",
    "y = post_dist.pdf(x)\n",
    "plt.plot(x, y, color='black', label='Posterior')\n",
    "plt.plot([data.mean(), data.mean()], [0, y.max()], \n",
    "          color='black', linestyle='dashed', label='Sample mean')\n",
    "plt.plot([exact_dist.mean(), exact_dist.mean()], [0, y.max() * 1.05], \n",
    "          color='red', linestyle='dashed', label='Exact p')\n",
    "plt.title(\"Posterior Distribution\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(p - 0.1, p + 0.1)\n",
    "plt.gca().xaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b8ff4",
   "metadata": {
    "papermill": {
     "duration": 0.017444,
     "end_time": "2026-01-21T04:25:31.560008",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.542564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the example comparing two groups, $p_A$ is set, and $p_B$ is 5% higher. `nsample` data points are generated for each group, and posterior distributions of the conversions are built. By sampling from these distributions, we estimate the probability that group B's conversion exceeds group A's, $P(p_B > p_A)$. The graph shows $P(p_B > p_A) = 84.0 \\%$. Since nsample is relatively small, the values may change with each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882842d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:31.596542Z",
     "iopub.status.busy": "2026-01-21T04:25:31.596172Z",
     "iopub.status.idle": "2026-01-21T04:25:31.916747Z",
     "shell.execute_reply": "2026-01-21T04:25:31.915400Z"
    },
    "papermill": {
     "duration": 0.341714,
     "end_time": "2026-01-21T04:25:31.919233",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.577519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "p_A = 0.1\n",
    "p_B = p_A * 1.05\n",
    "nsample = 1000\n",
    "\n",
    "exact_dist_A = stats.bernoulli(p=p_A)\n",
    "exact_dist_B = stats.bernoulli(p=p_B)\n",
    "data_A = exact_dist_A.rvs(nsample)\n",
    "data_B = exact_dist_B.rvs(nsample)\n",
    "\n",
    "post_dist_A = posterior_dist_binom(ns=np.sum(data_A), ntotal=len(data_A))\n",
    "post_dist_B = posterior_dist_binom(ns=np.sum(data_B), ntotal=len(data_B))\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.figure()\n",
    "yA = post_dist_A.pdf(x)\n",
    "yB = post_dist_B.pdf(x)\n",
    "plt.plot(x, yA, color='black', label='A')\n",
    "plt.plot(x, yB, color='black', alpha=0.2, label='B')\n",
    "plt.plot([exact_dist_A.mean(), exact_dist_A.mean()], [0, yA.max() * 1.05], \n",
    "          color='black', linestyle='dashed', label='Exact A')\n",
    "plt.plot([exact_dist_B.mean(), exact_dist_B.mean()], [0, yA.max() * 1.05], \n",
    "          color='black', linestyle='dashed', alpha=0.2, label='Exact B')\n",
    "plt.title(\"Posterior Distributions\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(p_A - 0.1, p_A + 0.1)\n",
    "plt.gca().xaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'P(pB > pA): {prob_pb_gt_pa(post_dist_A, post_dist_B)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d088a",
   "metadata": {
    "papermill": {
     "duration": 0.019758,
     "end_time": "2026-01-21T04:25:31.959287",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.939529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The example below shows how conversion estimates and the probability $P(p_B > p_A)$ change as more data is collected. Two groups are compared: Group A has a fixed conversion rate $p_A$, while Group B’s conversion rate $p_B$ is set 5% higher. In each group, `npoints` samples are generated over `nstep` iterations (for a total of `N = npoints * nstep` samples). At each step, posterior distributions are updated based on the accumulated data, and the probability $P(p_B > p_A)$ is calculated. The plot also shows 95% credible intervals for both groups. As the sample size grows, the intervals narrow, and the probability tends toward 1 in favor of the better-performing group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1997134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:31.999166Z",
     "iopub.status.busy": "2026-01-21T04:25:31.998822Z",
     "iopub.status.idle": "2026-01-21T04:25:33.856887Z",
     "shell.execute_reply": "2026-01-21T04:25:33.855872Z"
    },
    "papermill": {
     "duration": 1.880607,
     "end_time": "2026-01-21T04:25:33.858887",
     "exception": false,
     "start_time": "2026-01-21T04:25:31.978280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def posterior_binom_approx_95pdi(post_dist):\n",
    "    lower = post_dist.ppf(0.025)\n",
    "    upper = post_dist.ppf(0.975)\n",
    "    return lower, upper\n",
    "\n",
    "pa = 0.1\n",
    "pb = pa * 1.05\n",
    "\n",
    "npoints = 1000\n",
    "nstep = 150\n",
    "sa = stats.binom.rvs(p=pa, n=npoints, size=nstep)\n",
    "sb = stats.binom.rvs(p=pb, n=npoints, size=nstep)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['npoints'] = [npoints] * nstep\n",
    "df['sa_step'] = sa\n",
    "df['sb_step'] = sb\n",
    "df['N'] = df['npoints'].cumsum()\n",
    "df['sa'] = df['sa_step'].cumsum()\n",
    "df['sb'] = df['sb_step'].cumsum()\n",
    "df['pa'] = df.apply(lambda r: posterior_dist_binom(r['sa'], r['N']).mean(), axis=1)\n",
    "df[['pa_lower', 'pa_upper']] = df.apply(lambda r: posterior_binom_approx_95pdi(posterior_dist_binom(r['sa'], r['N'])), axis=1, result_type=\"expand\")\n",
    "df['pb'] = df.apply(lambda r: posterior_dist_binom(r['sb'], r['N']).mean(), axis=1)\n",
    "df[['pb_lower', 'pb_upper']] = df.apply(lambda r: posterior_binom_approx_95pdi(posterior_dist_binom(r['sb'], r['N'])), axis=1, result_type=\"expand\")\n",
    "df['pb_gt_pa'] = df.apply(lambda r: prob_pb_gt_pa(posterior_dist_binom(r['sa'], r['N']), posterior_dist_binom(r['sb'], r['N']), post_samp=10_000), axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"N\"], df[\"pa\"], color=\"black\", label=\"A\")\n",
    "plt.fill_between(df[\"N\"], df[\"pa_lower\"], df[\"pa_upper\"], color=\"black\", alpha=0.2, label=\"A, 95% PDI\")\n",
    "plt.plot(df[\"N\"], df[\"pb\"], color=\"blue\", label=\"B\")\n",
    "plt.fill_between(df[\"N\"], df[\"pb_lower\"], df[\"pb_upper\"], color=\"blue\", alpha=0.2, label=\"B, 95% PDI\")\n",
    "plt.title(r\"$p_A, p_B$\")\n",
    "plt.xlabel(\"N\")\n",
    "plt.gca().yaxis.set_major_formatter(lambda x, _: f\"{x:.1%}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"N\"], df[\"pb_gt_pa\"], color=\"black\")\n",
    "plt.title(r\"$P(p_B > p_A)$\")\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ff6a5",
   "metadata": {
    "papermill": {
     "duration": 0.021379,
     "end_time": "2026-01-21T04:25:33.902352",
     "exception": false,
     "start_time": "2026-01-21T04:25:33.880973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The accuracy of identifying the better group is demonstrated as follows. Group A is assigned a conversion rate `p`. Group B is assigned a random value within $\\pm 5\\%$ of `p`. Data for both groups is generated in steps of `n_samp_step`. At each step, posterior distributions are updated, and the probability $P(p_B > p_A)$ is calculated. The experiment stops when either $P(p_B > p_A)$ or $P(p_A > p_B)$ reaches the stopping threshold `prob_stop = 0.95`, or when the maximum number of samples `n_samp_max` is reached. The procedure is repeated `nexps` times. The share of correctly identified better groups is calculated across all experiments. In this case, with `nexps = 100`, the correct group was identified 94 times. The resulting accuracy of 0.94 is close to the stopping threshold of `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f33f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:25:33.946847Z",
     "iopub.status.busy": "2026-01-21T04:25:33.946402Z",
     "iopub.status.idle": "2026-01-21T04:27:11.869753Z",
     "shell.execute_reply": "2026-01-21T04:27:11.868684Z"
    },
    "papermill": {
     "duration": 97.947602,
     "end_time": "2026-01-21T04:27:11.871806",
     "exception": false,
     "start_time": "2026-01-21T04:25:33.924204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "\n",
    "p = 0.1\n",
    "nexps = 100\n",
    "cmp['A'] = [p] * nexps\n",
    "cmp['B'] = p * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 30_000_000\n",
    "n_samp_step = 10_000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    pA = cmp.at[i, 'A']\n",
    "    pB = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.bernoulli(p=pA)\n",
    "    exact_dist_B = stats.bernoulli(p=pB)\n",
    "    n_samp_total = 0\n",
    "    ns_A = 0\n",
    "    ns_B = 0\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA = exact_dist_A.rvs(n_samp_step)\n",
    "        dB = exact_dist_B.rvs(n_samp_step)\n",
    "        n_samp_total += n_samp_step\n",
    "        ns_A = ns_A + np.sum(dA)\n",
    "        ns_B = ns_B + np.sum(dB)\n",
    "        post_dist_A = posterior_dist_binom(ns=ns_A, ntotal=n_samp_total)\n",
    "        post_dist_B = posterior_dist_binom(ns=ns_B, ntotal=n_samp_total)\n",
    "        pb_gt_pa = prob_pb_gt_pa(post_dist_A, post_dist_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_dist_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_dist_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(b>a) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9824568",
   "metadata": {
    "papermill": {
     "duration": 0.023923,
     "end_time": "2026-01-21T04:27:11.919837",
     "exception": false,
     "start_time": "2026-01-21T04:27:11.895914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Experiments stop when the specified confidence level is reached. Time estimates and other stopping criteria are discussed in the appendices [[Apx](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab44a1",
   "metadata": {
    "papermill": {
     "duration": 0.023609,
     "end_time": "2026-01-21T04:27:11.967577",
     "exception": false,
     "start_time": "2026-01-21T04:27:11.943968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc3eea",
   "metadata": {
    "papermill": {
     "duration": 0.023581,
     "end_time": "2026-01-21T04:27:12.014656",
     "exception": false,
     "start_time": "2026-01-21T04:27:11.991075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Bayesian approach requires assumptions about the distributions of the quantities being compared. Model selection is always somewhat arbitrary, and its justification is never free of questions. In many cases, a full distribution is unnecessary — comparing means is enough: average revenue per user, average session duration, and so on. For means, the Central Limit Theorem often applies [[CLT](https://en.wikipedia.org/wiki/Central_limit_theorem)]. This allows the normal distribution [[NormDist](https://en.wikipedia.org/wiki/Normal_distribution), [SciPyNorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=norm)] to be used as a likelihood function, even when the shape of the original distribution is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9aee1b",
   "metadata": {
    "papermill": {
     "duration": 0.02394,
     "end_time": "2026-01-21T04:27:12.062073",
     "exception": false,
     "start_time": "2026-01-21T04:27:12.038133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Central Limit Theorem formalizes a simple observation. Take any distribution with mean $\\mu$ and variance $\\sigma^2$. Draw samples of size $N$ and calculate the mean of each sample. The resulting sample means will follow an approximately normal distribution: $Norm(\\mu, \\sigma^2/N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a6173",
   "metadata": {
    "papermill": {
     "duration": 0.02372,
     "end_time": "2026-01-21T04:27:12.109997",
     "exception": false,
     "start_time": "2026-01-21T04:27:12.086277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/figs/en_central_limit_theorem.png?raw=true\" alt=\"Central Limit Theorem\" width=\"800\"/>\n",
    "<br>\n",
    "<em>\n",
    "    Sample means from any distribution with a finite mean and variance are approximately normally distributed.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40eaa15",
   "metadata": {
    "papermill": {
     "duration": 0.023799,
     "end_time": "2026-01-21T04:27:12.157748",
     "exception": false,
     "start_time": "2026-01-21T04:27:12.133949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are several versions of the central limit theorem [[CLT](https://en.wikipedia.org/wiki/Central_limit_theorem)]. One is as follows. Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed random variables with a finite mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$ be the sample mean. As $N$ increases, the distribution of the centered and scaled sample means converges to a normal distribution with mean 0 and variance 1. Convergence here refers to convergence in distribution [[RandVarsConv](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution)].\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P \\left( \\frac{\\overline{X}_N - \\mu}{\\sigma / \\sqrt{N}} = x \\right) & \\to Norm(x; 0, 1), \\quad N \\to \\infty\n",
    "\\\\ \n",
    "Norm(x ; \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\tfrac{(x-\\mu)^2}{2 \\sigma^2} }\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaa4ed",
   "metadata": {
    "papermill": {
     "duration": 0.023733,
     "end_time": "2026-01-21T04:27:12.206003",
     "exception": false,
     "start_time": "2026-01-21T04:27:12.182270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The graph compares the distribution of sample means with the normal distribution based on the central limit theorem. A gamma distribution $P(x; \\alpha, \\beta) \\propto x^{\\alpha-1} \\exp(-\\beta x)$ [[GammaDist](https://en.wikipedia.org/wiki/Gamma_distribution), [SciPyGamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html)] is used to generate `n_sample` samples, each containing `sample_len` points. The means of these samples are computed, and their distribution is plotted. Using the exact mean and variance of the original distribution, the parameters for the normal distribution based on the central limit theorem `clt_mu, clt_stdev` are calculated. This distribution is also shown on the graph. Visually, the distribution of the sample means closely approximates the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa375d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:12.255691Z",
     "iopub.status.busy": "2026-01-21T04:27:12.255316Z",
     "iopub.status.idle": "2026-01-21T04:27:13.306322Z",
     "shell.execute_reply": "2026-01-21T04:27:13.305215Z"
    },
    "papermill": {
     "duration": 1.07888,
     "end_time": "2026-01-21T04:27:13.309023",
     "exception": false,
     "start_time": "2026-01-21T04:27:12.230143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "sample_len = 100\n",
    "n_samples = 3000\n",
    "\n",
    "exact_dist = stats.gamma(a=a)\n",
    "samp = exact_dist.rvs(size=(n_samples, sample_len))\n",
    "means = np.array([x.mean() for x in samp])\n",
    "clt_mu = exact_dist.mean()\n",
    "clt_stdev = exact_dist.std() / np.sqrt(sample_len)\n",
    "means_stdev = means.std()\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='black', linestyle='solid', label='Original distribution')\n",
    "plt.hist(np.concatenate(samp), bins=500, density=True, color='black', alpha=0.3, label='Sample')\n",
    "plt.plot(x, stats.norm.pdf(x, loc=clt_mu, scale=clt_stdev), color='black', linestyle='dashed', label=r'$\\mathrm{Norm}(\\mu, \\sigma^2/N)$')\n",
    "plt.hist(means, bins=50, density=True, color='green', alpha=0.5, label='Sample means')\n",
    "plt.title(\"Sample Means\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, 5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564285b",
   "metadata": {
    "papermill": {
     "duration": 0.024561,
     "end_time": "2026-01-21T04:27:13.359509",
     "exception": false,
     "start_time": "2026-01-21T04:27:13.334948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The central limit theorem states that the distribution of centered and scaled sample means $\\overline{X}_N$ converges to a normal distribution as $N$ approaches infinity. For finite $N$, normality is not guaranteed. The deviation from normality for a finite number of terms is explained by the Berry-Esseen theorem [[BerryEsseenTheorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)]. The difference depends on $N$, the variance, and the skewness of the original distribution. The central limit theorem assumes the original distribution has finite mean and variance. Distributions that may not meet these conditions include the Pareto distribution [[ParetoDist](https://en.wikipedia.org/wiki/Pareto_distribution), [SciPyPareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html)] and the Lomax distribution [[LomaxDist](https://en.wikipedia.org/wiki/Lomax_distribution), [SciPyLomax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lomax.html)]. The probability density function of the Lomax distribution is as follows:\n",
    "\n",
    "$$\n",
    "P(x; c) = \\frac{c}{(1 + x )^{c + 1}}, \\quad x \\ge 0, c > 0.\n",
    "$$\n",
    "\n",
    "For values of the parameter $c$ less than or equal to 2, the variance of the Lomax distribution is infinite.\n",
    "The histograms below show the distribution of `n_samples` sample means, each with `sample_len` terms, along with a normal distribution based on the central limit theorem parameters `clt_mu,clt_stdev`. The distribution of sample means is skewed and deviates more from the normal distribution than in the previous case. This skewness arises from the inclusion of large values from the tail of the original distribution in the samples. In practice, distributions are bounded, so variances and means are finite. The central limit theorem still applies, but a large number of points will be needed to approximate sample means with a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440977f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:13.411366Z",
     "iopub.status.busy": "2026-01-21T04:27:13.410683Z",
     "iopub.status.idle": "2026-01-21T04:27:14.517720Z",
     "shell.execute_reply": "2026-01-21T04:27:14.516797Z"
    },
    "papermill": {
     "duration": 1.137054,
     "end_time": "2026-01-21T04:27:14.521233",
     "exception": false,
     "start_time": "2026-01-21T04:27:13.384179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = 1.7\n",
    "sample_len = 500\n",
    "n_samples = 1000\n",
    "\n",
    "exact_dist = stats.lomax(c=c)\n",
    "samp = exact_dist.rvs(size=(n_samples, sample_len))\n",
    "means = np.array([x.mean() for x in samp])\n",
    "clt_mu = exact_dist.mean()\n",
    "clt_stdev = exact_dist.std() / np.sqrt(sample_len)\n",
    "means_stdev = means.std()\n",
    "\n",
    "xaxis_max=10\n",
    "x = np.linspace(0, xaxis_max, 2000)\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='black', linestyle='solid', label='Original distribution')\n",
    "plt.hist(np.concatenate(samp)[np.concatenate(samp) < xaxis_max], \n",
    "         bins=500, density=True, color='black', alpha=0.3, label='Sample')\n",
    "plt.plot(x, stats.norm.pdf(x, loc=clt_mu, scale=means_stdev), \n",
    "         color='black', linestyle='dashed', label=r'$\\mathrm{Norm}(\\mu, \\sigma_{\\overline{X}_N}^2)$')\n",
    "plt.hist(means, bins=150, density=True, color='green', alpha=0.5, label='Sample means')\n",
    "plt.title(\"Sample Means\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, xaxis_max)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa73565",
   "metadata": {
    "papermill": {
     "duration": 0.025796,
     "end_time": "2026-01-21T04:27:14.580616",
     "exception": false,
     "start_time": "2026-01-21T04:27:14.554820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For Bayesian estimation of the parameters of a normal distribution based on a sample, the likelihood function is given by $P(\\mathcal{D} | \\mathcal{H}) = Norm(x | \\mu, \\sigma_x^2)$. This function has two parameters: $\\mu$ and $\\sigma_x$. For this model, there exists a conjugate prior distribution [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior), [Apx](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices)]. Below is a simplified version of this model is used where only $\\mu$ is adjusted, and $\\sigma_x$ is fixed. The conjugate prior distribution for $\\mu$ is normal $P(\\mu) = Norm(\\mu | \\mu_0, \\sigma_0^2)$, with parameters $\\mu_0$ and $\\sigma_0$. To calculate the posterior distribution, multiply the likelihood functions for all data points $x_i$: $P(\\mathcal{H} | \\mathcal{D}) \\propto \\prod_i^N Norm(x_i | \\mu, \\sigma_x^2) Norm(\\mu | \\mu_0, \\sigma_0^2)$. Only the dependence on $\\mu$ matters in the transformations, as the terms without $\\mu$ will contribute to the normalization constant. The posterior distribution remains normal: $P(\\mu | \\mathcal{D}) = Norm(\\mu | \\mu_N, \\sigma_N^2)$ with updated parameters $\\mu_N$ and $\\sigma_N$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Norm(x | \\mu, \\sigma_x^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_x^2}} e^{-\\tfrac{(x - \\mu)^2}{2 \\sigma_x^2}}\n",
    "\\\\\n",
    "P(\\mathcal{H}) & = Norm(\\mu | \\mu_0, \\sigma_0^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_{0}^2}} e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_{0}^2}} \n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto\n",
    "\\prod_i^N\n",
    "Norm(x_i | \\mu, \\sigma_x^2)\n",
    "Norm(\\mu | \\mu_0, \\sigma_0^2)\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "\\prod_i^N\n",
    "e^{-\\tfrac{(x_i - \\mu)^2}{2 \\sigma_x^2}}\n",
    "e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_0^2}} \n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\mu^2 \\left[\\tfrac{N}{2 \\sigma_x^2} + \\tfrac{1}{2 \\sigma_0^2} \\right] + \n",
    "   2\\mu \\left[\\tfrac{\\mu_0}{2 \\sigma_0^2} + \\tfrac{1}{2 \\sigma_x^2} \\sum_i^N x_i \\right]}\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\tfrac{(\\mu - \\mu_N)^2}{2 \\sigma_N^2}}\n",
    "= Norm(\\mu | \\mu_N, \\sigma_N^2),\n",
    "\\quad\n",
    "\\sigma_N^2 = \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_x^2 + N \\sigma_0^2},\n",
    "\\quad\n",
    "\\mu_N = \\mu_0 \\frac{\\sigma_N^2}{\\sigma_0^2} + \\frac{\\sigma_N^2}{\\sigma_x^2} \\sum_i^N x_i\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e3890",
   "metadata": {
    "papermill": {
     "duration": 0.027313,
     "end_time": "2026-01-21T04:27:14.633637",
     "exception": false,
     "start_time": "2026-01-21T04:27:14.606324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To verify the construction of the posterior distribution from data, a normal distribution with parameters `mu` and `sigma` is defined, and a sample of size `nsample` is generated. The initial parameters $\\sigma_x, \\sigma_0$ are set to the sample’s standard deviation; $\\mu_0$ is set to the first data point. The posterior parameters $\\mu_N, \\sigma_N$ are computed using the remaining data. Using the full dataset to define prior parameters is incorrect — it's better to use a subset or historical data. The first plot compares the posterior distribution of $\\mu$ to the true mean. The mode is close to both the sample mean and the true mean. The second plot compares the posterior predictive distribution of $x$ to the original distribution. To build this distribution, sample $\\mu \\sim Norm(\\mu | \\mu_N, \\sigma_N^2)$, then sample $x \\sim Norm(x | \\mu, \\sigma_x^2)$. The resulting histogram of $x$ visually matches the original normal distribution. The final plot compares the distributions of $x$ and $\\mu$. These are fundamentally different: $P(x|\\mathcal{D})$ approximates the original distribution, while $P(\\mu|\\mathcal{D})$ reflects uncertainty in the mean estimate. The distribution of $\\mu$ is significantly narrower, with variance $\\sigma_N$, while the variance of $P(x|\\mathcal{D})$ depends on both $\\sigma_x$ and the variation in $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26b8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:14.687110Z",
     "iopub.status.busy": "2026-01-21T04:27:14.686787Z",
     "iopub.status.idle": "2026-01-21T04:27:15.891607Z",
     "shell.execute_reply": "2026-01-21T04:27:15.890378Z"
    },
    "papermill": {
     "duration": 1.234404,
     "end_time": "2026-01-21T04:27:15.893722",
     "exception": false,
     "start_time": "2026-01-21T04:27:14.659318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ConjugateNormalParams = namedtuple('ConjugateNormalParams', 'mu sigma sx')\n",
    "\n",
    "def initial_params_normal(mu, sigma, sx):\n",
    "    return ConjugateNormalParams(mu=mu, sigma=sigma, sx=sx)\n",
    "\n",
    "def posterior_params_normal(data, initial_pars):\n",
    "    N = len(data)\n",
    "    sigma_n_2 = (initial_pars.sigma**2 * initial_pars.sx**2) / (initial_pars.sx**2 + N * initial_pars.sigma**2)\n",
    "    mu_n = initial_pars.mu * sigma_n_2 / initial_pars.sigma**2 + np.sum(data) * sigma_n_2 / initial_pars.sx**2    \n",
    "    return ConjugateNormalParams(mu=mu_n, sigma=np.sqrt(sigma_n_2), sx=initial_pars.sx)\n",
    "\n",
    "def posterior_mu_dist(params):\n",
    "    return stats.norm(loc=params.mu, scale=params.sigma)\n",
    "\n",
    "def posterior_rvs(params, nsamp):\n",
    "    mus = stats.norm.rvs(loc=params.mu, scale=params.sigma, size=nsamp)\n",
    "    return stats.norm.rvs(loc=mus, scale=params.sx, size=nsamp)\n",
    "\n",
    "mu = 3\n",
    "sigma = 1\n",
    "nsample = 1000\n",
    "npostsamp = 100000\n",
    "\n",
    "exact_dist = stats.norm(loc=mu, scale=sigma)\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "sx = np.std(data)\n",
    "mu0 = data[0]\n",
    "sigma0 = np.std(data)\n",
    "pars = initial_params_normal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_normal(data[1:], pars)\n",
    "post_mu = posterior_mu_dist(pars)\n",
    "post_samp = posterior_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.figure()\n",
    "y = post_mu.pdf(x)\n",
    "plt.plot(x, y, color='black', label=r'Posterior $\\mu$')\n",
    "plt.plot([data.mean(), data.mean()], [0, y.max()], \n",
    "          color='black', linestyle='dashed', label='Sample mean')\n",
    "plt.plot([exact_dist.mean(), exact_dist.mean()], [0, y.max() * 1.05], \n",
    "          color='red', linestyle='dashed', label='Exact mean')\n",
    "plt.title(r'Posterior Distribution $\\mu$')\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(2, 4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='red', linestyle='dashed', label='Exact distribution')\n",
    "plt.hist(post_samp, bins=100, density=True, color='black', alpha=0.8, label='Posterior sample x')\n",
    "plt.title(\"Posterior Sample x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, post_mu.pdf(x), color='black', label=r'Posterior $\\mu$')\n",
    "plt.plot(x, exact_dist.pdf(x), color='red', linestyle='dashed', label='Exact distribution')\n",
    "plt.hist(post_samp, bins=100, density=True, color='black', alpha=0.8, label='Posterior sample x')\n",
    "plt.title(r'Distributions of $x$ and $\\mu$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(0, 6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b84832",
   "metadata": {
    "papermill": {
     "duration": 0.02877,
     "end_time": "2026-01-21T04:27:15.953644",
     "exception": false,
     "start_time": "2026-01-21T04:27:15.924874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The estimation of a mean from an arbitrary distribution is illustrated using the gamma distribution. A sample of `nsample` points is divided into segments of `nsplit` points. The mean is computed for each segment. These sample means `means`, are assumed to follow a normal distribution and are modeled using Bayesian inference. The initial parameters, $\\sigma_x$ and $\\sigma_0$, are set to the standard deviation of the sample means `sx = np.std(means)`, and $\\mu_0$ is set to the first sample mean `mu0 = means[0]`. The segment size `nsplit = 100` is chosen arbitrarily; the Berry–Esseen theorem can guide a more accurate choice. Using the full sample mean instead is possible, but this would leave only one point for estimating $P(\\mu|\\mathcal{D})$, making model validation infeasible. It's better to treat nsplit as a model hyperparameter. The first plot shows the original distribution and the distribution of the sample means, which is roughly normal. The second plot displays the posterior distribution of $\\mu$, whose mode is close to the sample and true means. The third plot shows the posterior predictive distribution of the sample means. It resembles a normal distribution with parameters from the central limit theorem. As before, the posterior distribution of $\\mu$ is narrower than that of the sample means. For mean comparison, focus on the distribution of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dddb6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:16.012105Z",
     "iopub.status.busy": "2026-01-21T04:27:16.011770Z",
     "iopub.status.idle": "2026-01-21T04:27:17.329044Z",
     "shell.execute_reply": "2026-01-21T04:27:17.328097Z"
    },
    "papermill": {
     "duration": 1.350141,
     "end_time": "2026-01-21T04:27:17.331892",
     "exception": false,
     "start_time": "2026-01-21T04:27:15.981751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_and_compute_means(sample, n_split):\n",
    "    n_means = len(sample) // n_split\n",
    "    samp_reshaped = np.reshape(sample[0 : n_means * n_split], (n_means, n_split))\n",
    "    means = np.array([x.mean() for x in samp_reshaped])\n",
    "    return means\n",
    "\n",
    "def exact_clt_dist(exact_dist, n_split):\n",
    "    clt_mu = exact_dist.mean()\n",
    "    clt_stdev = exact_dist.std() / np.sqrt(n_split)\n",
    "    return stats.norm(loc=clt_mu, scale=clt_stdev)\n",
    "\n",
    "def sample_clt_dist(means):\n",
    "    clt_mu = means.mean()\n",
    "    clt_std = means.std()\n",
    "    return stats.norm(loc=clt_mu, scale=clt_std)\n",
    "\n",
    "nsample = 50000\n",
    "nsplit = 100\n",
    "npostsamp = 50000\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "exact_dist = stats.gamma(a=a, scale=1/b)\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "means = reshape_and_compute_means(data, nsplit)\n",
    "clt_dist_exact = exact_clt_dist(exact_dist, nsplit)\n",
    "\n",
    "sx = np.std(means)\n",
    "mu0 = means[0]\n",
    "sigma0 = sx\n",
    "pars = initial_params_normal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_normal(means[1:], pars)\n",
    "post_mu = posterior_mu_dist(pars)\n",
    "post_samp = posterior_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='black', linestyle='solid', label='Original distribution')\n",
    "plt.plot(x, clt_dist_exact.pdf(x), color='black', linestyle='dashed', label=r'$Norm(\\mu, \\sigma^2/N)$')\n",
    "plt.hist(means, bins=50, density=True, color='green', alpha=0.5, label='Sample means')\n",
    "plt.title(\"Sample Means\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, 3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 4, 10000)\n",
    "plt.figure()\n",
    "y = post_mu.pdf(x)\n",
    "plt.plot(x, y, color='black', label=r'$\\mu$ distribution')\n",
    "plt.plot([data.mean(), data.mean()], [0, y.max()], \n",
    "          color='black', linestyle='dashed', label='Sample mean')\n",
    "plt.plot([exact_dist.mean(), exact_dist.mean()], [0, y.max() * 1.05], \n",
    "          color='red', linestyle='dashed', label='Exact mean')\n",
    "plt.title(r'Distribution of $\\mu$')\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(exact_dist.mean() - 0.1, exact_dist.mean() + 0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='black', linestyle='solid', label='Original distribution')\n",
    "plt.plot(x, clt_dist_exact.pdf(x), color='black', linestyle='dashed', label=r'$Norm(\\mu, \\sigma^2/N)$')\n",
    "plt.hist(post_samp, bins=300, density=True, color='black', alpha=0.2, label=r'Posterior $\\overline{X}_N$')\n",
    "plt.title(r'Posterior Distribution $\\overline{X}_N$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(0, 3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92f972",
   "metadata": {
    "papermill": {
     "duration": 0.032371,
     "end_time": "2026-01-21T04:27:17.398707",
     "exception": false,
     "start_time": "2026-01-21T04:27:17.366336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the group comparison example, two gamma distributions are defined. The parameters $a$ are identical, while the parameter $b$ for group B is 5% smaller than that of group A. A sample of size nsample is drawn from each, and sample means are calculated. The posterior distributions of $\\mu$ are constructed based on the sample means. From these distributions, the probability that the mean for group B is greater than for group A, $P(\\mu_B > \\mu_A)$, is computed. In the first plot, the original distributions and exact means are shown. In the second plot, the distributions of $\\mu$ and the exact means are shown. With the chosen samples, the distributions $P(\\mu|\\mathcal{D})$ barely overlap, and $P(\\mu_B > \\mu_A) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddca63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:17.463739Z",
     "iopub.status.busy": "2026-01-21T04:27:17.463307Z",
     "iopub.status.idle": "2026-01-21T04:27:18.083932Z",
     "shell.execute_reply": "2026-01-21T04:27:18.082744Z"
    },
    "papermill": {
     "duration": 0.655952,
     "end_time": "2026-01-21T04:27:18.085945",
     "exception": false,
     "start_time": "2026-01-21T04:27:17.429993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "nsample = 30000\n",
    "npostsamp = 50000\n",
    "nsplit = 100\n",
    "\n",
    "A, B = {}, {}\n",
    "A['dist_pars'] = {'a': 1, 'b': 2}\n",
    "B['dist_pars'] = {'a': 1, 'b': 2*0.95}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.gamma(a=g['dist_pars']['a'], scale=1/g['dist_pars']['b'])\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['means'] = reshape_and_compute_means(g['data'], nsplit)\n",
    "    g['post_pars'] = initial_params_normal(mu=g['means'][0], sigma=np.std(g['means']), sx=np.std(g['means']))\n",
    "    g['post_pars'] = posterior_params_normal(g['means'][1:], g['post_pars'])\n",
    "    g['post_mu'] = posterior_mu_dist(g['post_pars'])\n",
    "    g['post_samp'] = posterior_rvs(g['post_pars'], npostsamp)\n",
    "\n",
    "x = np.linspace(0, 3, 5000)\n",
    "plt.figure()\n",
    "yA = A[\"exact_dist\"].pdf(x)\n",
    "yB = B[\"exact_dist\"].pdf(x)\n",
    "plt.plot(x, yA, color=\"black\", alpha=0.2, label=\"A\")\n",
    "plt.plot(x, yB, color=\"black\", label=\"B\")\n",
    "plt.plot([A[\"exact_dist\"].mean(), A[\"exact_dist\"].mean()], [0, yA.max() * 1.05], \n",
    "          color=\"black\", linestyle=\"dashed\", alpha=0.2, label=\"Exact mean A\")\n",
    "plt.plot([B[\"exact_dist\"].mean(), B[\"exact_dist\"].mean()], [0, yB.max() * 1.05], \n",
    "          color=\"black\", linestyle=\"dashed\", label=\"Exact mean B\")\n",
    "plt.title(\"Original Distributions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(0, 3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "yA = A[\"post_mu\"].pdf(x)\n",
    "yB = B[\"post_mu\"].pdf(x)\n",
    "plt.plot(x, yA, color=\"black\", alpha=0.2, label=\"A\")\n",
    "plt.plot(x, yB, color=\"black\", label=\"B\")\n",
    "plt.plot([A[\"exact_dist\"].mean(), A[\"exact_dist\"].mean()], [0, yA.max() * 1.05],\n",
    "         color=\"black\", linestyle=\"dashed\", alpha=0.2, label=\"Exact mean A\")\n",
    "plt.plot([B[\"exact_dist\"].mean(), B[\"exact_dist\"].mean()], [0, yB.max() * 1.05],\n",
    "         color=\"black\", linestyle=\"dashed\", label=\"Exact mean B\")\n",
    "plt.title(r\"$\\mathrm{Distributions\\ of\\ }\\mu$\")\n",
    "plt.xlabel(r\"$\\mu$\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlim(A[\"exact_dist\"].mean() - 0.1, A[\"exact_dist\"].mean() + 0.1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(mu_B > mu_A): {prob_pb_gt_pa(A['post_mu'], B['post_mu'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e7c36",
   "metadata": {
    "papermill": {
     "duration": 0.033678,
     "end_time": "2026-01-21T04:27:18.155895",
     "exception": false,
     "start_time": "2026-01-21T04:27:18.122217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To demonstrate the proportion of correctly identified options, two groups with gamma distributions are set up. In group A, the parameters of the gamma distribution are fixed, while in group B, the parameter bb changes within $\\pm5\\%$ of group A. Along with the parameters, the means change. Data are generated from the distributions with a step size of `n_samp_step`. At each step, sample means `nsplit` are calculated. The parameters $\\mu$ are estimated based on the sample means. These distributions of parameters are compared. The data collection stops when either $P(\\mu_B > \\mu_A)$ or $P(\\mu_A > \\mu_B)$ reaches `prob_stop = 0.95`, or the maximum number of points `n_samp_max` is reached. A total of `nexps` experiments are conducted, and the proportion of correctly identified groups with the larger mean is calculated. In this example, the proportion of 0.97 is close to `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492efbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:18.224788Z",
     "iopub.status.busy": "2026-01-21T04:27:18.224273Z",
     "iopub.status.idle": "2026-01-21T04:27:54.455059Z",
     "shell.execute_reply": "2026-01-21T04:27:54.453659Z"
    },
    "papermill": {
     "duration": 36.317336,
     "end_time": "2026-01-21T04:27:54.506699",
     "exception": false,
     "start_time": "2026-01-21T04:27:18.189363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nexps = 100\n",
    "prob_stop = 0.95\n",
    "nsplit = 100\n",
    "n_samp_max = 1_000_000\n",
    "n_samp_step = 10_000\n",
    "\n",
    "A = {'a': 1, 'b': 2}\n",
    "\n",
    "cmp = pd.DataFrame(columns=['A_pars', 'B_pars', 'A_mean', 'B_mean', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "cmp['A_pars'] = [A] * nexps\n",
    "cmp['B_pars'] = cmp['A_pars'].apply(lambda x: {'a': x['a'], 'b': x['b'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1))})\n",
    "cmp['A_mean'] = cmp['A_pars'].apply(lambda x: stats.gamma(a=x['a'], scale=1/x['b']).mean())\n",
    "cmp['B_mean'] = cmp['B_pars'].apply(lambda x: stats.gamma(a=x['a'], scale=1/x['b']).mean())\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B_mean'] > r['A_mean'] else 'A', axis=1)\n",
    "\n",
    "for i in range(nexps):\n",
    "    A_pars = cmp.at[i, 'A_pars']\n",
    "    B_pars = cmp.at[i, 'B_pars']\n",
    "    exact_dist_A = stats.gamma(a=A_pars['a'], scale=1/A_pars['b'])\n",
    "    exact_dist_B = stats.gamma(a=B_pars['a'], scale=1/B_pars['b'])\n",
    "    n_samp_total = 0\n",
    "    dA = []\n",
    "    dB = []\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA.extend(exact_dist_A.rvs(n_samp_step))\n",
    "        dB.extend(exact_dist_B.rvs(n_samp_step))\n",
    "        n_samp_total += n_samp_step\n",
    "        means_A = reshape_and_compute_means(dA, nsplit)\n",
    "        post_pars_A = initial_params_normal(mu=means_A[0], sigma=np.std(means_A), sx=np.std(means_A))\n",
    "        post_pars_A = posterior_params_normal(means_A[1:], post_pars_A)\n",
    "        post_mu_A = posterior_mu_dist(post_pars_A)\n",
    "        means_B = reshape_and_compute_means(dB, nsplit)\n",
    "        post_pars_B = initial_params_normal(mu=means_B[0], sigma=np.std(means_B), sx=np.std(means_B))\n",
    "        post_pars_B = posterior_params_normal(means_B[1:], post_pars_B)\n",
    "        post_mu_B = posterior_mu_dist(post_pars_B)\n",
    "        pb_gt_pa = prob_pb_gt_pa(post_mu_A, post_mu_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_mu_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_mu_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(B>A) {pb_gt_pa}')\n",
    "\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(8))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e49a04",
   "metadata": {
    "papermill": {
     "duration": 0.036009,
     "end_time": "2026-01-21T04:27:54.585854",
     "exception": false,
     "start_time": "2026-01-21T04:27:54.549845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transactional Revenue per User"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e199c89",
   "metadata": {
    "papermill": {
     "duration": 0.035845,
     "end_time": "2026-01-21T04:27:54.657043",
     "exception": false,
     "start_time": "2026-01-21T04:27:54.621198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To estimate the monetary effect, the revenue per user in the groups $P_{\\text{users}}(x)$ is compared. It is useful to separate the revenue for paying users, denoted as $P_{\\text{paying}}(x)$. With a conversion rate $p$, the distribution of non-zero revenue per user is $pP_{\\text{paying}}(x)$, and with probability $1−p$, the revenue is zero.\n",
    "\n",
    "$$\n",
    "P_{\\text{users}}(x) = \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p P_{\\text{paying}}(x), \\, x > 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848130e",
   "metadata": {
    "papermill": {
     "duration": 0.035619,
     "end_time": "2026-01-21T04:27:54.728608",
     "exception": false,
     "start_time": "2026-01-21T04:27:54.692989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The conversion rate $p$ was estimated earlier. The revenue per paying user can be modeled using a log-normal distribution [[LognormDist](https://en.wikipedia.org/wiki/Log-normal_distribution),\n",
    "[SciPyLognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html)] or a Pareto distribution [[ParetoDist](https://en.wikipedia.org/wiki/Pareto_distribution), [SciPyPareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html)], similar to wealth distribution. For transactional services, particularly marketplaces, the log-normal distribution is more appropriate. A random variable $X$ is log-normal $X \\sim Lognormal(\\mu, s^2)$ if the logarithm of $X$ is normally distributed $\\ln(X) \\sim Norm(\\mu, s^2)$. The probability density function is provided below.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(x) & = \\frac{1}{x s \\sqrt{2 \\pi}} e^{-\\tfrac{(\\ln(x) - \\mu)^2}{2 s^2}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bde834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:54.803240Z",
     "iopub.status.busy": "2026-01-21T04:27:54.802901Z",
     "iopub.status.idle": "2026-01-21T04:27:55.090937Z",
     "shell.execute_reply": "2026-01-21T04:27:55.090150Z"
    },
    "papermill": {
     "duration": 0.327777,
     "end_time": "2026-01-21T04:27:55.093784",
     "exception": false,
     "start_time": "2026-01-21T04:27:54.766007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 20, 2000)\n",
    "plt.figure()\n",
    "mu, s = 1, 1\n",
    "plt.plot(x,stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), color=\"black\", label=r\"$\\mu=1,\\,s=1$\")\n",
    "mu, s = 2, 1\n",
    "plt.plot(x,stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), color=\"black\", label=r\"$\\mu=2,\\,s=1$\")\n",
    "mu, s = 1, 2\n",
    "plt.plot(x,stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), color=\"black\", label=r\"$\\mu=1,\\,s=2$\")\n",
    "plt.text(2.95, 0.25, r\"$\\mu=1,\\,s=1$\", ha='center', va='center', fontsize=9)\n",
    "plt.text(11.2, 0.07, r\"$\\mu=2,\\,s=1$\", ha='center', va='center', fontsize=9)\n",
    "plt.text(1.80, 0.48, r\"$\\mu=1,\\,s=2$\", ha='center', va='center', fontsize=9)\n",
    "plt.title(\"Log-normal Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a50bf",
   "metadata": {
    "papermill": {
     "duration": 0.035355,
     "end_time": "2026-01-21T04:27:55.166893",
     "exception": false,
     "start_time": "2026-01-21T04:27:55.131538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The conjugate prior distribution for the log-normal likelihood function $P(\\mathcal{D} | \\mathcal{H}) = Lognorm(x | \\mu, s^2)$ is constructed similarly to that of the normal distribution [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior)]. In a simplified model where only the parameter $\\mu$ is estimated and the scale parameter $s$ is fixed, the conjugate prior for $\\mu$ is a normal distribution $P(\\mu) = Norm(\\mu | \\mu_0, \\sigma_0^2)$ with parameters $\\mu_0$ and $\\sigma_0$. The resulting posterior is also a normal distribution $P(\\mu | \\mathcal{D}) = Norm(\\mu | \\mu_N, \\sigma_N^2)$, with updated parameters $\\mu_N$ and $\\sigma_N$. The posterior mean $\\mu_N$ is computed using the logarithms of the observed sample values.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Lognorm(x | \\mu, s^2) = \n",
    "\\frac{1}{x \\sqrt{2 \\pi s^2}} e^{-\\tfrac{(\\ln x - \\mu)^2}{2 s^2}}\n",
    "\\\\\n",
    "P(\\mathcal{H}) & = Norm(\\mu | \\mu_0, \\sigma_0^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_{0}^2}} e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_{0}^2}} \n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto\n",
    "\\prod_i^N\n",
    "Lognorm(x_i | \\mu, s^2)\n",
    "Norm(\\mu | \\mu_0, \\sigma_0^2)\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "\\prod_i^N\n",
    "e^{-\\tfrac{(\\ln x_i - \\mu)^2}{2 s^2}}\n",
    "e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_0^2}} \n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\mu^2 \\left[\\tfrac{N}{2 s^2} + \\tfrac{1}{2 \\sigma_0^2} \\right] + \n",
    "   2\\mu \\left[\\tfrac{\\mu_0}{2 \\sigma_0^2} + \\tfrac{1}{2 s^2} \\sum_i^N \\ln x_i \\right]}\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\tfrac{(\\mu - \\mu_N)^2}{2 \\sigma_N^2}}\n",
    "= Norm(\\mu | \\mu_N, \\sigma_N^2),\n",
    "\\quad\n",
    "\\sigma_N^2 = \\frac{\\sigma_0^2 s^2}{s^2 + N \\sigma_0^2},\n",
    "\\quad\n",
    "\\mu_N = \\mu_0 \\frac{\\sigma_N^2}{\\sigma_0^2} + \\frac{\\sigma_N^2}{s^2} \\sum_i^N \\ln x_i\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d00aa",
   "metadata": {
    "papermill": {
     "duration": 0.035062,
     "end_time": "2026-01-21T04:27:55.236790",
     "exception": false,
     "start_time": "2026-01-21T04:27:55.201728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To illustrate posterior inference from a log-normal distribution, a sample of size `nsample` is generated using parameters `mu` and `s`. The logarithm of the sample values is computed. The parameters $s$ and $\\sigma_0$ are set equal to the standard deviation of the log-transformed sample, and $\\mu_0$ is initialized as the value of the first point. The remaining points are used to compute the posterior parameters $\\mu_N$ and $\\sigma_N$. The posterior distribution of $\\mu$ is shown in the first plot. The mean of a log-normal distribution is given by $E[x] = \\exp(\\mu + s^2/2)$, so $\\mu + s^2/2$ should estimate the logarithm of the true mean. Since $\\mu$ has normal distribution $P(\\mu) = Norm(\\mu_N, \\sigma_N^2)$, the expression $\\mu + s^2/2$ is also normally distributed as $Norm(\\mu_N + s^2/2, \\sigma_N^2)$. In the first plot, the mode of the distribution $\\mu + s^2/2$ is close to the logarithm of the sample mean and the exact mean. The second plot compares the posterior predictive distribution of $x$ with the original distribution. The histogram of predicted $x$ values closely matches the original log-normal distribution.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(x) & = Lognorm(x | \\mu, s^2)\n",
    "\\\\\n",
    "E[x] & = e^{\\mu + s^2/2}\n",
    "\\\\\n",
    "P(\\mu) & = Norm(\\mu | \\mu_N, \\sigma_N^2)\n",
    "\\\\\n",
    "P_{\\mu + s^2/2}(y) & = Norm(y | \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773c62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:55.309840Z",
     "iopub.status.busy": "2026-01-21T04:27:55.309508Z",
     "iopub.status.idle": "2026-01-21T04:27:56.069824Z",
     "shell.execute_reply": "2026-01-21T04:27:56.068769Z"
    },
    "papermill": {
     "duration": 0.799535,
     "end_time": "2026-01-21T04:27:56.071800",
     "exception": false,
     "start_time": "2026-01-21T04:27:55.272265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ConjugateLognormalParams = namedtuple('ConjugateLognormalParams', 'mu sigma sx')\n",
    "\n",
    "def initial_params_lognormal(mu, sigma, sx):\n",
    "    return ConjugateLognormalParams(mu=mu, sigma=sigma, sx=sx)\n",
    "\n",
    "def posterior_params_lognormal(data, initial_pars):\n",
    "    N = len(data)\n",
    "    lnx = np.log(data)\n",
    "    sigma_n_2 = (initial_pars.sigma**2 * initial_pars.sx**2) / (initial_pars.sx**2 + N * initial_pars.sigma**2)\n",
    "    mu_n = initial_pars.mu * sigma_n_2 / initial_pars.sigma**2 + np.sum(lnx) * sigma_n_2 / initial_pars.sx**2    \n",
    "    return ConjugateLognormalParams(mu=mu_n, sigma=np.sqrt(sigma_n_2), sx=initial_pars.sx)\n",
    "\n",
    "def posterior_mu_dist_lognormal(params):\n",
    "    return stats.norm(loc=params.mu, scale=params.sigma)\n",
    "\n",
    "def posterior_lognormal_rvs(params, nsamp):\n",
    "    mus = stats.norm.rvs(loc=params.mu, scale=params.sigma, size=nsamp)\n",
    "    return stats.lognorm.rvs(s=params.sx, scale=np.exp(mus), size=nsamp)\n",
    "\n",
    "def posterior_mean_dist_lognormal(params):\n",
    "    return stats.lognorm(scale=np.exp(params.mu + params.sx**2/2), s=params.sigma)\n",
    "\n",
    "def posterior_ln_mean_dist_lognormal(params):\n",
    "    return stats.norm(loc=params.mu + params.sx**2/2, scale=params.sigma)\n",
    "    \n",
    "s = 1\n",
    "mu = 1.5\n",
    "nsample = 1000\n",
    "\n",
    "exact_dist = stats.lognorm(s=s, scale=np.exp(mu))\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "lnx = np.log(data)\n",
    "sx = np.std(lnx)\n",
    "mu0 = lnx[0]\n",
    "sigma0 = sx\n",
    "\n",
    "pars = initial_params_lognormal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_lognormal(data[1:], pars)\n",
    "post_mu = posterior_mu_dist_lognormal(pars)\n",
    "post_lnmeans = posterior_ln_mean_dist_lognormal(pars)\n",
    "npostsamp = 10000\n",
    "post_samp = posterior_lognormal_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "plt.figure()\n",
    "y_mu = post_mu.pdf(x)\n",
    "y_ln = post_lnmeans.pdf(x)\n",
    "plt.plot(x, y_mu, color='black', label=r'$\\mu$ distribution')\n",
    "plt.plot(x, y_ln, color='black', alpha=0.2, label=r'$\\mu + s^2/2$ distribution')\n",
    "plt.plot([np.log(data.mean()), np.log(data.mean())], [0, y_mu.max()], \n",
    "          color='black', linestyle='dashed', label='Logarithm of sample mean')\n",
    "plt.plot([np.log(exact_dist.mean()), np.log(exact_dist.mean())], [0, y_mu.max()*1.05], color='red', \n",
    "          linestyle='dashed', label='Logarithm of exact mean')\n",
    "plt.title(r'Posterior Distributions of $\\mu$ and $\\mu + s^2/2$')\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "xaxis_max=20\n",
    "x = np.linspace(0, xaxis_max, 10000)\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pdf(x), color='red', linestyle='dashed', label='Exact distribution')\n",
    "plt.hist(post_samp[post_samp < xaxis_max], bins=100, density=True, color='black', alpha=0.8, label='Posterior sample')\n",
    "plt.title(r'Posterior Distribution $x$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069befc9",
   "metadata": {
    "papermill": {
     "duration": 0.038306,
     "end_time": "2026-01-21T04:27:56.151997",
     "exception": false,
     "start_time": "2026-01-21T04:27:56.113691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To compare groups by expected revenue per paying user, use $E[x]=\\exp(\\mu + s^2/2)$. It's sufficient to compare $\\mu + s^2/2$. This quantity is normally distributed as $Norm(\\mu + s^2/2 | \\mu_N, \\sigma_N)$. In this example, two log-normal distributions are defined: one with parameters `s, mu`, and the other with `mu` increased by 5\\%. A sample of `nsample` points is generated. Posterior distributions are constructed. The probability that the expected revenue in group B exceeds that of group A, $P(E[x]_B > E[x]_A)$, is close to 1. The first plot shows the original distributions and their exact means. The second shows the distributions $Norm(\\mu + s^2/2 | \\mu_N, \\sigma_N)$ for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801ebb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:56.230427Z",
     "iopub.status.busy": "2026-01-21T04:27:56.229857Z",
     "iopub.status.idle": "2026-01-21T04:27:56.813604Z",
     "shell.execute_reply": "2026-01-21T04:27:56.812647Z"
    },
    "papermill": {
     "duration": 0.625585,
     "end_time": "2026-01-21T04:27:56.815603",
     "exception": false,
     "start_time": "2026-01-21T04:27:56.190018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "nsample = 3000\n",
    "npostsamp = 50000\n",
    "\n",
    "A, B = {}, {}\n",
    "s = 1\n",
    "mu = 2\n",
    "A['dist_pars'] = {'s': s, 'scale': np.exp(mu)}\n",
    "B['dist_pars'] = {'s': s, 'scale': np.exp(mu * 1.05)}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.lognorm(s=g['dist_pars']['s'], scale=g['dist_pars']['scale'])\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['post_pars'] = initial_params_lognormal(mu=np.log(g['data'])[0], sigma=np.std(np.log(g['data'])), sx=np.std(np.log(g['data'])))\n",
    "    g['post_pars'] = posterior_params_lognormal(g['data'][1:], g['post_pars'])\n",
    "    g['post_ln_means_dist'] = posterior_ln_mean_dist_lognormal(g['post_pars'])\n",
    "    \n",
    "x = np.linspace(0, 30, 1000)\n",
    "plt.figure()\n",
    "yA = A[\"exact_dist\"].pdf(x)\n",
    "yB = B[\"exact_dist\"].pdf(x)\n",
    "plt.plot(x, yA, color=\"black\", alpha=0.2, label=\"Original A\")\n",
    "plt.plot(x, yB, color=\"black\", label=\"Original B\")\n",
    "plt.plot([A[\"exact_dist\"].mean(), A[\"exact_dist\"].mean()], [0, yA.max() * 1.05], \n",
    "          color=\"red\", linestyle=\"dashed\", alpha=0.2, label=\"Exact mean A\")\n",
    "plt.plot([B[\"exact_dist\"].mean(), B[\"exact_dist\"].mean()], [0, yB.max() * 1.05], \n",
    "          color=\"red\", linestyle=\"dashed\", label=\"Exact mean B\")\n",
    "plt.title(\"Original Distributions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 3, 1000)\n",
    "plt.figure()\n",
    "yA = A['post_ln_means_dist'].pdf(x)\n",
    "yB = B['post_ln_means_dist'].pdf(x)\n",
    "plt.plot(x, yA, color='black', alpha=0.2, label='A')\n",
    "plt.plot(x, yB, color='black', label='B')\n",
    "plt.plot([np.log(A['exact_dist'].mean()), np.log(A['exact_dist'].mean())], [0, yA.max()*1.05], \n",
    "         color='red', linestyle='dashed', alpha=0.3, label='Logarithm of exact mean A')\n",
    "plt.plot([np.log(B['exact_dist'].mean()), np.log(B['exact_dist'].mean())], [0, yB.max()*1.05], \n",
    "         color='red', linestyle='dashed', label='Logarithm of exact mean B')\n",
    "plt.title(r'Distribution of $\\mu + s^2/2$')\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(2, 3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(E[x]_B > E[x]_A): {prob_pb_gt_pa(A['post_ln_means_dist'], B['post_ln_means_dist'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5eb8ad",
   "metadata": {
    "papermill": {
     "duration": 0.041682,
     "end_time": "2026-01-21T04:27:56.901256",
     "exception": false,
     "start_time": "2026-01-21T04:27:56.859574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The share of correctly identified winning groups is evaluated for user revenue distributions, $P_{\\text{users}}(x)$. Group A is assigned a fixed conversion rate $p$ and parameters $\\mu, s$ for revenue per paying user. For group B, $p$ and $\\mu$ are randomly varied within $\\pm 5\\%$ of A’s values. These parameters are changed independently, though in practice they often shift together in opposite directions. Groups are compared by expected user revenue $E_{\\text{users}}[x] = p \\exp(\\mu + s^2/2)$. The conversion rate $p$ is estimated using a Beta distribution: $P(p) = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)$, where $N$ is the total number of users, and $n_s$ is the number of paying users. Revenue per paying user is modeled with a log-normal distribution. Since $\\mu + s^2/2$ is normally distributed, the expected revenue $\\exp(\\mu + s^2/2)$ follows a log-normal distribution $P_{\\exp(\\mu + s^2/2)}(y) = Lognorm(y | \\mu_N + s^2/2, \\sigma_N^2)$. Thus, the distribution of $p\\exp(\\mu + s^2/2)$ is modeled as the product of a Beta and a log-normal distribution $P_{p\\exp(\\mu + s^2/2)} \\sim \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s) Lognorm(y ; \\mu_N + s^2/2, \\sigma_N^2)$. In each experiment, data is added in increments of `n_samp_step`. The experiment stops when the probability that one group’s mean exceeds the other reaches `prob_stop`, or when the sample size hits `n_samp_max`. If `n_samp_step` is small, the proportion of correctly identified groups may fall short of `prob_stop` due to model inaccuracies or outliers. With a sufficiently large `n_samp_step`, the share of correctly identified groups aligns with the expected `prob_stop`.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P_{\\text{users}}(x) & = \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p P_{\\text{paying}}(x), \\, x > 0\n",
    "\\end{cases}\n",
    "= \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p Lognorm(x | s, \\mu_N, \\sigma_N), \\, x > 0\n",
    "\\end{cases}\n",
    "\\\\\n",
    "E_{\\text{users}}[x] & = p e^{\\mu + s^2/2}\n",
    "\\\\\n",
    "P(p) & = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s),\n",
    "\\\\\n",
    "P_{\\exp(\\mu + s^2/2)}(y) & = Lognorm(y | \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\\\\n",
    "P_{p\\exp(\\mu + s^2/2)} & \\sim \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s) Lognorm(y ; \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235320b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:27:56.985166Z",
     "iopub.status.busy": "2026-01-21T04:27:56.984663Z",
     "iopub.status.idle": "2026-01-21T04:28:27.624238Z",
     "shell.execute_reply": "2026-01-21T04:28:27.623355Z"
    },
    "papermill": {
     "duration": 30.684223,
     "end_time": "2026-01-21T04:28:27.626432",
     "exception": false,
     "start_time": "2026-01-21T04:27:56.942209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ConjugateRevPerUserParams = namedtuple('ConjugateRevPerUserParams', 'a b mu sigma sx')\n",
    "\n",
    "def posterior_params_rev_per_user(data):\n",
    "    d_paying = data[data != 0]\n",
    "    d_paying_total = len(d_paying)\n",
    "    d_total = len(data)\n",
    "    a, b = posterior_params_binom(ns=d_paying_total, ntotal=d_total)\n",
    "    post_pars = initial_params_lognormal(mu=np.log(d_paying)[0], sigma=np.std(np.log(d_paying)), sx=np.std(np.log(d_paying)))\n",
    "    post_pars = posterior_params_lognormal(d_paying[1:], post_pars)\n",
    "    return ConjugateRevPerUserParams(a=a, b=b, mu=post_pars.mu, sigma=post_pars.sigma, sx=post_pars.sx)\n",
    "\n",
    "def posterior_params_binom(ns, ntotal, a_prior=1, b_prior=1):\n",
    "    a = a_prior + ns\n",
    "    b = b_prior + ntotal - ns\n",
    "    return a, b\n",
    "\n",
    "def rev_per_user_p_dist(params):\n",
    "    return stats.beta(a=params.a, b=params.b)\n",
    "\n",
    "def posterior_mean_rev_per_user_rvs(params, nsamples=100_000):\n",
    "    p_dist = rev_per_user_p_dist(params)\n",
    "    ps = p_dist.rvs(size=nsamples)\n",
    "    means_dist = posterior_mean_dist_lognormal(params)\n",
    "    means = means_dist.rvs(nsamples)\n",
    "    return ps * means\n",
    "\n",
    "def exact_rev_per_user_rvs(p, mu, s, nsamples):\n",
    "    conv = stats.bernoulli.rvs(p=p, size=nsamples)\n",
    "    rev = stats.lognorm.rvs(s=s, scale=np.exp(mu), size=nsamples)\n",
    "    return conv * rev\n",
    "\n",
    "def prob_pb_gt_pa_samples(post_samp_A, post_samp_B):\n",
    "    if len(post_samp_A) != len(post_samp_B):\n",
    "        return None\n",
    "    b_gt_a = np.sum(post_samp_B > post_samp_A)\n",
    "    return b_gt_a / len(post_samp_A)\n",
    "\n",
    "nexps = 100\n",
    "prob_stop = 0.95\n",
    "n_samp_max = 3_000_000\n",
    "n_samp_step = 30000\n",
    "n_post_samp = 50000\n",
    "\n",
    "A = {'p': 0.1, 'mu': 2, 's': 1}\n",
    "\n",
    "cmp = pd.DataFrame(columns=['A_pars', 'B_pars', 'A_mean', 'B_mean', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "cmp['A_pars'] = [A] * nexps\n",
    "cmp['B_pars'] = cmp['A_pars'].apply(lambda x: {'p': x['p'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1)), 's': x['s'], 'mu': x['mu'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1))})\n",
    "cmp['A_mean'] = cmp['A_pars'].apply(lambda x: x['p'] * stats.lognorm(s=x['s'], scale=np.exp(x['mu'])).mean())\n",
    "cmp['B_mean'] = cmp['B_pars'].apply(lambda x: x['p'] * stats.lognorm(s=x['s'], scale=np.exp(x['mu'])).mean())\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B_mean'] > r['A_mean'] else 'A', axis=1)\n",
    "\n",
    "for i in range(nexps):\n",
    "    A_pars = cmp.at[i, 'A_pars']\n",
    "    B_pars = cmp.at[i, 'B_pars']\n",
    "    n_samp_total = 0\n",
    "    dA = np.array([])\n",
    "    dB = np.array([])\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA = np.append(dA, exact_rev_per_user_rvs(p=A_pars['p'], mu=A_pars['mu'], s=A_pars['s'], nsamples=n_samp_step))\n",
    "        dB = np.append(dB, exact_rev_per_user_rvs(p=B_pars['p'], mu=B_pars['mu'], s=B_pars['s'], nsamples=n_samp_step))\n",
    "        n_samp_total += n_samp_step\n",
    "        post_pars_A = posterior_params_rev_per_user(dA)\n",
    "        post_pars_B = posterior_params_rev_per_user(dB)\n",
    "        post_samp_A = posterior_mean_rev_per_user_rvs(post_pars_A)\n",
    "        post_samp_B = posterior_mean_rev_per_user_rvs(post_pars_B)\n",
    "        pb_gt_pa = prob_pb_gt_pa_samples(post_samp_A, post_samp_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_samp_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_samp_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: n_samp {n_samp_total}, best_group {best_gr}, P(b>a) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca1b15",
   "metadata": {
    "papermill": {
     "duration": 0.042535,
     "end_time": "2026-01-21T04:28:27.712045",
     "exception": false,
     "start_time": "2026-01-21T04:28:27.669510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Orders per Visitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f5f6e",
   "metadata": {
    "papermill": {
     "duration": 0.043314,
     "end_time": "2026-01-21T04:28:27.797998",
     "exception": false,
     "start_time": "2026-01-21T04:28:27.754684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A visitor may place several orders or none at all. The distribution of the number of orders per visitor, $P_{\\text{orders}}(n)$, where $n \\in 0, 1, 2, \\dots$ can be modeled as a discrete analogue of the log-normal distribution or as a power-law distribution such as Zipf’s law: $P(n ; s) \\propto n^{-s}$ [[ZipfDist](https://en.wikipedia.org/wiki/Zipf%27s_law#Formal_definition), [SciPyZipfian](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipfian.html), [SciPyZipf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipf.html)]. It is important to model the probabilities of low-order counts accurately. Zipf’s law may not provide sufficient flexibility for this. A more adaptable approach is to model the exact probabilities $p_i$ of making $i$ orders. Assume a maximum number of orders $N$, and let $n_i$ be the number of users with $i$ orders, for $i=0, 1, 2, \\dots, N$. The likelihood is then given by the multinomial distribution: $P(\\mathcal{D} | \\mathcal{H}) = Mult(n_0, \\dots, n_N | p_0, \\dots, p_N)$ [[MultiDist](https://en.wikipedia.org/wiki/Multinomial_distribution), [SciPyMult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html)]. The conjugate prior for this likelihood is the Dirichlet distribution: $P(\\mathcal{H}) = Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right)$ [[DirDist](https://en.wikipedia.org/wiki/Dirichlet_distribution), [SciPyDir](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet.html)]. In the posterior distribution, each parameter is updated as $\\alpha_i + n_i$. The marginal distributions of each $p_i$ are Beta distributions, consistent with interpreting $p_i$ as the conversion rate to exactly $i$ orders.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Mult(n_0, \\dots, n_N | p_0, \\dots, p_N) = \\frac{(n_0 + \\dots + n_N)!}{n_{0}! \\dots n_{N}!} p_{0}^{n_{0}} \\dots p_{N}^{n_{N}} \n",
    "\\\\\n",
    "P(\\mathcal{H}) & = \n",
    "Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right) = \n",
    "\\dfrac{1}{B( \\alpha_{0}, \\dots, \\alpha_{N} )} \\prod_{i=0}^{N} p_{i}^{\\alpha_{i}-1},\n",
    "\\qquad\n",
    "\\sum_{i=0}^{N} p_i = 1,\n",
    "\\qquad\n",
    "p_i \\in [0, 1], \n",
    "\\qquad\n",
    "B(\\alpha_{0}, \\dots, \\alpha_{N}) = \n",
    "\\frac{\\prod \\limits_{i=0}^{N} \\Gamma( \\alpha_{i} )}\n",
    "{\\Gamma \\left( \\sum \\limits_{i=0}^{N} \\alpha_{i} \\right)}\n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto Mult(n_0, \\dots, n_N | p_0, \\dots, p_N) Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right)\n",
    "\\\\\n",
    "& \\propto\n",
    "p_{0}^{n_{0}} \\dots p_{N}^{n_{N}} \n",
    "\\prod _{i=0}^{N} p_{i}^{\\alpha_{i}-1}\n",
    "\\\\\n",
    "& \\propto\n",
    "\\prod_{i=0}^{N} p_{i}^{n_{i} + \\alpha_{i} - 1}\n",
    "\\\\\n",
    "& =\n",
    "Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0} + n_0, \\dots, \\alpha_{N} + n_N \\right)\n",
    "\\\\\n",
    "P(p_i | \\mathcal{D} ) & = \n",
    "\\int dp_0 \\dots dp_{i-1}dp_{i+i} \\dots dp_N P(\\mathcal{H} | \\mathcal{D}) \n",
    "=\n",
    "Beta( p_i; \\alpha_i + n_i, \\sum_{k=0}^{N} (\\alpha_k + n_k) - \\alpha_i - n_i )\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4e1a9",
   "metadata": {
    "papermill": {
     "duration": 0.04531,
     "end_time": "2026-01-21T04:28:27.886847",
     "exception": false,
     "start_time": "2026-01-21T04:28:27.841537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To illustrate parameter estimation, we define a Zipf distribution with parameters `s` and `Nmax`. A sample is drawn from this distribution, and the posterior distribution is then constructed based on the sample. In addition, the conversion probabilities for exactly $i$ orders, $p_i$, are computed. The plot shows the original distribution, the observed sample, the posterior predictive distribution $x$, the estimated values of $p_i$, and their 95\\% credible intervals. For most values of $i$, the true probabilities fall within the corresponding credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722ac0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:28:27.975133Z",
     "iopub.status.busy": "2026-01-21T04:28:27.974774Z",
     "iopub.status.idle": "2026-01-21T04:28:32.229704Z",
     "shell.execute_reply": "2026-01-21T04:28:32.228740Z"
    },
    "papermill": {
     "duration": 4.302236,
     "end_time": "2026-01-21T04:28:32.232364",
     "exception": false,
     "start_time": "2026-01-21T04:28:27.930128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_params_dir(N):\n",
    "    return np.ones(N)\n",
    "\n",
    "def posterior_params_dir(data, initial_pars):\n",
    "    u, c = np.unique(data, return_counts=True)\n",
    "    post_pars = np.copy(initial_pars)\n",
    "    for k, v in zip(u, c):\n",
    "        post_pars[k] = post_pars[k] + v\n",
    "    return post_pars\n",
    "\n",
    "def posterior_dist_dir(params):\n",
    "    return stats.dirichlet(alpha=params)\n",
    "\n",
    "def posterior_nords_dir_rvs(params, nsamp):\n",
    "    nords = np.empty(nsamp)\n",
    "    d = posterior_dist_dir(params)\n",
    "    probs = d.rvs(size=nsamp)\n",
    "    for i, p in enumerate(probs):\n",
    "        nords[i] = np.argmax(stats.multinomial.rvs(n=1, p=p))\n",
    "    return nords\n",
    "\n",
    "def marginal_pi_dist_dir(i, params):\n",
    "    return stats.beta(a=params[i], b=np.sum(params) - params[i])\n",
    "\n",
    "def posterior_pi_mean_95pdi(i, params):\n",
    "    p = marginal_pi_dist_dir(i, params)\n",
    "    m = p.mean()\n",
    "    lower = p.ppf(0.025)\n",
    "    upper = p.ppf(0.975)\n",
    "    return m, lower, upper\n",
    "\n",
    "Nmax = 30\n",
    "s = 1.5\n",
    "nsample = 1000\n",
    "\n",
    "Npars = Nmax + 1\n",
    "exact_dist = stats.zipfian(a=s, n=Npars, loc=-1)\n",
    "data = exact_dist.rvs(nsample)\n",
    "pars = initial_params_dir(Npars)\n",
    "pars = posterior_params_dir(data, pars)\n",
    "post_samp = posterior_nords_dir_rvs(pars, 100000)\n",
    "pi = [posterior_pi_mean_95pdi(i, pars) for i in range(Npars)]\n",
    "\n",
    "x = np.arange(0, Npars+1)\n",
    "plt.figure()\n",
    "plt.plot(x, exact_dist.pmf(x), color=\"black\", label=\"Exact Zipf Distribution\")\n",
    "counts_data = np.bincount(data, minlength=len(x))\n",
    "pmf_data = counts_data / counts_data.sum()\n",
    "counts_post = np.bincount(post_samp.astype(int), minlength=len(x))\n",
    "pmf_post = counts_post / counts_post.sum()\n",
    "w = 0.25\n",
    "plt.bar(x - w/2, pmf_data, width=w, color=\"black\", label=\"Sample\")\n",
    "plt.bar(x + w/2, pmf_post, width=w, color=\"black\", alpha=0.2, label=r\"Posterior $n_i$\")\n",
    "y = [p[0] for p in pi]\n",
    "yerr_lower = [p[0] - p[1] for p in pi]\n",
    "yerr_upper = [p[2] - p[0] for p in pi]\n",
    "plt.errorbar(x[:-1], y, yerr=[yerr_lower, yerr_upper], fmt='.', color='red', alpha=0.8, label=r'$p_i$ estimates')\n",
    "plt.xlabel(\"Orders\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlim(-1, Nmax + 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770cdc5",
   "metadata": {
    "papermill": {
     "duration": 0.043856,
     "end_time": "2026-01-21T04:28:32.323342",
     "exception": false,
     "start_time": "2026-01-21T04:28:32.279486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The distribution of order counts allows us to estimate the average number of orders $E[n] = \\sum_{i=0}^N i, p_i$, the conversion to at least one order $1 - p_0$, and the conversion to two or more orders $1 - p_0 - p_1$. Below is an example comparing the expected number of orders $E[n]$. Two Zipf distributions are defined, where group B has a shape parameter `s` that is 5\\% smaller than group A. The first chart shows the true distributions, exact means, and estimated $p_i$ values. For most values of $i$, the true $p_i$ falls within the estimated intervals. The second chart presents the posterior distribution of the average number of orders. Given the selected parameters, the probability that the mean of group B exceeds that of group A is $P(E[n]_B > E[n]_A) = 90\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140b323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:28:32.413372Z",
     "iopub.status.busy": "2026-01-21T04:28:32.412703Z",
     "iopub.status.idle": "2026-01-21T04:28:43.854658Z",
     "shell.execute_reply": "2026-01-21T04:28:43.853718Z"
    },
    "papermill": {
     "duration": 11.489365,
     "end_time": "2026-01-21T04:28:43.856750",
     "exception": false,
     "start_time": "2026-01-21T04:28:32.367385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def posterior_nords_mean_rvs(params, nsample):\n",
    "    ns = np.arange(len(params))\n",
    "    probs = stats.dirichlet.rvs(alpha=params, size=nsample)\n",
    "    means = np.sum(ns * probs, axis=1)\n",
    "    return means\n",
    "\n",
    "def prob_pb_gt_pa_samples(post_samp_A, post_samp_B):\n",
    "    if len(post_samp_A) != len(post_samp_B):\n",
    "        return None\n",
    "    b_gt_a = np.sum(post_samp_B > post_samp_A)\n",
    "    return b_gt_a / len(post_samp_A)\n",
    "\n",
    "nsample = 3000\n",
    "Nmax = 30\n",
    "Npars = Nmax + 1\n",
    "\n",
    "post_samp_len = 100000\n",
    "A, B = {}, {}\n",
    "s = 1.5\n",
    "A['dist_pars'] = {'s': s}\n",
    "B['dist_pars'] = {'s': s * 0.95}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.zipfian(a=g['dist_pars']['s'], n=Npars, loc=-1)\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['post_pars'] = initial_params_dir(Npars)\n",
    "    g['post_pars'] = posterior_params_dir(g['data'], g['post_pars'])\n",
    "    g['post_nords'] = posterior_nords_dir_rvs(g['post_pars'], post_samp_len)\n",
    "    g['post_means'] = posterior_nords_mean_rvs(g['post_pars'], post_samp_len)\n",
    "    g['pi'] = [posterior_pi_mean_95pdi(i, g['post_pars']) for i in range(Npars)]\n",
    "\n",
    "x = np.arange(0, Npars)\n",
    "plt.figure()\n",
    "w = 0.35\n",
    "plt.bar(x - w/2, A[\"exact_dist\"].pmf(x), width=w, color=\"black\", alpha=0.2, label=\"Exact distribution A\")\n",
    "plt.bar(x + w/2, B[\"exact_dist\"].pmf(x), width=w, color=\"black\", alpha=0.8, label=\"Exact distribution B\")\n",
    "plt.plot([A[\"exact_dist\"].mean(), A[\"exact_dist\"].mean()],[0, np.max(A[\"exact_dist\"].pmf(x))*1.1],\n",
    "         color=\"black\", linestyle=\"dashed\", alpha=0.3, label=\"Exact mean A\")\n",
    "plt.plot([B[\"exact_dist\"].mean(), B[\"exact_dist\"].mean()],[0, np.max(B[\"exact_dist\"].pmf(x))*1.1],\n",
    "         color=\"black\", linestyle=\"dashed\", label=\"Exact mean B\")\n",
    "yA = [p[0] for p in A[\"pi\"]]\n",
    "yAerr = [[p[0]-p[1] for p in A[\"pi\"]],[p[2]-p[0] for p in A[\"pi\"]]]\n",
    "plt.errorbar(x - w/2, yA, yerr=yAerr, fmt=\".\", color=\"black\", alpha=0.3, label=r\"$p_i, A$\")\n",
    "yB = [p[0] for p in B[\"pi\"]]\n",
    "yBerr = [[p[0]-p[1] for p in B[\"pi\"]],[p[2]-p[0] for p in B[\"pi\"]]]\n",
    "plt.errorbar(x + w/2, yB, yerr=yBerr, fmt=\".\", color=\"black\", label=r\"$p_i, B$\")\n",
    "plt.xlabel(\"Orders\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlim(-1, Npars + 1 - 20)\n",
    "plt.title(\"Orders per User\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, Npars)\n",
    "plt.figure()\n",
    "bins = np.linspace(-0.5, Nmax + 0.5, 10*Nmax)\n",
    "hA, _ = np.histogram(A['post_means'], bins=bins, density=True)\n",
    "hB, _ = np.histogram(B['post_means'], bins=bins, density=True)\n",
    "centers = (bins[:-1] + bins[1:])/2\n",
    "w = (bins[1]-bins[0])/2\n",
    "plt.bar(centers - w/2, hA, width=w, color='black', alpha=0.3, label=r'$E[n], A$')\n",
    "plt.bar(centers + w/2, hB, width=w, color='black', alpha=1.0, label=r'$E[n], B$')\n",
    "plt.plot([A['exact_dist'].mean(), A['exact_dist'].mean()], [0, np.max(hA)*1.1],\n",
    "         color='black', linestyle='dashed', alpha=0.3, label='Exact mean A')\n",
    "plt.plot([B['exact_dist'].mean(), B['exact_dist'].mean()], [0, np.max(hB)*1.1],\n",
    "         color='black', linestyle='dashed', label='Exact mean B')\n",
    "plt.xlabel('Orders')\n",
    "plt.ylabel('Probability density')\n",
    "plt.xlim(-1, Npars + 1 - 20)\n",
    "plt.title('Average Order Number')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"P(E[n]_B > E[n]_A): {prob_pb_gt_pa_samples(A['post_means'], B['post_means'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f881f1b",
   "metadata": {
    "papermill": {
     "duration": 0.047617,
     "end_time": "2026-01-21T04:28:43.952537",
     "exception": false,
     "start_time": "2026-01-21T04:28:43.904920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The number of correctly identified \"better\" groups is tested across `nexps` experiments. In Group A, the number of orders per user follows a Zipf distribution with parameter `s`, while in Group B the parameter varies within $\\pm 5\\%$ of A. Groups are compared based on their mean number of orders. In each experiment, samples are added incrementally by `n_samp_step`, posterior parameters are updated, and the probability $P(E[n]_B > E[n]_A)$ is computed. The experiment stops when the probability that one group’s mean exceeds the other’s reaches `prob_stop`, or when the maximum number of samples `n_samp_max` is reached. The proportion of correctly identified groups, 0.94, is close to the expected threshold `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db1c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T04:28:44.044309Z",
     "iopub.status.busy": "2026-01-21T04:28:44.043940Z",
     "iopub.status.idle": "2026-01-21T05:08:30.938920Z",
     "shell.execute_reply": "2026-01-21T05:08:30.937771Z"
    },
    "papermill": {
     "duration": 2386.94385,
     "end_time": "2026-01-21T05:08:30.940719",
     "exception": false,
     "start_time": "2026-01-21T04:28:43.996869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "\n",
    "s = 1.5\n",
    "Nmax = 30\n",
    "Npars = Nmax + 1\n",
    "nexps = 100\n",
    "cmp['A'] = [s] * nexps\n",
    "cmp['B'] = s * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "\n",
    "n_samp_max = 200000\n",
    "n_samp_step = 5000\n",
    "\n",
    "prob_stop = 0.95\n",
    "for i in range(nexps):\n",
    "    s_a = cmp.at[i, 'A']\n",
    "    s_b = cmp.at[i, 'B']\n",
    "    exact_dist_a = stats.zipfian(a=s_a, n=Npars, loc=-1)\n",
    "    exact_dist_b = stats.zipfian(a=s_b, n=Npars, loc=-1)\n",
    "    cmp.at[i, 'best_exact'] = 'A' if exact_dist_a.mean() > exact_dist_b.mean() else 'B'\n",
    "    n_samp_total = 0\n",
    "    pars_a = initial_params_dir(Npars)\n",
    "    pars_b = initial_params_dir(Npars)\n",
    "    while n_samp_total < n_samp_max:\n",
    "        data_a = exact_dist_a.rvs(n_samp_step)\n",
    "        data_b = exact_dist_b.rvs(n_samp_step)\n",
    "        n_samp_total += n_samp_step\n",
    "        pars_a = posterior_params_dir(data_a, pars_a)\n",
    "        pars_b = posterior_params_dir(data_b, pars_b)\n",
    "        post_samp_len = 10000\n",
    "        post_means_a = posterior_nords_mean_rvs(pars_a, post_samp_len)\n",
    "        post_means_b = posterior_nords_mean_rvs(pars_b, post_samp_len)\n",
    "        pb_gt_pa = prob_pb_gt_pa_samples(post_means_a, post_means_b)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_means_a.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_means_b.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(B>A) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55abbd7",
   "metadata": {
    "papermill": {
     "duration": 0.051661,
     "end_time": "2026-01-21T05:08:31.042726",
     "exception": false,
     "start_time": "2026-01-21T05:08:30.991065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e69b2",
   "metadata": {
    "papermill": {
     "duration": 0.051829,
     "end_time": "2026-01-21T05:08:31.144737",
     "exception": false,
     "start_time": "2026-01-21T05:08:31.092908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Bayesian modeling has been applied to compare conversions, means using the central limit theorem, transactional revenue per user, and orders per visitor. For each metric, a model distribution was proposed. The model parameters are defined using conjugate prior distributions, enabling the analytical construction of posterior distributions. The process demonstrated parameter estimation from a sample, comparison of two groups, and the evaluation of the proportion of correctly identified \"better\" groups in a series of experiments. The proportion aligns with expectations. Model validation was not discussed, but in practice, it is crucial to assess the model's applicability to specific situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668dff6",
   "metadata": {
    "papermill": {
     "duration": 0.050036,
     "end_time": "2026-01-21T05:08:31.244745",
     "exception": false,
     "start_time": "2026-01-21T05:08:31.194709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82761177",
   "metadata": {
    "papermill": {
     "duration": 0.052319,
     "end_time": "2026-01-21T05:08:31.347155",
     "exception": false,
     "start_time": "2026-01-21T05:08:31.294836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Apx] - [Bayesian A/B-Testing, Appendices](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices), *GitHub*.  \n",
    "[BaseFal] - [Base Rate Fallacy](https://en.wikipedia.org/wiki/Base_rate_fallacy), *Wikipedia.*  \n",
    "[BernProc] - [Bernoulli Process](https://en.wikipedia.org/wiki/Bernoulli_process), *Wikipedia.*  \n",
    "[BerryEsseenTheorem] - [Berry-Esseen Theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem), *Wikipedia.*   \n",
    "[BetaDist] - [Beta Distribution](https://en.wikipedia.org/wiki/Beta_distribution), *Wikipedia.*     \n",
    "[BinomDist] - [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution), *Wikipedia.*  \n",
    "[CausalDAG] - [Causal Graph](https://en.wikipedia.org/wiki/Causal_graph), *Wikipedia.*    \n",
    "[CDF] - [Cumulative Distribution Function](https://en.wikipedia.org/wiki/Cumulative_distribution_function), *Wikipedia.*  \n",
    "[CLT] - [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), *Wikipedia.*    \n",
    "[ConjPrior] - [Conjugate Prior](https://en.wikipedia.org/wiki/Conjugate_prior), *Wikipedia.*   \n",
    "[DirDist] - [Dirichlet Distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution), *Wikipedia.*    \n",
    "[GammaDist] - [Gamma Distribution](https://en.wikipedia.org/wiki/Gamma_distribution), *Wikipedia.*     \n",
    "[LognormDist] - [Log-normal Distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), *Wikipedia.*    \n",
    "[LomaxDist] - [Lomax Distribution](https://en.wikipedia.org/wiki/Lomax_distribution), *Wikipedia.*     \n",
    "[MultiDist] - [Multinomial Distribution](https://en.wikipedia.org/wiki/Multinomial_distribution), *Wikipedia.*    \n",
    "[NormDist] - [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), *Wikipedia.*  \n",
    "[NormSum] - [Sum of Normally Distributed Random Variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables), *Wikipedia.*  \n",
    "[ParetoDist] - [Pareto Distribution](https://en.wikipedia.org/wiki/Pareto_distribution), *Wikipedia.*     \n",
    "[ProbConv] - [Convolution of Probability Distributions](https://en.wikipedia.org/wiki/Convolution_of_probability_distributions), *Wikipedia.*    \n",
    "[RandVarsConv] - [Convergence of Random Variables](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution), *Wikipedia.*   \n",
    "[SGBS] - B. Lambert, A Student’s Guide to Bayesian Statistics ([Textbook](https://www.amazon.co.uk/Students-Guide-Bayesian-Statistics/dp/1473916364), [Student Resources](https://study.sagepub.com/lambert)).     \n",
    "[SR] - R. McElreath, Statistical Rethinking: A Bayesian Course with Examples in R and STAN ([Textbook](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919), [Video Lectures](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus), [Course Materials](https://github.com/rmcelreath/stat_rethinking_2024)).   \n",
    "[SciPyBern] - [scipy.stats.bernoulli](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html), *SciPy Reference.*  \n",
    "[SciPyBeta] - [scipy.stats.beta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html), *SciPy Reference.*   \n",
    "[SciPyBinom] - [scipy.stats.binom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html), *SciPy Reference.*   \n",
    "[SciPyDir] - [scipy.stats.dirichlet](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet.html), *SciPy Reference.*  \n",
    "[SciPyGamma] - [scipy.stats.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html), *SciPy Reference.*     \n",
    "[SciPyLognorm] - [scipy.stats.lognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html), *SciPy Reference.*      \n",
    "[SciPyLomax] - [scipy.stats.lomax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lomax.html), *SciPy Reference.*       \n",
    "[SciPyMult] - [scipy.stats.multinomial](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html), *SciPy Reference.*   \n",
    "[SciPyNorm] - [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), *SciPy Reference.*   \n",
    "[SciPyPareto] - [scipy.stats.pareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html), *SciPy Reference.*    \n",
    "[SciPyZipf] - [scipy.stats.zipf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipf.html), *SciPy Reference.*   \n",
    "[SciPyZipfian] - [scipy.stats.zipfian](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipfian.html), *SciPy Reference.*    \n",
    "[SubjProb] - [Probability Interpretations](https://en.wikipedia.org/wiki/Probability_interpretations#Subjectivism), *Wikipedia.*    \n",
    "[ZipfDist] - [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law#Formal_definition), *Wikipedia.*   "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2590.546357,
   "end_time": "2026-01-21T05:08:32.017336",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-21T04:25:21.470979",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
