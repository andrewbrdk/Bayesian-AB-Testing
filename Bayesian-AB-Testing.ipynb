{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b087aaa2",
   "metadata": {},
   "source": [
    "# Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84793b",
   "metadata": {},
   "source": [
    "*A/B testing is reviewed, and Bayesian modelling is discussed. Conversions, means, revenue per user, orders per visitor are compared in Bayesian approach.*\n",
    "\n",
    "&nbsp; &nbsp; *- [A/B Tests](#A/B-Tests)*  \n",
    "&nbsp; &nbsp; *- [Bayesian Modelling](#Bayesian-Modelling)*  \n",
    "&nbsp; &nbsp; *- [Conversions](#Conversions)*   \n",
    "&nbsp; &nbsp; *- [Means](#Means)*    \n",
    "&nbsp; &nbsp; *- [Revenue per User](#Revenue-per-User)*  \n",
    "&nbsp; &nbsp; *- [Orders per Visitor](#Orders-per-Visitor)*  \n",
    "&nbsp; &nbsp; *- [Conclusion](#Conclusion)*  \n",
    "&nbsp; &nbsp; *- [References](#References)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e4c34",
   "metadata": {},
   "source": [
    "# A/B Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b0098",
   "metadata": {},
   "source": [
    "Mobile apps and web services are constantly updated to drive revenue, conversions, engagement, and other key metrics. Some updates can hurt the product. It is essential to measure the impact of new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d8e59",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_experiment_versions.png\" alt=\"experiment_versions\" width=\"400\"/>\n",
    "    \n",
    "<em>Raising prices (on the right) would increase average order value but drop conversion. The net effect on revenue is unpredictable and should be measured. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98fdba",
   "metadata": {},
   "source": [
    "Metrics difference before and after a release doesn't show the real impact. Other factors, such as marketing campaigns, also affect metrics. Changes in metrics over time can't be attributed to a particular release unless the effect is sharp enough to stand out over natural fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5468b",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_effect_size.png\" alt=\"effect_size\"  width=\"900\"/>\n",
    "<em>Metrics dynamics is influenced by various factors. Unless impact of a feature is sharp enough (e.g., a 30% drop), a change can't be attributed to a particular release. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b24bb",
   "metadata": {},
   "source": [
    "A/B tests isolate the impact of the new feature. The original and modified versions are run in parallel. Users are randomly split between them. Key metrics are compared in each group. The test stops when one version clearly performs better or there’s no longer value in continuing. Next steps are decided upon the resutls. Usually it is to roll out the winning version for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583dcb1",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_ab_test.png\" alt=\"ab_test\" width=\"800\"/>\n",
    "    \n",
    "<em>A/B experiment setup: the test versions run in parallel, and users are randomly assigned to one of them. Key metrics are tracked for each group, and based on the comparison results, next steps are determined. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a9520",
   "metadata": {},
   "source": [
    "The causal diagram [[CausalDAG](https://en.wikipedia.org/wiki/Causal_graph)] for A/B testing is as follows. Metrics are driven by user actions within the service, which depend on the version of the website or app (e.g., available pricing plans), external factors (e.g., seasonality), and user segments (e.g., new vs. returning customers). In an A/B test, the versions run simultaneously, and users are randomly assigned to one of them. External factors remain consistent, and their influence on both groups is assumed to be equal over the same period. Since users are randomly divided, the segment composition is considered identical. As a result, differences in metrics between the groups can be attributed to the application’s functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89384924",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_causal.png\" alt=\"causal\" width=\"600\"/>\n",
    "    \n",
    "<em>Metrics are driven by user behavior and actions within the service. Behavior depends on the current version of the service, external factors, and user segments. Running A/B test variants simultaneously ensures external factors impact metrics equally, while random user assignment ensures segment influence is consistent. As a result, differences in metrics between groups can be attributed to the tested functionality. </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de2fb",
   "metadata": {},
   "source": [
    "At the end of the experiment, metrics and effects need to be evaluated to select the \"best\" group. The exact metric values are unknown, so estimates must be built from the collected data. It's useful to treat metrics as random variables. Probability distributions for these variables should be chosen to best match the experimental data. Comparing these distributions helps assess the effect. For presentation, it’s convenient to show point estimates and the highest density probability interval. For example, the average metric in Group A is $p_A = 7.1 \\pm 0.2$, and in Group B it’s $p_B = 7.4 \\pm 0.3$. The effect can be expressed as the relative difference: $(p_B - p_A) / p_A = 4.2 \\pm 0.2%$. To select the \"best\" group, we evaluate the probability that the metric in Group B is higher than in Group A, e.g., $P(p_B > p_A) = 95%$. Here and below, probability is understood in the subjective sense — as a measure of confidence in a particular outcome among several possible ones [[SubjProb](https://en.wikipedia.org/wiki/Probability_interpretations#Subjectivism)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702718f8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_ab_metric_random.png\" alt=\"ab_metric_random\" width=\"500\"/>\n",
    "<br/>   \n",
    "<em>\n",
    "In an A/B test, metrics, effects, and the \"best\" group need to be evaluated. Exact metric values are unknown, so estimates must be built from the collected data. It's useful to treat these estimates as random variables. The probability distributions for these variables should be chosen to best fit the experimental data. Comparing these distributions helps assess the effect.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bec192",
   "metadata": {},
   "source": [
    "With limited data, there is high uncertainty in metric estimates. As more data is collected, these estimates become more precise, and confidence grows in identifying the better group. Once confidence reaches a sufficient level, the experiment can be stopped. Other stopping criteria may also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef663589",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_ab_dynamics.png\" alt=\"ab_dynamics\" width=\"430\"/>\n",
    "<br/>\n",
    "<em>\n",
    "As data accumulates, the accuracy of metric estimates improves, and confidence in identifying the better group grows. The experiment can be stopped once confidence reaches a sufficient level.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaa4cd",
   "metadata": {},
   "source": [
    "Bayesian modeling is used to estimate the distributions of metrics based on experimental data [[SR](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919), [SGBS](https://www.amazon.co.uk/Students-Guide-Bayesian-Statistics/dp/1473916364)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd317e22",
   "metadata": {},
   "source": [
    "# Bayesian Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24a767",
   "metadata": {},
   "source": [
    "The first example. If it's cloudy in the morning, will it rain later? To answer this, we can calculate the probability of rain given that it's cloudy, $P(\\mbox{Rain | Clouds}) = (\\mbox{Clouds, Rain}) / (\\mbox{Clouds})$. The total number of cloudy days is the sum of cloudy days with rain and cloudy days without rain, $(\\mbox{Clouds}) = \\mbox{(Clouds, Rain)} + \\mbox{(Clouds, No Rain)}$. Let’s assume 20% of days are rainy, $P(\\text{Rain}) = 20\\%$, the probability of morning cloudiness on a rainy day is $P(\\mbox{Clouds | Rain}) = 70\\%$, and on a dry day, $P(\\mbox{Clouds | No Rain}) = 10\\%$. The number of cloudy days with rain can be expressed using these probabilities: $\\mbox{(Clouds, Rain)} = (\\mbox{Total Days}) P(\\mbox{Rain}) P(\\mbox{Clouds | Rain})$. Similarly, the number of cloudy days without rain can be calculated. After substituting values, we get $P(\\mbox{Rain | Clouds}) = (0.7 \\cdot 0.2) / (0.7 \\cdot 0.2 + 0.1 \\cdot 0.8) = 63.6\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f775492",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_bayes_rain.png\" alt=\"bayes_rain\" width=\"600\"/>\n",
    "<br/>\n",
    "<em>\n",
    "The probability of a rainy day given a cloudy morning is estimated by the ratio of rainy, cloudy days to all cloudy days — both with and without rain.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc131d1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "P(\\mbox{Rain | Clouds}) & = \\frac{\\mbox{Clouds, Rain}}{\\mbox{Clouds, Rain + Clouds, No Rain}} \n",
    "\\\\\n",
    "\\\\\n",
    "& = \\frac{P(\\mbox{Clouds | Rain})P(\\mbox{Rain})}{P(\\mbox{Clouds | Rain})P(\\mbox{Rain}) + P(\\mbox{Clouds | No Rain})P(\\mbox{No Rain})}\n",
    "\\\\\n",
    "\\\\\n",
    "& = \\frac{0.7 \\cdot 0.2}{0.7 \\cdot 0.2 + 0.1 \\cdot 0.8} = 63.6 \\%\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e7180",
   "metadata": {},
   "source": [
    "In estimating the probability of rain given cloudy weather, $P(\\text{Rain | Clouds})$, it's important to consider not only the probability of cloudiness on a rainy day, $P(\\text{Clouds | Rain})$, but also the proportion of rainy days, $P(\\text{Rain})$, and the probability of cloudiness on a dry day, $P(\\text{Clouds | No Rain})$. Ignoring these factors can lead to a base rate fallacy [[BaseFal](https://en.wikipedia.org/wiki/Base_rate_fallacy)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1e67b",
   "metadata": {},
   "source": [
    "Another example. In the evening, you see an unfamiliar object in the park. From a distance, only its outline is visible. You try to guess what it is. Formally, this is a Bayes’ rule problem. You list possible options — leaves, a lost hat, a bird, a puddle. For each, you estimate how likely it is to match the observed shape: $P(\\mbox{Shape | Leaves})P(\\mbox{Shape | Leaves})$, $P(\\mbox{Shape | Hat})P(\\mbox{Shape | Hat})$, etc. You also factor in how common each option is — leaves are much more likely than a lost hat: $P(\\mbox{Leaves})>P(\\mbox{Hat})$. Bayes’ rule combines this information to estimate the probability of each option given what you see: $P(\\mbox{Leaves | Shape}) \\propto P(\\mbox{Shape | Leaves}) P(\\mbox{Leaves})$. As you get closer, the object glances at you and quickly climbs a tree — it turns out to be a squirrel you haven’t seen in the park for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56031c7",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_bayes_park.png\" alt=\"park\" width=\"600\"/>\n",
    "<em>\n",
    "<br/>\n",
    "To choose between options using Bayes' rule, you need to account for two things: how common each option is (the width of the vertical bars) and how likely it is to produce the observed shape (the height of the shaded area within the bar). The probability of each option is proportional to the area of its shaded region relative to the total shaded area across all options.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e29a9",
   "metadata": {},
   "source": [
    "This example illustrates the key elements of Bayesian modeling. We have data or observations $\\mathcal{D}$ and possible explanations or hypotheses $\\mathcal{H}_i$. To choose between them, we evaluate how well each hypothesis explains the data — this is done by calculating likelihoods $P(\\mathcal{D}|\\mathcal{H}_i)$. Prior knowledge or experience is reflected in the prior probabilities $P(\\mathcal{H}_i)$. Bayes' rule combines the likelihood and the prior to compute the posterior probability $P(\\mathcal{H}_i|\\mathcal{D})$ — our updated belief in each hypothesis given the data. Based on these posteriors, we select the most suitable model. It's important to validate models: even if one hypothesis fits the data better than the others, none of them may fully reflect reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa07aa",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{H}_i | \\mathcal{D}) &= \\frac{ P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }{P(\\mathcal{D})}\n",
    "= \\frac{ P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }{\\sum \\limits_i P(\\mathcal{D} | \\mathcal{H}_i) P(\\mathcal{H}_i) }\n",
    "\\\\\n",
    "P(\\mathcal{H}_i | \\mathcal{D}) &\\mbox{ - posterior probability distribution} \n",
    "\\\\\n",
    "P(\\mathcal{D} | \\mathcal{H}_i) &\\mbox{ - likelihood function}\n",
    "\\\\\n",
    "P(\\mathcal{H}_i) &\\mbox{ - prior probability distribution}\n",
    "\\\\\n",
    "P(\\mathcal{D}) &\\mbox{ - normalization constant}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567cefea",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_bayes_hypotheses_square.png\" alt=\"bayes_hypotheses_square\" width=\"900\"/>\n",
    "<em>\n",
    "<br/>\n",
    "A set of models is selected to explain the data. For each model, we specify prior belief — its probability relative to other models. We then calculate the likelihood — the probability of observing the data assuming the model is true. Using Bayes' rule, we update our belief in each model given the data — this gives the posterior probability.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68672877",
   "metadata": {},
   "source": [
    "Next example. Suppose $N=1000$ users visited a webpage, and $n_s=100$ of them clicked the “Continue” button. What does the distribution of the possible conversion rate $p$ look like? We assume each user has the same probability of converting, and — before seeing the data — all possible values of $p$ are considered equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d286c",
   "metadata": {},
   "source": [
    "We need to estimate the probability $P(\\mathcal{H} | \\mathcal{D}) = P(p | n_s, N)$ for given $n_s$ и $N$. Using Bayes’ rule, we have: $P(p | n_s, N) \\propto P(n_s, N | p) P(p)$. Each user clicks with probability $p$ and does not click with probability $1-p$. The clicks of $N$ users can be modeled as a sequence of Bernoulli random variables [[BernProc](https://en.wikipedia.org/wiki/Bernoulli_process), [SciPyBern](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html)]. The probability of $n_s$ conversions out of $N$ with success probability $p$ is given by a binomial distribution $P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) = \\mbox{Binom}(n_s, N; p)$ [[BinomDist](https://en.wikipedia.org/wiki/Binomial_distribution), [SciPyBinom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html)]. Since all possible prior values of the conversion rate are equally likely, the prior distribution is uniform $P(\\mathcal{H}) = P(p) = \\mbox{Unif}(0, 1) = 1$. The posterior distribution $P(p | n_s, N)$ will be a Beta distribution [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution), [SciPyBeta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html)].\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) & = \\mbox{Binom}(n_s, N; p) = C^{n_s}_{N} p^{n_s} (1 - p)^{N-n_s}\n",
    "\\\\\n",
    "\\\\\n",
    "P(\\mathcal{H}) = P(p) & = \\mbox{Unif}(0, 1) = 1\n",
    "\\\\\n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) = P(p | n_s, N) \n",
    "& = \\frac{P(n_s, N | p) P(p)}{P(n_s, N)}\n",
    "= \\frac{P(n_s, N | p) P(p)}{\\int_0^1 d p P(n_s, N | p) P(p)}\n",
    "\\\\\n",
    "& = \\frac{p^{n_s} (1 - p)^{N-n_s}}{\\int_0^1 d p (1 - p)^{N-n_s} p^{n_s} }\n",
    "= \\mbox{Beta}(p; n_s + 1, N - n_s + 1)\n",
    "\\\\\n",
    "\\\\\n",
    "\\mbox{Beta}(x; \\alpha, \\beta) & \\equiv \\frac{x^{\\alpha-1} (1 - x)^{\\beta-1}}{\\int_0^1 dx x^{\\alpha-1} (1 - x)^{\\beta-1}}\n",
    " = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1 - x)^{\\beta-1}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136734e",
   "metadata": {},
   "source": [
    "The graph of the posterior distribution $ P(p | n_s, N) $ is shown below. The mode coincides with the sample mean $n_s/N$, and the most likely values of $p$ are near this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d072080",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 100\n",
    "ntotal = 1000\n",
    "\n",
    "p_samp = ns / ntotal\n",
    "p_dist = stats.beta(a=ns+1, b=ntotal-ns+1)\n",
    "\n",
    "xaxis_max = 0.2\n",
    "x = np.linspace(0, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=p_dist.pdf(x), line_color='black', name='Distribution'))\n",
    "fig.add_trace(go.Scatter(x=[p_samp, p_samp], y=[0, max(p_dist.pdf(x))], \n",
    "                         line_color='black', mode='lines', line_dash='dash', name='Sample mean'))\n",
    "fig.update_layout(title='$\\mbox{Posterior distribution} \\, P(p | n_s, N)$',\n",
    "                  xaxis_title='$p$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[0, xaxis_max],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch2_conv_example.png\", scale=2)\n",
    "#The probability density of conversions is given by the Beta distribution. The mode coincides with the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2edc81",
   "metadata": {},
   "source": [
    "Another example. For version A of a webpage, $N_A=1000$ users visited, and $n_{s_A}=100$ clicked the \"Continue\" button. For version B, $N_B=1000$ users visited, and $n_{s_B}=110$ clicked the \"Continue\" button. What is the probability that the conversion rate for page B is higher than for page A?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5109d",
   "metadata": {},
   "source": [
    "You need the probability $P(p_B > p_A)$. The posterior distribution of the conversion rates for each group is calculated as in the previous example: $P(p; n_s, N) = \\mbox{Beta}(p; n_s + 1, N - n_s + 1)$. To estimate $P(p_B > p_A)$, we can calculate the distribution of $p_B - p_A$ and find the probability $P(p_B - p_A > 0)$. The distribution of $p_B - p_A$ in general is given by the convolution of the two distributions [[ProbConv](https://en.wikipedia.org/wiki/Convolution_of_probability_distributions)]. In this case, we can use an approximation. Given the parameters, the posterior Beta distributions are close to normal distributions [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution#Special_and_limiting_cases), [NormDist](https://en.wikipedia.org/wiki/Normal_distribution), [SciPyNorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=norm)]. The difference of random variables with normal distributions is also a random variable with a normal distribution [[NormSum](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)]. Therefore, the distribution of $p_B - p_A$ is approximately normal, with a mean equal to the difference of the means of the posterior Beta distributions and a variance equal to the sum of the variances. The desired probability is expressed using the cumulative distribution function $P(p_B - p_A > 0) = 1 - F(0)$ [[CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function)]. Instead of analytical transformations, a numerical estimate can be performed. For this, samples are generated from the posterior distributions and compared. The first graph shows the posterior distributions in the groups. The second shows the approximate analytical distribution of $p_B - p_A$ and the distribution of the difference between samples from the posterior distributions. Both calculations yield a similar result: $P(p_B > p_A) = 77 \\%$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(p_B > p_A) & = P(p_B - p_A > 0)\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_A}(x) = \\mbox{Beta}(x; n_{s_A} + 1, N_A - n_{s_A} + 1)\n",
    "& \\approx \\mbox{Norm}(x; \\mu_A, \\sigma_A^2),\n",
    "\\quad\n",
    "\\mu_A = n_{s_A}/N_A, \n",
    "\\,\n",
    "\\sigma_A^2 = \\mu_A (1 - \\mu_A) / N_A,\n",
    "\\quad\n",
    "N_A \\gg n_{s_A} \\gg 1\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_B}(x) = \\mbox{Beta}(x; n_{s_B} + 1, N_B - n_{s_B} + 1)\n",
    "& \\approx \\mbox{Norm}(x; \\mu_B, \\sigma_B^2),\n",
    "\\quad\n",
    "\\mu_B = n_{s_B}/N_B, \n",
    "\\,\n",
    "\\sigma_B^2 = \\mu_B (1 - \\mu_B) / N_B,\n",
    "\\quad\n",
    "N_B \\gg n_{s_B} \\gg 1\n",
    "\\\\\n",
    "\\\\\n",
    "P_{p_B - p_A}(x) = \n",
    "\\int_{-\\infty}^{\\infty} dy P_{p_B}(y) P_{p_A}(y-x)\n",
    "& \\approx \\mbox{Norm}\\left(x; \\mu_B - \\mu_A, \\sigma_A^2 + \\sigma_B^2\\right),\n",
    "\\quad\n",
    "\\mbox{Norm}(x ; \\mu, \\sigma^2) \\equiv \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\tfrac{(x-\\mu)^2}{2 \\sigma^2} }\n",
    "\\\\\n",
    "\\\\\n",
    "P(p_B - p_A > 0) & = 1 - F_{p_B - p_A}(0)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762b9a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "na = 1000\n",
    "sa = 100\n",
    "nb = 1000\n",
    "sb = 110\n",
    "\n",
    "p_dist_a = stats.beta(a=sa+1, b=na-sa+1)\n",
    "p_dist_b = stats.beta(a=sb+1, b=nb-sb+1)\n",
    "\n",
    "approx_diff_dist = stats.norm(loc=p_dist_b.mean() - p_dist_a.mean(), \n",
    "                              scale=np.sqrt(p_dist_b.std()**2 + p_dist_a.std()**2))\n",
    "dist_p_b_gt_a = 1 - approx_diff_dist.cdf(0)\n",
    "\n",
    "npost = 50000\n",
    "samp_a = p_dist_a.rvs(size=npost)\n",
    "samp_b = p_dist_b.rvs(size=npost)\n",
    "samp_p_b_gt_a = np.sum(samp_b > samp_a) / npost\n",
    "\n",
    "\n",
    "xaxis_max = 0.2\n",
    "x = np.linspace(0, xaxis_max, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=p_dist_a.pdf(x), line_color='black', name='A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=p_dist_b.pdf(x), line_color='black', opacity=0.3, name='B'))\n",
    "fig.update_layout(title='Posterior distributions',\n",
    "                  xaxis_title='$p$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[0, xaxis_max],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch2_conv_cmp_example.png\", scale=2)\n",
    "#The posterior distributions of the conversion rates in both groups are given by Beta distributions.\n",
    "\n",
    "x = np.linspace(-0.3, 0.3, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=approx_diff_dist.pdf(x), \n",
    "                         line_color='black', name='Analytical approximation'))\n",
    "fig.add_trace(go.Histogram(x=samp_b - samp_a, histnorm='probability density', \n",
    "                           name='Posterior samples difference', nbinsx=500,\n",
    "                           marker_color='black', opacity=0.3))\n",
    "fig.add_trace(go.Scatter(x=[0, 0], y=[0, max(approx_diff_dist.pdf(x))*1.05], name='0',\n",
    "                         line_color='black', mode='lines', line_dash='dash', showlegend=False))\n",
    "fig.update_layout(title='$p_B - p_A$',\n",
    "                  xaxis_title='$x$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[-0.1, 0.1],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch2_conv_cmp_diff.png\", scale=2)\n",
    "#The conversion rate of group B is higher than group A with a probability of 77%.\n",
    "\n",
    "print(f\"P(p_b > p_a) diff dist: {dist_p_b_gt_a}\")\n",
    "print(f\"P(p_b > p_a) post samples: {samp_p_b_gt_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20d872",
   "metadata": {},
   "source": [
    "# Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e7da1",
   "metadata": {},
   "source": [
    "In A/B tests, conversions to orders, clicks on buttons, and other actions are often compared. If a user's behavior does not affect others, a Bernoulli process can be used for modeling. With a conversion rate $p$, the probability that $n_s$ out of $N$ users will perform the target action follows a binomial distribution $P(n_s, N | p) = \\mbox{Binom}(n_s, N | p)$. The prior distribution for conversions is conveniently modeled with a Beta distribution $P(p) = \\mbox{Beta}(p; \\alpha, \\beta)$. The Beta distribution's dependency on $p$, without normalization constants, is $\\mbox{Beta}(p; \\alpha, \\beta) \\propto p^{\\alpha-1}(1-p)^{\\beta-1}$. This form remains valid for the product of the likelihood and the prior distribution $P(p | n_s, N) \\propto \\mbox{Binom}(p, N) \\mbox{Beta}(p; \\alpha, \\beta) \\propto p^{n_s + \\alpha - 1} (1-p)^{N - n_s + \\beta - 1}$. The only important part is the dependency on $p$; the other factors will be included in the normalization constant. Therefore, the posterior distribution will also be a Beta distribution, but with different parameters $P(p | n_s, N) = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)$. Prior distributions with this property are called conjugate priors [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61b53d",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\mathcal{H} | \\mathcal{D}) \\propto P(\\mathcal{D} | \\mathcal{H}) P(\\mathcal{H})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathcal{D} | \\mathcal{H}) = P(n_s, N | p) = \\mbox{Binom}(n_s, N | p) = C_{N}^{n_s} p^{n_s} (1-p)^{N-n_s}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathcal{H}) = P(p) = \\mbox{Beta}(p; \\alpha, \\beta) = \n",
    "\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{H} | \\mathcal{D}) & = P(p | n_s, N) \n",
    "\\\\\n",
    "& \\propto \\mbox{Binom}(n_s, N | p) \\mbox{Beta}(p; \\alpha, \\beta)\n",
    "\\\\\n",
    "& \\propto C_{N}^{n_s} p^{n_s} (1-p)^{N-n_s}\n",
    "\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "\\\\\n",
    "& \\propto p^{n_s + \\alpha - 1} (1-p)^{N - n_s + \\beta - 1}\n",
    "\\\\\n",
    "& = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060da4a",
   "metadata": {},
   "source": [
    "Beta distributions for different parameters are shown in the graph below [[BetaDist](https://en.wikipedia.org/wiki/Beta_distribution), [SciPyBeta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html)]. When $\\alpha = 1, \\beta=1$, the distribution is uniform—these values are convenient for use as prior distributions. In other cases, the maximum of the distribution occurs at $p = (\\alpha-1) / (\\alpha + \\beta - 2)$. As $\\alpha$ and $\\beta$ increase, the distribution narrows and approaches a normal distribution. Instead of using $\\alpha = 1, \\beta=1$ for the prior, the initial values of $\\alpha$ and $\\beta$ can be chosen based on historical data to make the prior distribution of conversions match the historical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "fig = go.Figure()\n",
    "a, b = 1, 1\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta.pdf(x, a=a, b=b), \n",
    "                             mode='lines', line_color='black', line_dash='dash',\n",
    "                             name=f'a={a}, b={b}'))\n",
    "a, b = 1, 5\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta.pdf(x, a=a, b=b), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'a={a}, b={b}'))\n",
    "a, b = 3, 5\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta.pdf(x, a=a, b=b), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'a={a}, b={b}'))\n",
    "a, b = 25, 30\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta.pdf(x, a=a, b=b), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'a={a}, b={b}'))\n",
    "a, b = 150, 50\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.beta.pdf(x, a=a, b=b), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'a={a}, b={b}')) \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0.93, 0.08, 0.30, 0.53, 0.84],\n",
    "    y=[1.35, 5.00, 2.80, 6.20, 12.0],\n",
    "    mode=\"text\",\n",
    "    name=None,\n",
    "    showlegend=False,\n",
    "    text=[\"a=1, b=1\", \"a=1, b=5\", \"a=3, b=5\", \"a=25, b=30\", \"a=150, b=50\"],\n",
    "    textposition=\"middle center\"\n",
    "))\n",
    "fig.update_layout(title='Beta distribution Beta(a, b)',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  showlegend=False,\n",
    "                  xaxis_range=[0, 1],\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch3_beta.png\", scale=2)\n",
    "#The Beta distribution for different parameters is shown below. When α=1, β=1, the Beta distribution becomes uniform. As α and β increase, the distribution narrows and approaches a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7918681",
   "metadata": {},
   "source": [
    "To verify the conversion calculation using the data, we set an exact value for the conversion `p`, then generate `nsample` binary random variables. From this sample, we build the posterior distribution of possible conversion values, denoted as `post_dist`. On the graph, the mode of the posterior distribution coincides with the sample mean and is close to the true value of `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e25350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_dist_binom(ns, ntotal, a_prior=1, b_prior=1):\n",
    "    a = a_prior + ns\n",
    "    b = b_prior + ntotal - ns \n",
    "    return stats.beta(a=a, b=b)\n",
    "    \n",
    "p = 0.1\n",
    "nsample = 1000\n",
    "\n",
    "exact_dist = stats.bernoulli(p=p)\n",
    "data = exact_dist.rvs(nsample)\n",
    "post_dist = posterior_dist_binom(ns=np.sum(data), ntotal=len(data))\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_dist.pdf(x), line_color='black', name='Posterior'))\n",
    "fig.add_trace(go.Scatter(x=[data.mean(), data.mean()], y=[0, max(post_dist.pdf(x))], \n",
    "                         line_color='black', mode='lines', line_dash='dash', name='Sample mean'))\n",
    "fig.add_trace(go.Scatter(x=[exact_dist.mean(), exact_dist.mean()], y=[0, max(post_dist.pdf(x))*1.05], \n",
    "                         line_color='red', mode='lines', line_dash='dash', name='Exact p'))\n",
    "fig.update_layout(title='Posterior distribution',\n",
    "                  xaxis_title='p',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[p-0.1, p+0.1],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch3_postdist.png\", scale=2)\n",
    "#The mode of the posterior distribution of conversion is close to the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e3d75",
   "metadata": {},
   "source": [
    "For the example comparing two groups, $p_A$ is set, and $p_B$ is 5% higher. `nsample` data points are generated for each group, and posterior distributions of the conversions are built. By sampling from these distributions, we estimate the probability that group B's conversion exceeds group A's, $P(p_B > p_A)$. The graph shows $P(p_B > p_A) = 84.0 \\%$. Since nsample is relatively small, the values may change with each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "p_A = 0.1\n",
    "p_B = p_A * 1.05\n",
    "nsample = 1000\n",
    "\n",
    "exact_dist_A = stats.bernoulli(p=p_A)\n",
    "exact_dist_B = stats.bernoulli(p=p_B)\n",
    "data_A = exact_dist_A.rvs(nsample)\n",
    "data_B = exact_dist_B.rvs(nsample)\n",
    "\n",
    "post_dist_A = posterior_dist_binom(ns=np.sum(data_A), ntotal=len(data_A))\n",
    "post_dist_B = posterior_dist_binom(ns=np.sum(data_B), ntotal=len(data_B))\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_dist_A.pdf(x), line_color='black', name='A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_dist_B.pdf(x), line_color='black', opacity=0.2, name='B'))\n",
    "fig.add_trace(go.Scatter(x=[exact_dist_A.mean(), exact_dist_A.mean()], y=[0, max(post_dist_A.pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', name='Exact A'))\n",
    "fig.add_trace(go.Scatter(x=[exact_dist_B.mean(), exact_dist_B.mean()], y=[0, max(post_dist_A.pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', opacity=0.2, name='Exact B'))\n",
    "fig.update_layout(title='Posterior distributions',\n",
    "                  xaxis_title='p',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[p_A/2, p_A*2],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch3_groups_cmp.png\", scale=2)\n",
    "#Posterior distributions of the conversions in the groups. The conversion rate of group B is higher than that of group A with a probability of 84%.\n",
    "\n",
    "print(f'P(pB > pA): {prob_pb_gt_pa(post_dist_A, post_dist_B)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5abff",
   "metadata": {},
   "source": [
    "The example below shows how conversion estimates and the probability $P(p_B > p_A)$ change as more data is collected. Two groups are compared: Group A has a fixed conversion rate $p_A$, while Group B’s conversion rate $p_B$ is set 5% higher. In each group, `npoints` samples are generated over `nstep` iterations (for a total of `N = npoints * nstep` samples). At each step, posterior distributions are updated based on the accumulated data, and the probability $P(p_B > p_A)$ is calculated. The plot also shows 95% credible intervals for both groups. As the sample size grows, the intervals narrow, and the probability tends toward 1 in favor of the better-performing group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca6765",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def posterior_binom_approx_95pdi(post_dist):\n",
    "    lower = post_dist.ppf(0.025)\n",
    "    upper = post_dist.ppf(0.975)\n",
    "    return lower, upper\n",
    "\n",
    "pa = 0.1\n",
    "pb = pa * 1.05\n",
    "\n",
    "npoints = 1000\n",
    "nstep = 150\n",
    "sa = stats.binom.rvs(p=pa, n=npoints, size=nstep)\n",
    "sb = stats.binom.rvs(p=pb, n=npoints, size=nstep)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['npoints'] = [npoints] * nstep\n",
    "df['sa_step'] = sa\n",
    "df['sb_step'] = sb\n",
    "df['N'] = df['npoints'].cumsum()\n",
    "df['sa'] = df['sa_step'].cumsum()\n",
    "df['sb'] = df['sb_step'].cumsum()\n",
    "df['pa'] = df.apply(lambda r: posterior_dist_binom(r['sa'], r['N']).mean(), axis=1)\n",
    "df[['pa_lower', 'pa_upper']] = df.apply(lambda r: posterior_binom_approx_95pdi(posterior_dist_binom(r['sa'], r['N'])), axis=1, result_type=\"expand\")\n",
    "df['pb'] = df.apply(lambda r: posterior_dist_binom(r['sb'], r['N']).mean(), axis=1)\n",
    "df[['pb_lower', 'pb_upper']] = df.apply(lambda r: posterior_binom_approx_95pdi(posterior_dist_binom(r['sb'], r['N'])), axis=1, result_type=\"expand\")\n",
    "df['pb_gt_pa'] = df.apply(lambda r: prob_pb_gt_pa(posterior_dist_binom(r['sa'], r['N']), posterior_dist_binom(r['sb'], r['N']), post_samp=10_000), axis=1)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['N'], y=df['pa'], name='A',\n",
    "                         line_color='black'))\n",
    "fig.add_trace(go.Scatter(x=list(df['N']) + list(reversed(df['N'])), \n",
    "                         y=list(df['pa_upper']) + list(reversed(df['pa_lower'])),\n",
    "                         fill=\"toself\", name='A, 95% PDI', marker_color='black', opacity=0.2))\n",
    "fig.add_trace(go.Scatter(x=df['N'], y=df['pb'], name='B',\n",
    "                         line_color='blue'))\n",
    "fig.add_trace(go.Scatter(x=list(df['N']) + list(reversed(df['N'])), \n",
    "                         y=list(df['pb_upper']) + list(reversed(df['pb_lower'])),\n",
    "                         fill=\"toself\", name='B, 95% PDI', marker_color='blue', opacity=0.2))\n",
    "fig.update_layout(title='$p_A, p_B$',\n",
    "                  yaxis_tickformat = ',.1%',\n",
    "                  xaxis_title='N')\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch3_conv_dynamics.png\", scale=2)\n",
    "#Conversion estimates become more accurate as more data is collected.\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['N'], y=df['pb_gt_pa'], name='P(pb > pa)',\n",
    "                         line_color='black'))\n",
    "fig.update_layout(title='$P(p_B > p_A)$',\n",
    "                  yaxis_range=[0, 1],\n",
    "                  xaxis_title='N')\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch3_pbgta_dynamics.png\", scale=2)\n",
    "#Confidence in the better group grows as more data is collected and conversion estimates improve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e39c0",
   "metadata": {},
   "source": [
    "The accuracy of identifying the better group is demonstrated as follows. Group A is assigned a conversion rate `p`. Group B is assigned a random value within $\\pm 5\\%$ of `p`. Data for both groups is generated in steps of `n_samp_step`. At each step, posterior distributions are updated, and the probability $P(p_B > p_A)$ is calculated. The experiment stops when either $P(p_B > p_A)$ or $P(p_A > p_B)$ reaches the stopping threshold `prob_stop = 0.95`, or when the maximum number of samples `n_samp_max` is reached. The procedure is repeated `nexps` times. The share of correctly identified better groups is calculated across all experiments. In this case, with `nexps = 100`, the correct group was identified 94 times. The resulting accuracy of 0.94 is close to the stopping threshold of `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "\n",
    "p = 0.1\n",
    "nexps = 100\n",
    "cmp['A'] = [p] * nexps\n",
    "cmp['B'] = p * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B'] > r['A'] else 'A', axis=1)\n",
    "\n",
    "n_samp_max = 30_000_000\n",
    "n_samp_step = 10_000\n",
    "prob_stop = 0.95\n",
    "\n",
    "for i in range(nexps):\n",
    "    pA = cmp.at[i, 'A']\n",
    "    pB = cmp.at[i, 'B']\n",
    "    exact_dist_A = stats.bernoulli(p=pA)\n",
    "    exact_dist_B = stats.bernoulli(p=pB)\n",
    "    n_samp_total = 0\n",
    "    ns_A = 0\n",
    "    ns_B = 0\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA = exact_dist_A.rvs(n_samp_step)\n",
    "        dB = exact_dist_B.rvs(n_samp_step)\n",
    "        n_samp_total += n_samp_step\n",
    "        ns_A = ns_A + np.sum(dA)\n",
    "        ns_B = ns_B + np.sum(dB)\n",
    "        post_dist_A = posterior_dist_binom(ns=ns_A, ntotal=n_samp_total)\n",
    "        post_dist_B = posterior_dist_binom(ns=ns_B, ntotal=n_samp_total)\n",
    "        pb_gt_pa = prob_pb_gt_pa(post_dist_A, post_dist_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_dist_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_dist_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(b>a) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fe9ea",
   "metadata": {},
   "source": [
    "Experiments stop when the specified confidence level is reached. Time estimates and other stopping criteria are discussed in the appendices [[Apx](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f2a0f",
   "metadata": {},
   "source": [
    "# Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a0af2",
   "metadata": {},
   "source": [
    "The Bayesian approach requires assumptions about the distributions of the quantities being compared. Model selection is always somewhat arbitrary, and its justification is never free of questions. In many cases, a full distribution is unnecessary — comparing means is enough: average revenue per user, average session duration, and so on. For means, the Central Limit Theorem often applies [[CLT](https://en.wikipedia.org/wiki/Central_limit_theorem)]. This allows the normal distribution [[NormDist](https://en.wikipedia.org/wiki/Normal_distribution), [SciPyNorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=norm)] to be used as a likelihood function, even when the shape of the original distribution is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c0cff",
   "metadata": {},
   "source": [
    "The Central Limit Theorem formalizes a simple observation. Take any distribution with mean $\\mu$ and variance $\\sigma^2$. Draw samples of size $N$ and calculate the mean of each sample. The resulting sample means will follow an approximately normal distribution: $Norm(\\mu, \\sigma^2/N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455a8b2",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./figs/en_central_limit_theorem.png\" alt=\"Central Limit Theorem\" width=\"800\"/>\n",
    "<br>\n",
    "<em>\n",
    "    Sample means from any distribution with a finite mean and variance are approximately normally distributed.\n",
    "</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88227652",
   "metadata": {},
   "source": [
    "There are several versions of the central limit theorem [CLT]. One is as follows. Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed random variables with a finite mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$ be the sample mean. As $N$ increases, the distribution of the centered and scaled sample means converges to a normal distribution with mean 0 and variance 1. Convergence here refers to convergence in distribution [[RandVarsConv](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution)].\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P \\left( \\frac{\\overline{X}_N - \\mu}{\\sigma / \\sqrt{N}} = x \\right) & \\to Norm(x; 0, 1), \\quad N \\to \\infty\n",
    "\\\\ \n",
    "Norm(x ; \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\tfrac{(x-\\mu)^2}{2 \\sigma^2} }\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094021f4",
   "metadata": {},
   "source": [
    "The graph compares the distribution of sample means with the normal distribution based on the central limit theorem. A gamma distribution $P(x; \\alpha, \\beta) \\propto x^{\\alpha-1} \\exp(-\\beta x)$ [[GammaDist](https://en.wikipedia.org/wiki/Gamma_distribution), [SciPyGamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html)] is used to generate `n_sample` samples, each containing `sample_len` points. The means of these samples are computed, and their distribution is plotted. Using the exact mean and variance of the original distribution, the parameters for the normal distribution based on the central limit theorem `clt_mu, clt_stdev` are calculated. This distribution is also shown on the graph. Visually, the distribution of the sample means closely approximates the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "sample_len = 100\n",
    "n_samples = 1000\n",
    "\n",
    "exact_dist = stats.gamma(a=a)\n",
    "samp = exact_dist.rvs(size=(n_samples, sample_len))\n",
    "means = np.array([x.mean() for x in samp])\n",
    "clt_mu = exact_dist.mean()\n",
    "clt_stdev = exact_dist.std() / np.sqrt(sample_len)\n",
    "means_stdev = means.std()\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), \n",
    "                         mode='lines', line_color='black', line_dash='solid', name='Original distribution'))\n",
    "fig.add_trace(go.Histogram(x=np.concatenate(samp), histnorm='probability density', name='Sample', nbinsx=500,\n",
    "                           marker_color='black', opacity=0.3))\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.norm.pdf(x, loc=clt_mu, scale=clt_stdev), \n",
    "                         mode='lines', line_color='black', line_dash='dash', name='$Norm(\\mu, \\sigma^2/N)$'))\n",
    "fig.add_trace(go.Histogram(x=means, histnorm='probability density', name='Sample means', nbinsx=50,\n",
    "                           marker_color='green', opacity=0.5))\n",
    "fig.update_layout(title='Sample means',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=550)\n",
    "fig.update_layout(xaxis_range=[0, 5])\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_clt_gamma.png\", scale=2)\n",
    "#The distribution of sample means from a gamma distribution closely approximates a normal distribution, with parameters based on the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb7b62",
   "metadata": {},
   "source": [
    "The central limit theorem states that the distribution of centered and scaled sample means $\\overline{X}_N$ converges to a normal distribution as $N$ approaches infinity. For finite $N$, normality is not guaranteed. The deviation from normality for a finite number of terms is explained by the Berry-Esseen theorem [[BerryEsseenTheorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)]. The difference depends on $N$, the variance, and the skewness of the original distribution. The central limit theorem assumes the original distribution has finite mean and variance. Distributions that may not meet these conditions include the Pareto distribution [[ParetoDist](https://en.wikipedia.org/wiki/Pareto_distribution), [SciPyPareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html)] and the Lomax distribution [[LomaxDist](https://en.wikipedia.org/wiki/Lomax_distribution), [SciPyLomax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lomax.html)]. The probability density function of the Lomax distribution is as follows:\n",
    "\n",
    "$$\n",
    "P(x; c) = \\frac{c}{(1 + x )^{c + 1}}, \\quad x \\ge 0, c > 0.\n",
    "$$\n",
    "\n",
    "For values of the parameter $c$ less than or equal to 2, the variance of the Lomax distribution is infinite.\n",
    "The histograms below show the distribution of `n_samples` sample means, each with `sample_len` terms, along with a normal distribution based on the central limit theorem parameters `clt_mu,clt_stdev`. The distribution of sample means is skewed and deviates more from the normal distribution than in the previous case. This skewness arises from the inclusion of large values from the tail of the original distribution in the samples. In practice, distributions are bounded, so variances and means are finite. The central limit theorem still applies, but a large number of points will be needed to approximate sample means with a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1.7\n",
    "sample_len = 500\n",
    "n_samples = 1000\n",
    "\n",
    "exact_dist = stats.lomax(c=c)\n",
    "samp = exact_dist.rvs(size=(n_samples, sample_len))\n",
    "means = np.array([x.mean() for x in samp])\n",
    "clt_mu = exact_dist.mean()\n",
    "clt_stdev = exact_dist.std() / np.sqrt(sample_len)\n",
    "means_stdev = means.std()\n",
    "\n",
    "xaxis_max=10\n",
    "x = np.linspace(0, xaxis_max, 2000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), \n",
    "                         mode='lines', line_color='black', line_dash='solid', name='Original distribution'))\n",
    "fig.add_trace(go.Histogram(x=np.concatenate(samp)[np.concatenate(samp) < xaxis_max], histnorm='probability density', \n",
    "                           name='Sample', nbinsx=500,\n",
    "                           marker_color='black', opacity=0.3))\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.norm.pdf(x, loc=clt_mu, scale=means_stdev), \n",
    "                         mode='lines', line_color='black', line_dash='dash', name='$Norm(\\mu, \\sigma^2/N)$'))\n",
    "fig.add_trace(go.Histogram(x=means, histnorm='probability density', name='Sample means', nbinsx=150,\n",
    "                          marker_color='green', opacity=0.5))\n",
    "fig.update_layout(title='Sample means',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  xaxis_range=[0, xaxis_max],\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_clt_lomax.png\", scale=2)\n",
    "#The variance of the Lomax distribution is unbounded for certain parameters. \n",
    "#The distribution of sample means deviates from normal. \n",
    "#For heavily skewed distributions, even with finite mean and variance, a large number of points is needed to approximate sample means with a normal distribution.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb9782",
   "metadata": {},
   "source": [
    "For Bayesian estimation of the parameters of a normal distribution based on a sample, the likelihood function is given by $P(\\mathcal{D} | \\mathcal{H}) = Norm(x | \\mu, \\sigma_x^2)$. This function has two parameters: $\\mu$ and $\\sigma_x$. For this model, there exists a conjugate prior distribution [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior), [Apx](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices)]. Below is a simplified version of this model is used where only $\\mu$ is adjusted, and $\\sigma_x$ is fixed. The conjugate prior distribution for $\\mu$ is normal $P(\\mu) = Norm(\\mu | \\mu_0, \\sigma_0^2)$, with parameters $\\mu_0$ and $\\sigma_0$. To calculate the posterior distribution, multiply the likelihood functions for all data points $x_i$: $P(\\mathcal{H} | \\mathcal{D}) \\propto \\prod_i^N Norm(x_i | \\mu, \\sigma_x^2) Norm(\\mu | \\mu_0, \\sigma_0^2)$. Only the dependence on $\\mu$ matters in the transformations, as the terms without $\\mu$ will contribute to the normalization constant. The posterior distribution remains normal: $P(\\mu | \\mathcal{D}) = Norm(\\mu | \\mu_N, \\sigma_N^2)$ with updated parameters $\\mu_N$ and $\\sigma_N$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Norm(x | \\mu, \\sigma_x^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_x^2}} e^{-\\tfrac{(x - \\mu)^2}{2 \\sigma_x^2}}\n",
    "\\\\\n",
    "P(\\mathcal{H}) & = Norm(\\mu | \\mu_0, \\sigma_0^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_{0}^2}} e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_{0}^2}} \n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto\n",
    "\\prod_i^N\n",
    "Norm(x_i | \\mu, \\sigma_x^2)\n",
    "Norm(\\mu | \\mu_0, \\sigma_0^2)\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "\\prod_i^N\n",
    "e^{-\\tfrac{(x_i - \\mu)^2}{2 \\sigma_x^2}}\n",
    "e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_0^2}} \n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\mu^2 \\left[\\tfrac{N}{2 \\sigma_x^2} + \\tfrac{1}{2 \\sigma_0^2} \\right] + \n",
    "   2\\mu \\left[\\tfrac{\\mu_0}{2 \\sigma_0^2} + \\tfrac{1}{2 \\sigma_x^2} \\sum_i^N x_i \\right]}\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\tfrac{(\\mu - \\mu_N)^2}{2 \\sigma_N^2}}\n",
    "= Norm(\\mu | \\mu_N, \\sigma_N^2),\n",
    "\\quad\n",
    "\\sigma_N^2 = \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_x^2 + N \\sigma_0^2},\n",
    "\\quad\n",
    "\\mu_N = \\mu_0 \\frac{\\sigma_N^2}{\\sigma_0^2} + \\frac{\\sigma_N^2}{\\sigma_x^2} \\sum_i^N x_i\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda70fb7",
   "metadata": {},
   "source": [
    "To verify the construction of the posterior distribution from data, a normal distribution with parameters `mu` and `sigma` is defined, and a sample of size `nsample` is generated. The initial parameters $\\sigma_x, \\sigma_0$ are set to the sample’s standard deviation; $\\mu_0$ is set to the first data point. The posterior parameters $\\mu_N, \\sigma_N$ are computed using the remaining data. Using the full dataset to define prior parameters is incorrect — it's better to use a subset or historical data. The first plot compares the posterior distribution of $\\mu$ to the true mean. The mode is close to both the sample mean and the true mean. The second plot compares the posterior predictive distribution of $x$ to the original distribution. To build this distribution, sample $\\mu \\sim Norm(\\mu | \\mu_N, \\sigma_N^2)$, then sample $x \\sim Norm(x | \\mu, \\sigma_x^2)$. The resulting histogram of $x$ visually matches the original normal distribution. The final plot compares the distributions of $x$ and $\\mu$. These are fundamentally different: $P(x|\\mathcal{D})$ approximates the original distribution, while $P(\\mu|\\mathcal{D})$ reflects uncertainty in the mean estimate. The distribution of $\\mu$ is significantly narrower, with variance $\\sigma_N$, while the variance of $P(x|\\mathcal{D})$ depends on both $\\sigma_x$ and the variation in $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConjugateNormalParams = namedtuple('ConjugateNormalParams', 'mu sigma sx')\n",
    "\n",
    "def initial_params_normal(mu, sigma, sx):\n",
    "    return ConjugateNormalParams(mu=mu, sigma=sigma, sx=sx)\n",
    "\n",
    "def posterior_params_normal(data, initial_pars):\n",
    "    N = len(data)\n",
    "    sigma_n_2 = (initial_pars.sigma**2 * initial_pars.sx**2) / (initial_pars.sx**2 + N * initial_pars.sigma**2)\n",
    "    mu_n = initial_pars.mu * sigma_n_2 / initial_pars.sigma**2 + np.sum(data) * sigma_n_2 / initial_pars.sx**2    \n",
    "    return ConjugateNormalParams(mu=mu_n, sigma=np.sqrt(sigma_n_2), sx=initial_pars.sx)\n",
    "\n",
    "def posterior_mu_dist(params):\n",
    "    return stats.norm(loc=params.mu, scale=params.sigma)\n",
    "\n",
    "def posterior_rvs(params, nsamp):\n",
    "    mus = stats.norm.rvs(loc=params.mu, scale=params.sigma, size=nsamp)\n",
    "    return stats.norm.rvs(loc=mus, scale=params.sx, size=nsamp)\n",
    "\n",
    "mu = 3\n",
    "sigma = 1\n",
    "nsample = 1000\n",
    "npostsamp = 100000\n",
    "\n",
    "exact_dist = stats.norm(loc=mu, scale=sigma)\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "sx = np.std(data)\n",
    "mu0 = data[0]\n",
    "sigma0 = np.std(data)\n",
    "pars = initial_params_normal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_normal(data[1:], pars)\n",
    "post_mu = posterior_mu_dist(pars)\n",
    "post_samp = posterior_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_mu.pdf(x), line_color='black', name='$\\mbox{Posterior }\\mu$'))\n",
    "fig.add_trace(go.Scatter(x=[data.mean(), data.mean()], y=[0, max(post_mu.pdf(x))], \n",
    "                         line_color='black', mode='lines', line_dash='dash', name='Sample mean'))\n",
    "fig.add_trace(go.Scatter(x=[exact_dist.mean(), exact_dist.mean()], y=[0, max(post_mu.pdf(x))*1.05], \n",
    "                         line_color='red', mode='lines', line_dash='dash', name='Exact mean'))\n",
    "fig.update_layout(title='$\\mbox{Posterior distribution } \\mu$',\n",
    "                  xaxis_title='$\\mu$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[2, 4],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_norm_postdist_mu.png\", scale=2)\n",
    "# The mode of the mu distribution is close to the true mean and the sample mean.\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), line_color='red', line_dash='dash', name='Exact distibution'))\n",
    "fig.add_trace(go.Histogram(x=post_samp, histnorm='probability density', name='Posterior sample x', nbinsx=100,\n",
    "                           marker_color='black', opacity=0.8))\n",
    "fig.update_layout(title='Posterior sample x',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  #xaxis_range=[0, 10],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_norm_postdist_x.png\", scale=2)\n",
    "#The posterior predictive distribution of xx closely matches the original normal distribution.\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_mu.pdf(x), line_color='black', name='$\\mbox{Posterior }\\mu$'))\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), line_color='red', line_dash='dash', name='Exact distribution'))\n",
    "fig.add_trace(go.Histogram(x=post_samp, histnorm='probability density', name='Posterior sample x', nbinsx=100,\n",
    "                           marker_color='black', opacity=0.8))\n",
    "fig.update_layout(title='$\\mbox{Distributions of } x \\mbox{ and } \\mu$',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[0, 6],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_norm_postdist_mu_x.png\", scale=2)\n",
    "#Comparison of the posterior distributions of x and mu. The distribution of mu estimates the original mean; the distribution of x approximates the full original normal distribution. The mu distribution is much narrower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832248a",
   "metadata": {},
   "source": [
    "The estimation of a mean from an arbitrary distribution is illustrated using the gamma distribution. A sample of `nsample` points is divided into segments of `nsplit` points. The mean is computed for each segment. These sample means `means`, are assumed to follow a normal distribution and are modeled using Bayesian inference. The initial parameters, $\\sigma_x$ and $\\sigma_0$, are set to the standard deviation of the sample means `sx = np.std(means)`, and $\\mu_0$ is set to the first sample mean `mu0 = means[0]`. The segment size `nsplit = 100` is chosen arbitrarily; the Berry–Esseen theorem can guide a more accurate choice. Using the full sample mean instead is possible, but this would leave only one point for estimating $P(\\mu|\\mathcal{D})$, making model validation infeasible. It's better to treat nsplit as a model hyperparameter. The first plot shows the original distribution and the distribution of the sample means, which is roughly normal. The second plot displays the posterior distribution of $\\mu$, whose mode is close to the sample and true means. The third plot shows the posterior predictive distribution of the sample means. It resembles a normal distribution with parameters from the central limit theorem. As before, the posterior distribution of $\\mu$ is narrower than that of the sample means. For mean comparison, focus on the distribution of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37578db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_compute_means(sample, n_split):\n",
    "    n_means = len(sample) // n_split\n",
    "    samp_reshaped = np.reshape(sample[0 : n_means * n_split], (n_means, n_split))\n",
    "    means = np.array([x.mean() for x in samp_reshaped])\n",
    "    return means\n",
    "\n",
    "def exact_clt_dist(exact_dist, n_split):\n",
    "    clt_mu = exact_dist.mean()\n",
    "    clt_stdev = exact_dist.std() / np.sqrt(n_split)\n",
    "    return stats.norm(loc=clt_mu, scale=clt_stdev)\n",
    "\n",
    "def sample_clt_dist(means):\n",
    "    clt_mu = means.mean()\n",
    "    clt_std = means.std()\n",
    "    return stats.norm(loc=clt_mu, scale=clt_std)\n",
    "\n",
    "nsample = 50000\n",
    "nsplit = 100\n",
    "npostsamp = 50000\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "exact_dist = stats.gamma(a=a, scale=1/b)\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "means = reshape_and_compute_means(data, nsplit)\n",
    "clt_dist_exact = exact_clt_dist(exact_dist, nsplit)\n",
    "\n",
    "sx = np.std(means)\n",
    "mu0 = means[0]\n",
    "sigma0 = sx\n",
    "pars = initial_params_normal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_normal(means[1:], pars)\n",
    "post_mu = posterior_mu_dist(pars)\n",
    "post_samp = posterior_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), \n",
    "                         mode='lines', line_color='black', line_dash='solid', name='Original distribution'))\n",
    "fig.add_trace(go.Scatter(x=x, y=clt_dist_exact.pdf(x), \n",
    "                         mode='lines', line_color='black', line_dash='dash', name='$Norm(\\mu, \\sigma^2/N)$'))\n",
    "fig.add_trace(go.Histogram(x=means, histnorm='probability density', name='Sample means', nbinsx=50,\n",
    "                           marker_color='green', opacity=0.5))\n",
    "fig.update_layout(title='Sample means',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=550)\n",
    "fig.update_layout(xaxis_range=[0, 3])\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_gamma_means.png\", scale=2)\n",
    "#Original Gamma Distribution and Sample Means. The sample means closely follow a normal distribution, as expected from the central limit theorem.\n",
    "\n",
    "x = np.linspace(0, 4, 10000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_mu.pdf(x), line_color='black', name='$\\mu \\mbox{ distribution}$'))\n",
    "fig.add_trace(go.Scatter(x=[data.mean(), data.mean()], y=[0, max(post_mu.pdf(x))], \n",
    "                         line_color='black', mode='lines', line_dash='dash', name='Sample mean'))\n",
    "fig.add_trace(go.Scatter(x=[exact_dist.mean(), exact_dist.mean()], y=[0, max(post_mu.pdf(x))*1.05], \n",
    "                         line_color='red', mode='lines', line_dash='dash', name='Exact mean'))\n",
    "fig.update_layout(title='$\\mbox{Distribution of }\\mu$',\n",
    "                  xaxis_title='$\\mu$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[exact_dist.mean()-0.1, exact_dist.mean()+0.1],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_gamma_postdist_mu.png\", scale=2)\n",
    "#Estimate of mu Based on Sample Means. The mode of the distribution is close to the true mean of the gamma distribution.\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), line_dash='solid', line_color='black', name='Original distribution'))\n",
    "fig.add_trace(go.Scatter(x=x, y=clt_dist_exact.pdf(x), \n",
    "                         mode='lines', line_color='black', line_dash='dash', name='$Norm(\\mu, \\sigma^2/N)$'))\n",
    "fig.add_trace(go.Histogram(x=post_samp, histnorm='probability density', name='$\\mbox{Posterior } \\overline{X}_N$', nbinsx=300,\n",
    "                           marker_color='black', opacity=0.2))\n",
    "fig.update_layout(title='$\\mbox{Posterior distribution } \\overline{X}_N$',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[0, 3],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_gamma_postdist_means.png\", scale=2)\n",
    "#Posterior Predictive Distribution of Sample Means. The posterior predictive distribution of the sample means is close to a normal distribution, as expected from the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82526e6d",
   "metadata": {},
   "source": [
    "For the group comparison example, two gamma distributions are defined. The parameters $a$ are identical, while the parameter $b$ for group B is 5% smaller than that of group A. A sample of size nsample is drawn from each, and sample means are calculated. The posterior distributions of $\\mu$ are constructed based on the sample means. From these distributions, the probability that the mean for group B is greater than for group A, $P(\\mu_B > \\mu_A)$, is computed. In the first plot, the original distributions and exact means are shown. In the second plot, the distributions of $\\mu$ and the exact means are shown. With the chosen samples, the distributions $P(\\mu|\\mathcal{D})$ barely overlap, and $P(\\mu_B > \\mu_A) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab84338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "nsample = 30000\n",
    "npostsamp = 50000\n",
    "nsplit = 100\n",
    "\n",
    "A, B = {}, {}\n",
    "A['dist_pars'] = {'a': 1, 'b': 2}\n",
    "B['dist_pars'] = {'a': 1, 'b': 2*0.95}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.gamma(a=g['dist_pars']['a'], scale=1/g['dist_pars']['b'])\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['means'] = reshape_and_compute_means(g['data'], nsplit)\n",
    "    g['post_pars'] = initial_params_normal(mu=g['means'][0], sigma=np.std(g['means']), sx=np.std(g['means']))\n",
    "    g['post_pars'] = posterior_params_normal(g['means'][1:], g['post_pars'])\n",
    "    g['post_mu'] = posterior_mu_dist(g['post_pars'])\n",
    "    g['post_samp'] = posterior_rvs(g['post_pars'], npostsamp)\n",
    "\n",
    "    \n",
    "x = np.linspace(0, 3, 5000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=A['exact_dist'].pdf(x), line_color='black', opacity=0.2, name='A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=B['exact_dist'].pdf(x), line_color='black', name='B'))\n",
    "fig.add_trace(go.Scatter(x=[A['exact_dist'].mean(), A['exact_dist'].mean()], y=[0, max(A['exact_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', opacity=0.2, name='Exact mean A'))\n",
    "fig.add_trace(go.Scatter(x=[B['exact_dist'].mean(), B['exact_dist'].mean()], y=[0, max(B['exact_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', name='Exact mean B'))\n",
    "fig.update_layout(title='Original distributions',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[0, 3],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_gamma_cmp_orig.png\", scale=2)\n",
    "#Original gamma distributions and exact means. The mean for group B is greater than that for group A.\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=A['post_mu'].pdf(x), line_color='black', opacity=0.2, name='A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=B['post_mu'].pdf(x), line_color='black', name='B'))\n",
    "fig.add_trace(go.Scatter(x=[A['exact_dist'].mean(), A['exact_dist'].mean()], y=[0, max(A['post_mu'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', opacity=0.2, name='Exact mean A'))\n",
    "fig.add_trace(go.Scatter(x=[B['exact_dist'].mean(), B['exact_dist'].mean()], y=[0, max(B['post_mu'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='black', name='Exact mean B'))\n",
    "fig.update_layout(title='$\\mbox{Distributions of } \\mu$',\n",
    "                  xaxis_title='$\\mu$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[A['exact_dist'].mean()-0.1, A['exact_dist'].mean()+0.1],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch4_gamma_cmp_mu.png\", scale=2)\n",
    "#Estimates of μ from sample means. Based on the collected data, the mean of group B is greater than that of group A with a probability of 1.\n",
    "\n",
    "print(f\"P(mu_B > mu_A): {prob_pb_gt_pa(A['post_mu'], B['post_mu'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b9afd",
   "metadata": {},
   "source": [
    "To demonstrate the proportion of correctly identified options, two groups with gamma distributions are set up. In group A, the parameters of the gamma distribution are fixed, while in group B, the parameter bb changes within $\\pm5\\%$ of group A. Along with the parameters, the means change. Data are generated from the distributions with a step size of `n_samp_step`. At each step, sample means `nsplit` are calculated. The parameters $\\mu$ are estimated based on the sample means. These distributions of parameters are compared. The data collection stops when either $P(\\mu_B > \\mu_A)$ or $P(\\mu_A > \\mu_B)$ reaches `prob_stop = 0.95`, or the maximum number of points `n_samp_max` is reached. A total of `nexps` experiments are conducted, and the proportion of correctly identified groups with the larger mean is calculated. In this example, the proportion of 0.97 is close to `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "nexps = 100\n",
    "prob_stop = 0.95\n",
    "nsplit = 100\n",
    "n_samp_max = 1_000_000\n",
    "n_samp_step = 10_000\n",
    "\n",
    "A = {'a': 1, 'b': 2}\n",
    "\n",
    "cmp = pd.DataFrame(columns=['A_pars', 'B_pars', 'A_mean', 'B_mean', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "cmp['A_pars'] = [A] * nexps\n",
    "cmp['B_pars'] = cmp['A_pars'].apply(lambda x: {'a': x['a'], 'b': x['b'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1))})\n",
    "cmp['A_mean'] = cmp['A_pars'].apply(lambda x: stats.gamma(a=x['a'], scale=1/x['b']).mean())\n",
    "cmp['B_mean'] = cmp['B_pars'].apply(lambda x: stats.gamma(a=x['a'], scale=1/x['b']).mean())\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B_mean'] > r['A_mean'] else 'A', axis=1)\n",
    "\n",
    "for i in range(nexps):\n",
    "    A_pars = cmp.at[i, 'A_pars']\n",
    "    B_pars = cmp.at[i, 'B_pars']\n",
    "    exact_dist_A = stats.gamma(a=A_pars['a'], scale=1/A_pars['b'])\n",
    "    exact_dist_B = stats.gamma(a=B_pars['a'], scale=1/B_pars['b'])\n",
    "    n_samp_total = 0\n",
    "    dA = []\n",
    "    dB = []\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA.extend(exact_dist_A.rvs(n_samp_step))\n",
    "        dB.extend(exact_dist_B.rvs(n_samp_step))\n",
    "        n_samp_total += n_samp_step\n",
    "        means_A = reshape_and_compute_means(dA, nsplit)\n",
    "        post_pars_A = initial_params_normal(mu=means_A[0], sigma=np.std(means_A), sx=np.std(means_A))\n",
    "        post_pars_A = posterior_params_normal(means_A[1:], post_pars_A)\n",
    "        post_mu_A = posterior_mu_dist(post_pars_A)\n",
    "        means_B = reshape_and_compute_means(dB, nsplit)\n",
    "        post_pars_B = initial_params_normal(mu=means_B[0], sigma=np.std(means_B), sx=np.std(means_B))\n",
    "        post_pars_B = posterior_params_normal(means_B[1:], post_pars_B)\n",
    "        post_mu_B = posterior_mu_dist(post_pars_B)\n",
    "        pb_gt_pa = prob_pb_gt_pa(post_mu_A, post_mu_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_mu_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_mu_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(B>A) {pb_gt_pa}')\n",
    "\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(8))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a347639",
   "metadata": {},
   "source": [
    "# Revenue per User"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908983d",
   "metadata": {},
   "source": [
    "To estimate the monetary effect, the revenue per user in the groups $P_{\\text{users}}(x)$ is compared. It is useful to separate the revenue for paying users, denoted as $P_{\\text{paying}}(x)$. With a conversion rate $p$, the distribution of non-zero revenue per user is $pP_{\\text{paying}}(x)$, and with probability $1−p$, the revenue is zero.\n",
    "\n",
    "$$\n",
    "P_{\\text{users}}(x) = \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p P_{\\text{paying}}(x), \\, x > 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b73ebe",
   "metadata": {},
   "source": [
    "The conversion rate $p$ was estimated earlier. The revenue per paying user can be modeled using a log-normal distribution [[LognormDist](https://en.wikipedia.org/wiki/Log-normal_distribution),\n",
    "[SciPyLognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html)] or a Pareto distribution [[ParetoDist](https://en.wikipedia.org/wiki/Pareto_distribution), [SciPyPareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html)], similar to wealth distribution. For transactional services, particularly marketplaces, the log-normal distribution is more appropriate. A random variable $X$ is log-normal $X \\sim Lognormal(\\mu, s^2)$ if the logarithm of $X$ is normally distributed $\\ln(X) \\sim Norm(\\mu, s^2)$. The probability density function is provided below.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(x) & = \\frac{1}{x s \\sqrt{2 \\pi}} e^{-\\tfrac{(\\ln(x) - \\mu)^2}{2 s^2}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943467ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 20, 2000)\n",
    "fig = go.Figure()\n",
    "mu, s = 1, 1\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'$\\mu={mu}, \\, s={s}$'))\n",
    "mu, s = 2, 1\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'$\\mu={mu}, \\, s={s}$'))\n",
    "mu, s = 1, 2\n",
    "fig.add_trace(go.Scatter(x=x, y=stats.lognorm.pdf(x, s=s, scale=np.exp(mu)), \n",
    "                             mode='lines', line_color='black', line_dash='solid',\n",
    "                             name=f'$\\mu={mu}, \\, s={s}$'))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[2.90, 11.2, 1.80],\n",
    "    y=[0.25, 0.06, 0.48],\n",
    "    mode=\"text\",\n",
    "    name=None,\n",
    "    showlegend=False,\n",
    "    text=[\"$\\mu=1, \\, s=1$\", \"$\\mu=2, \\, s=1$\", \"$\\mu=1, \\, s=2$\"],\n",
    "    textposition=\"middle center\"\n",
    "))\n",
    "fig.update_layout(title='Log-normal distribution',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  hovermode=\"x\",\n",
    "                  showlegend=False,\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch5_lognorm.png\", scale=2)\n",
    "#Log-normal distribution with different parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014435d5",
   "metadata": {},
   "source": [
    "The conjugate prior distribution for the log-normal likelihood function $P(\\mathcal{D} | \\mathcal{H}) = Lognorm(x | \\mu, s^2)$ is constructed similarly to that of the normal distribution [[ConjPrior](https://en.wikipedia.org/wiki/Conjugate_prior)]. In a simplified model where only the parameter $\\mu$ is estimated and the scale parameter $s$ is fixed, the conjugate prior for $\\mu$ is a normal distribution $P(\\mu) = Norm(\\mu | \\mu_0, \\sigma_0^2)$ with parameters $\\mu_0$ and $\\sigma_0$. The resulting posterior is also a normal distribution $P(\\mu | \\mathcal{D}) = Norm(\\mu | \\mu_N, \\sigma_N^2)$, with updated parameters $\\mu_N$ and $\\sigma_N$. The posterior mean $\\mu_N$ is computed using the logarithms of the observed sample values.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Lognorm(x | \\mu, s^2) = \n",
    "\\frac{1}{x \\sqrt{2 \\pi s^2}} e^{-\\tfrac{(\\ln x - \\mu)^2}{2 s^2}}\n",
    "\\\\\n",
    "P(\\mathcal{H}) & = Norm(\\mu | \\mu_0, \\sigma_0^2) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma_{0}^2}} e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_{0}^2}} \n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto\n",
    "\\prod_i^N\n",
    "Lognorm(x_i | \\mu, s^2)\n",
    "Norm(\\mu | \\mu_0, \\sigma_0^2)\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "\\prod_i^N\n",
    "e^{-\\tfrac{(\\ln x_i - \\mu)^2}{2 s^2}}\n",
    "e^{-\\tfrac{(\\mu-\\mu_0)^2}{2 \\sigma_0^2}} \n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\mu^2 \\left[\\tfrac{N}{2 s^2} + \\tfrac{1}{2 \\sigma_0^2} \\right] + \n",
    "   2\\mu \\left[\\tfrac{\\mu_0}{2 \\sigma_0^2} + \\tfrac{1}{2 s^2} \\sum_i^N \\ln x_i \\right]}\n",
    "\\\\\n",
    "& \\propto_{\\mu}\n",
    "e^{-\\tfrac{(\\mu - \\mu_N)^2}{2 \\sigma_N^2}}\n",
    "= Norm(\\mu | \\mu_N, \\sigma_N^2),\n",
    "\\quad\n",
    "\\sigma_N^2 = \\frac{\\sigma_0^2 s^2}{s^2 + N \\sigma_0^2},\n",
    "\\quad\n",
    "\\mu_N = \\mu_0 \\frac{\\sigma_N^2}{\\sigma_0^2} + \\frac{\\sigma_N^2}{s^2} \\sum_i^N \\ln x_i\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789d8c8",
   "metadata": {},
   "source": [
    "To illustrate posterior inference from a log-normal distribution, a sample of size `nsample` is generated using parameters `mu` and `s`. The logarithm of the sample values is computed. The parameters $s$ and $\\sigma_0$ are set equal to the standard deviation of the log-transformed sample, and $\\mu_0$ is initialized as the value of the first point. The remaining points are used to compute the posterior parameters $\\mu_N$ and $\\sigma_N$. The posterior distribution of $\\mu$ is shown in the first plot. The mean of a log-normal distribution is given by $E[x] = \\exp(\\mu + s^2/2)$, so $\\mu + s^2/2$ should estimate the logarithm of the true mean. Since $\\mu$ has normal distribution $P(\\mu) = Norm(\\mu_N, \\sigma_N^2)$, the expression $\\mu + s^2/2$ is also normally distributed as $Norm(\\mu_N + s^2/2, \\sigma_N^2)$. In the first plot, the mode of the distribution $\\mu + s^2/2$ is close to the logarithm of the sample mean and the exact mean. The second plot compares the posterior predictive distribution of $x$ with the original distribution. The histogram of predicted $x$ values closely matches the original log-normal distribution.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(x) & = Lognorm(x | \\mu, s^2)\n",
    "\\\\\n",
    "E[x] & = e^{\\mu + s^2/2}\n",
    "\\\\\n",
    "P(\\mu) & = Norm(\\mu | \\mu_N, \\sigma_N^2)\n",
    "\\\\\n",
    "P_{\\mu + s^2/2}(y) & = Norm(y | \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConjugateLognormalParams = namedtuple('ConjugateLognormalParams', 'mu sigma sx')\n",
    "\n",
    "def initial_params_lognormal(mu, sigma, sx):\n",
    "    return ConjugateLognormalParams(mu=mu, sigma=sigma, sx=sx)\n",
    "\n",
    "def posterior_params_lognormal(data, initial_pars):\n",
    "    N = len(data)\n",
    "    lnx = np.log(data)\n",
    "    sigma_n_2 = (initial_pars.sigma**2 * initial_pars.sx**2) / (initial_pars.sx**2 + N * initial_pars.sigma**2)\n",
    "    mu_n = initial_pars.mu * sigma_n_2 / initial_pars.sigma**2 + np.sum(lnx) * sigma_n_2 / initial_pars.sx**2    \n",
    "    return ConjugateLognormalParams(mu=mu_n, sigma=np.sqrt(sigma_n_2), sx=initial_pars.sx)\n",
    "\n",
    "def posterior_mu_dist_lognormal(params):\n",
    "    return stats.norm(loc=params.mu, scale=params.sigma)\n",
    "\n",
    "def posterior_lognormal_rvs(params, nsamp):\n",
    "    mus = stats.norm.rvs(loc=params.mu, scale=params.sigma, size=nsamp)\n",
    "    return stats.lognorm.rvs(s=params.sx, scale=np.exp(mus), size=nsamp)\n",
    "\n",
    "def posterior_mean_dist_lognormal(params):\n",
    "    return stats.lognorm(scale=np.exp(params.mu + params.sx**2/2), s=params.sigma)\n",
    "\n",
    "def posterior_ln_mean_dist_lognormal(params):\n",
    "    return stats.norm(loc=params.mu + params.sx**2/2, scale=params.sigma)\n",
    "    \n",
    "s = 1\n",
    "mu = 1.5\n",
    "nsample = 1000\n",
    "\n",
    "exact_dist = stats.lognorm(s=s, scale=np.exp(mu))\n",
    "data = exact_dist.rvs(nsample)\n",
    "\n",
    "lnx = np.log(data)\n",
    "sx = np.std(lnx)\n",
    "mu0 = lnx[0]\n",
    "sigma0 = sx\n",
    "\n",
    "pars = initial_params_lognormal(mu=mu0, sigma=sigma0, sx=sx)\n",
    "pars = posterior_params_lognormal(data[1:], pars)\n",
    "post_mu = posterior_mu_dist_lognormal(pars)\n",
    "post_lnmeans = posterior_ln_mean_dist_lognormal(pars)\n",
    "npostsamp = 10000\n",
    "post_samp = posterior_lognormal_rvs(pars, npostsamp)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=post_mu.pdf(x), line_color='black', name='$\\mu \\mbox{ distribution}$'))\n",
    "fig.add_trace(go.Scatter(x=x, y=post_lnmeans.pdf(x), line_color='black', opacity=0.2, name='$\\mu + s^2/2 \\mbox{ distribution}$'))\n",
    "fig.add_trace(go.Scatter(x=[np.log(data.mean()), np.log(data.mean())], y=[0, max(post_mu.pdf(x))], \n",
    "                         line_color='black', mode='lines', line_dash='dash', name='Logarithm of sample mean'))\n",
    "fig.add_trace(go.Scatter(x=[np.log(exact_dist.mean()), np.log(exact_dist.mean())], y=[0, max(post_mu.pdf(x))*1.05], \n",
    "                         line_color='red', mode='lines', line_dash='dash', name='Logarithm of exact mean'))\n",
    "fig.update_layout(title='$\\mbox{Posterior distibutions of } \\mu \\mbox{ and } \\mu + s^2/2$',\n",
    "                  xaxis_title='$\\mu$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  #xaxis_range=[2, 4],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch5_lognorm_postdist_mu_mean.png\", scale=2)\n",
    "#The posterior distribution of μ + s²⁄2 is close to the logarithm of the true mean.\n",
    "\n",
    "xaxis_max=20\n",
    "x = np.linspace(0, xaxis_max, 10000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pdf(x), line_dash='dash', line_color='red', name='Exact distribution'))\n",
    "fig.add_trace(go.Histogram(x=post_samp[post_samp < xaxis_max], histnorm='probability density', name='Posterior sample', nbinsx=100,\n",
    "                          marker_color='black', opacity=0.8))\n",
    "fig.update_layout(title='$\\mbox{Posterior distibution } x$',\n",
    "                  xaxis_title='$x$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  #xaxis_range=[0, 10],\n",
    "                  barmode='overlay',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)                  \n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch5_lognorm_postdist_x.png\", scale=2)\n",
    "#The posterior predictive distribution of x closely matches the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcfa8c",
   "metadata": {},
   "source": [
    "To compare groups by expected revenue per paying user, use $E[x]=\\exp(\\mu + s^2/2)$. It's sufficient to compare $\\mu + s^2/2$. This quantity is normally distributed as $Norm(\\mu + s^2/2 | \\mu_N, \\sigma_N)$. In this example, two log-normal distributions are defined: one with parameters `s, mu`, and the other with `mu` increased by 5\\%. A sample of `nsample` points is generated. Posterior distributions are constructed. The probability that the expected revenue in group B exceeds that of group A, $P(E[x]_B > E[x]_A)$, is close to 1. The first plot shows the original distributions and their exact means. The second shows the distributions $Norm(\\mu + s^2/2 | \\mu_N, \\sigma_N)$ for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_pb_gt_pa(post_dist_A, post_dist_B, post_samp=100_000):\n",
    "    sa = post_dist_A.rvs(size=post_samp)\n",
    "    sb = post_dist_B.rvs(size=post_samp)\n",
    "    b_gt_a = np.sum(sb > sa)\n",
    "    return b_gt_a / post_samp\n",
    "\n",
    "nsample = 3000\n",
    "npostsamp = 50000\n",
    "\n",
    "A, B = {}, {}\n",
    "s = 1\n",
    "mu = 2\n",
    "A['dist_pars'] = {'s': s, 'scale': np.exp(mu)}\n",
    "B['dist_pars'] = {'s': s, 'scale': np.exp(mu * 1.05)}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.lognorm(s=g['dist_pars']['s'], scale=g['dist_pars']['scale'])\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['post_pars'] = initial_params_lognormal(mu=np.log(g['data'])[0], sigma=np.std(np.log(g['data'])), sx=np.std(np.log(g['data'])))\n",
    "    g['post_pars'] = posterior_params_lognormal(g['data'][1:], g['post_pars'])\n",
    "    g['post_ln_means_dist'] = posterior_ln_mean_dist_lognormal(g['post_pars'])\n",
    "    \n",
    "x = np.linspace(0, 30, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=A['exact_dist'].pdf(x), line_color='black', opacity=0.2, name='Original A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=B['exact_dist'].pdf(x), line_color='black', name='Original B'))\n",
    "fig.add_trace(go.Scatter(x=[A['exact_dist'].mean(), A['exact_dist'].mean()], y=[0, max(A['exact_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='red', opacity=0.2, name='Exact mean A'))\n",
    "fig.add_trace(go.Scatter(x=[B['exact_dist'].mean(), B['exact_dist'].mean()], y=[0, max(B['exact_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='red', name='Exact mean B'))\n",
    "fig.update_layout(title='Original distributions',\n",
    "                  xaxis_title='x',\n",
    "                  yaxis_title='Probability density',\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch5_lognorm_cmp_orig.png\", scale=2)\n",
    "#Original distributions and exact means.\n",
    "\n",
    "x = np.linspace(0, 3, 1000)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=A['post_ln_means_dist'].pdf(x), line_color='black', opacity=0.2, name='A'))\n",
    "fig.add_trace(go.Scatter(x=x, y=B['post_ln_means_dist'].pdf(x), line_color='black', name='B'))\n",
    "fig.add_trace(go.Scatter(x=[np.log(A['exact_dist'].mean()), np.log(A['exact_dist'].mean())], y=[0, max(A['post_ln_means_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='red', opacity=0.3, name='Logarithm of exact mean A'))\n",
    "fig.add_trace(go.Scatter(x=[np.log(B['exact_dist'].mean()), np.log(B['exact_dist'].mean())], y=[0, max(B['post_ln_means_dist'].pdf(x))*1.05], \n",
    "                         mode='lines', line_dash='dash', line_color='red', name='Logarithm of exact mean B'))\n",
    "fig.update_layout(title='$\\mbox{Distribution of } \\mu + s^2/2$',\n",
    "                  xaxis_title='$\\mu$',\n",
    "                  yaxis_title='Probability density',\n",
    "                  xaxis_range=[2, 3],\n",
    "                  hovermode=\"x\",\n",
    "                  height=500)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch5_lognorm_cmp_means.png\", scale=2)\n",
    "#Distributions of estimated log-means mu + s^2/2. Mean for group B is greater than A with probability 1.\n",
    "\n",
    "print(f\"P(E[x]_B > E[x]_A): {prob_pb_gt_pa(A['post_ln_means_dist'], B['post_ln_means_dist'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411f12e",
   "metadata": {},
   "source": [
    "The share of correctly identified winning groups is evaluated for user revenue distributions, $P_{\\text{users}}(x)$. Group A is assigned a fixed conversion rate $p$ and parameters $\\mu, s$ for revenue per paying user. For group B, $p$ and $\\mu$ are randomly varied within $\\pm 5\\%$ of A’s values. These parameters are changed independently, though in practice they often shift together in opposite directions. Groups are compared by expected user revenue $E_{\\text{users}}[x] = p \\exp(\\mu + s^2/2)$. The conversion rate $p$ is estimated using a Beta distribution: $P(p) = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s)$, where $N$ is the total number of users, and $n_s$ is the number of paying users. Revenue per paying user is modeled with a log-normal distribution. Since $\\mu + s^2/2$ is normally distributed, the expected revenue $\\exp(\\mu + s^2/2)$ follows a log-normal distribution $P_{\\exp(\\mu + s^2/2)}(y) = Lognorm(y | \\mu_N + s^2/2, \\sigma_N^2)$. Thus, the distribution of $p\\exp(\\mu + s^2/2)$ is modeled as the product of a Beta and a log-normal distribution $P_{p\\exp(\\mu + s^2/2)} \\sim \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s) Lognorm(y ; \\mu_N + s^2/2, \\sigma_N^2)$. In each experiment, data is added in increments of `n_samp_step`. The experiment stops when the probability that one group’s mean exceeds the other reaches `prob_stop`, or when the sample size hits `n_samp_max`. If `n_samp_step` is small, the proportion of correctly identified groups may fall short of `prob_stop` due to model inaccuracies or outliers. With a sufficiently large `n_samp_step`, the share of correctly identified groups aligns with the expected `prob_stop`.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P_{\\text{users}}(x) & = \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p P_{\\text{paying}}(x), \\, x > 0\n",
    "\\end{cases}\n",
    "= \n",
    "\\begin{cases}\n",
    "1-p, \\, x = 0\n",
    "\\\\\n",
    "p Lognorm(x | s, \\mu_N, \\sigma_N), \\, x > 0\n",
    "\\end{cases}\n",
    "\\\\\n",
    "E_{\\text{users}}[x] & = p e^{\\mu + s^2/2}\n",
    "\\\\\n",
    "P(p) & = \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s),\n",
    "\\\\\n",
    "P_{\\exp(\\mu + s^2/2)}(y) & = Lognorm(y | \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\\\\n",
    "P_{p\\exp(\\mu + s^2/2)} & \\sim \\mbox{Beta}(p; \\alpha + n_s, \\beta + N - n_s) Lognorm(y ; \\mu_N + s^2/2, \\sigma_N^2)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba57502",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConjugateRevPerUserParams = namedtuple('ConjugateRevPerUserParams', 'a b mu sigma sx')\n",
    "\n",
    "def posterior_params_rev_per_user(data):\n",
    "    d_paying = data[data != 0]\n",
    "    d_paying_total = len(d_paying)\n",
    "    d_total = len(data)\n",
    "    a, b = posterior_params_binom(ns=d_paying_total, ntotal=d_total)\n",
    "    post_pars = initial_params_lognormal(mu=np.log(d_paying)[0], sigma=np.std(np.log(d_paying)), sx=np.std(np.log(d_paying)))\n",
    "    post_pars = posterior_params_lognormal(d_paying[1:], post_pars)\n",
    "    return ConjugateRevPerUserParams(a=a, b=b, mu=post_pars.mu, sigma=post_pars.sigma, sx=post_pars.sx)\n",
    "\n",
    "def posterior_params_binom(ns, ntotal, a_prior=1, b_prior=1):\n",
    "    a = a_prior + ns\n",
    "    b = b_prior + ntotal - ns\n",
    "    return a, b\n",
    "\n",
    "def rev_per_user_p_dist(params):\n",
    "    return stats.beta(a=params.a, b=params.b)\n",
    "\n",
    "def posterior_mean_rev_per_user_rvs(params, nsamples=100_000):\n",
    "    p_dist = rev_per_user_p_dist(params)\n",
    "    ps = p_dist.rvs(size=nsamples)\n",
    "    means_dist = posterior_mean_dist_lognormal(params)\n",
    "    means = means_dist.rvs(nsamples)\n",
    "    return ps * means\n",
    "\n",
    "def exact_rev_per_user_rvs(p, mu, s, nsamples):\n",
    "    conv = stats.bernoulli.rvs(p=p, size=nsamples)\n",
    "    rev = stats.lognorm.rvs(s=s, scale=np.exp(mu), size=nsamples)\n",
    "    return conv * rev\n",
    "\n",
    "def prob_pb_gt_pa_samples(post_samp_A, post_samp_B):\n",
    "    if len(post_samp_A) != len(post_samp_B):\n",
    "        return None\n",
    "    b_gt_a = np.sum(post_samp_B > post_samp_A)\n",
    "    return b_gt_a / len(post_samp_A)\n",
    "\n",
    "nexps = 100\n",
    "prob_stop = 0.95\n",
    "n_samp_max = 3_000_000\n",
    "n_samp_step = 30000\n",
    "n_post_samp = 50000\n",
    "\n",
    "A = {'p': 0.1, 'mu': 2, 's': 1}\n",
    "\n",
    "cmp = pd.DataFrame(columns=['A_pars', 'B_pars', 'A_mean', 'B_mean', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "cmp['A_pars'] = [A] * nexps\n",
    "cmp['B_pars'] = cmp['A_pars'].apply(lambda x: {'p': x['p'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1)), 's': x['s'], 'mu': x['mu'] * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1))})\n",
    "cmp['A_mean'] = cmp['A_pars'].apply(lambda x: x['p'] * stats.lognorm(s=x['s'], scale=np.exp(x['mu'])).mean())\n",
    "cmp['B_mean'] = cmp['B_pars'].apply(lambda x: x['p'] * stats.lognorm(s=x['s'], scale=np.exp(x['mu'])).mean())\n",
    "cmp['best_exact'] = cmp.apply(lambda r: 'B' if r['B_mean'] > r['A_mean'] else 'A', axis=1)\n",
    "\n",
    "for i in range(nexps):\n",
    "    A_pars = cmp.at[i, 'A_pars']\n",
    "    B_pars = cmp.at[i, 'B_pars']\n",
    "    n_samp_total = 0\n",
    "    dA = np.array([])\n",
    "    dB = np.array([])\n",
    "    while n_samp_total < n_samp_max:\n",
    "        dA = np.append(dA, exact_rev_per_user_rvs(p=A_pars['p'], mu=A_pars['mu'], s=A_pars['s'], nsamples=n_samp_step))\n",
    "        dB = np.append(dB, exact_rev_per_user_rvs(p=B_pars['p'], mu=B_pars['mu'], s=B_pars['s'], nsamples=n_samp_step))\n",
    "        n_samp_total += n_samp_step\n",
    "        post_pars_A = posterior_params_rev_per_user(dA)\n",
    "        post_pars_B = posterior_params_rev_per_user(dB)\n",
    "        post_samp_A = posterior_mean_rev_per_user_rvs(post_pars_A)\n",
    "        post_samp_B = posterior_mean_rev_per_user_rvs(post_pars_B)\n",
    "        pb_gt_pa = prob_pb_gt_pa_samples(post_samp_A, post_samp_B)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_samp_A.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_samp_B.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: n_samp {n_samp_total}, best_group {best_gr}, P(b>a) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346681e",
   "metadata": {},
   "source": [
    "# Orders per Visitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c3c5a",
   "metadata": {},
   "source": [
    "A visitor may place several orders or none at all. The distribution of the number of orders per visitor, $P_{\\text{orders}}(n)$, where $n \\in 0, 1, 2, \\dots$ can be modeled as a discrete analogue of the log-normal distribution or as a power-law distribution such as Zipf’s law: $P(n ; s) \\propto n^{-s}$ [[ZipfDist](https://en.wikipedia.org/wiki/Zipf%27s_law#Formal_definition), [SciPyZipfian](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipfian.html), [SciPyZipf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipf.html)]. It is important to model the probabilities of low-order counts accurately. Zipf’s law may not provide sufficient flexibility for this. A more adaptable approach is to model the exact probabilities $p_i$ of making $i$ orders. Assume a maximum number of orders $N$, and let $n_i$ be the number of users with $i$ orders, for $i=0, 1, 2, \\dots, N$. The likelihood is then given by the multinomial distribution: $P(\\mathcal{D} | \\mathcal{H}) = Mult(n_0, \\dots, n_N | p_0, \\dots, p_N)$ [[MultiDist](https://en.wikipedia.org/wiki/Multinomial_distribution), [SciPyMult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html)]. The conjugate prior for this likelihood is the Dirichlet distribution: $P(\\mathcal{H}) = Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right)$ [[DirDist](https://en.wikipedia.org/wiki/Dirichlet_distribution), [SciPyDir](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet.html)]. In the posterior distribution, each parameter is updated as $\\alpha_i + n_i$. The marginal distributions of each $p_i$ are Beta distributions, consistent with interpreting $p_i$ as the conversion rate to exactly $i$ orders.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(\\mathcal{D} | \\mathcal{H}) & = Mult(n_0, \\dots, n_N | p_0, \\dots, p_N) = \\frac{(n_0 + \\dots + n_N)!}{n_{0}! \\dots n_{N}!} p_{0}^{n_{0}} \\dots p_{N}^{n_{N}} \n",
    "\\\\\n",
    "P(\\mathcal{H}) & = \n",
    "Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right) = \n",
    "\\dfrac{1}{B( \\alpha_{0}, \\dots, \\alpha_{N} )} \\prod_{i=0}^{N} p_{i}^{\\alpha_{i}-1},\n",
    "\\qquad\n",
    "\\sum_{i=0}^{N} p_i = 1,\n",
    "\\qquad\n",
    "p_i \\in [0, 1], \n",
    "\\qquad\n",
    "B(\\alpha_{0}, \\dots, \\alpha_{N}) = \n",
    "\\frac{\\prod \\limits_{i=0}^{N} \\Gamma( \\alpha_{i} )}\n",
    "{\\Gamma \\left( \\sum \\limits_{i=0}^{N} \\alpha_{i} \\right)}\n",
    "\\\\\n",
    "P(\\mathcal{H} | \\mathcal{D}) \n",
    "& \\propto Mult(n_0, \\dots, n_N | p_0, \\dots, p_N) Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0}, \\dots, \\alpha_{N} \\right)\n",
    "\\\\\n",
    "& \\propto\n",
    "p_{0}^{n_{0}} \\dots p_{N}^{n_{N}} \n",
    "\\prod _{i=0}^{N} p_{i}^{\\alpha_{i}-1}\n",
    "\\\\\n",
    "& \\propto\n",
    "\\prod_{i=0}^{N} p_{i}^{n_{i} + \\alpha_{i} - 1}\n",
    "\\\\\n",
    "& =\n",
    "Dir \\left( p_{0}, \\dots, p_{N}; \\alpha_{0} + n_0, \\dots, \\alpha_{N} + n_N \\right)\n",
    "\\\\\n",
    "P(p_i | \\mathcal{D} ) & = \n",
    "\\int dp_0 \\dots dp_{i-1}dp_{i+i} \\dots dp_N P(\\mathcal{H} | \\mathcal{D}) \n",
    "=\n",
    "Beta( p_i; \\alpha_i + n_i, \\sum_{k=0}^{N} (\\alpha_k + n_k) - \\alpha_i - n_i )\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73e2ef",
   "metadata": {},
   "source": [
    "To illustrate parameter estimation, we define a Zipf distribution with parameters `s` and `Nmax`. A sample is drawn from this distribution, and the posterior distribution is then constructed based on the sample. In addition, the conversion probabilities for exactly $i$ orders, $p_i$, are computed. The plot shows the original distribution, the observed sample, the posterior predictive distribution $x$, the estimated values of $p_i$, and their 95\\% credible intervals. For most values of $i$, the true probabilities fall within the corresponding credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5714d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_params_dir(N):\n",
    "    return np.ones(N)\n",
    "\n",
    "def posterior_params_dir(data, initial_pars):\n",
    "    u, c = np.unique(data, return_counts=True)\n",
    "    post_pars = np.copy(initial_pars)\n",
    "    for k, v in zip(u, c):\n",
    "        post_pars[k] = post_pars[k] + v\n",
    "    return post_pars\n",
    "\n",
    "def posterior_dist_dir(params):\n",
    "    return stats.dirichlet(alpha=params)\n",
    "\n",
    "def posterior_nords_dir_rvs(params, nsamp):\n",
    "    nords = np.empty(nsamp)\n",
    "    d = posterior_dist_dir(params)\n",
    "    probs = d.rvs(size=nsamp)\n",
    "    for i, p in enumerate(probs):\n",
    "        nords[i] = np.argmax(stats.multinomial.rvs(n=1, p=p))\n",
    "    return nords\n",
    "\n",
    "def marginal_pi_dist_dir(i, params):\n",
    "    return stats.beta(a=params[i], b=np.sum(params) - params[i])\n",
    "\n",
    "def posterior_pi_mean_95pdi(i, params):\n",
    "    p = marginal_pi_dist_dir(i, params)\n",
    "    m = p.mean()\n",
    "    lower = p.ppf(0.025)\n",
    "    upper = p.ppf(0.975)\n",
    "    return m, lower, upper\n",
    "\n",
    "Nmax = 30\n",
    "s = 1.5\n",
    "nsample = 1000\n",
    "\n",
    "Npars = Nmax + 1\n",
    "exact_dist = stats.zipfian(a=s, n=Npars, loc=-1)\n",
    "data = exact_dist.rvs(nsample)\n",
    "pars = initial_params_dir(Npars)\n",
    "pars = posterior_params_dir(data, pars)\n",
    "post_samp = posterior_nords_dir_rvs(pars, 100000)\n",
    "pi = [posterior_pi_mean_95pdi(i, pars) for i in range(Npars)]\n",
    "\n",
    "x = np.arange(0, Npars+1)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=exact_dist.pmf(x), name='Exact Zipf Distribution', \n",
    "                         line_color='black'))\n",
    "fig.add_trace(go.Histogram(x=data, histnorm='probability', name='Sample', nbinsx=round(Nmax*2),\n",
    "                         marker_color='black'))\n",
    "fig.add_trace(go.Histogram(x=post_samp, histnorm='probability', name='$\\mbox{Posterior } n_i$', \n",
    "                         marker_color='black', opacity=0.2, nbinsx=round(Nmax*2)))\n",
    "fig.add_trace(go.Scatter(x=x, \n",
    "                         y=[p[0] for p in pi],\n",
    "                         error_y=dict(type='data', symmetric=False, array=[p[2] - p[0] for p in pi], arrayminus=[p[0] - p[1] for p in pi]), \n",
    "                         name='$p_i \\mbox{ estimates}$',\n",
    "                         mode='markers',\n",
    "                         line_color='red',\n",
    "                         opacity=0.8))\n",
    "fig.update_layout(title='Orders per Visitor',\n",
    "                  xaxis_title='$Orders$',\n",
    "                  yaxis_title='Probability',\n",
    "                  xaxis_range=[-1, Nmax+1],\n",
    "                  hovermode=\"x\",\n",
    "                  barmode=\"group\",\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch6_postdist.png\", scale=2)\n",
    "#Distribution of Orders per Visitor. The exact conversion rates into i orders fall within the estimated intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8502fe",
   "metadata": {},
   "source": [
    "The distribution of order counts allows us to estimate the average number of orders $E[n] = \\sum_{i=0}^N i, p_i$, the conversion to at least one order $1 - p_0$, and the conversion to two or more orders $1 - p_0 - p_1$. Below is an example comparing the expected number of orders $E[n]$. Two Zipf distributions are defined, where group B has a shape parameter `s` that is 5\\% smaller than group A. The first chart shows the true distributions, exact means, and estimated $p_i$ values. For most values of $i$, the true $p_i$ falls within the estimated intervals. The second chart presents the posterior distribution of the average number of orders. Given the selected parameters, the probability that the mean of group B exceeds that of group A is $P(E[n]_B > E[n]_A) = 90\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_nords_mean_rvs(params, nsample):\n",
    "    ns = np.arange(len(params))\n",
    "    probs = stats.dirichlet.rvs(alpha=params, size=nsample)\n",
    "    means = np.sum(ns * probs, axis=1)\n",
    "    return means\n",
    "\n",
    "def prob_pb_gt_pa_samples(post_samp_A, post_samp_B):\n",
    "    if len(post_samp_A) != len(post_samp_B):\n",
    "        return None\n",
    "    b_gt_a = np.sum(post_samp_B > post_samp_A)\n",
    "    return b_gt_a / len(post_samp_A)\n",
    "\n",
    "nsample = 3000\n",
    "Nmax = 30\n",
    "Npars = Nmax + 1\n",
    "\n",
    "post_samp_len = 100000\n",
    "A, B = {}, {}\n",
    "s = 1.5\n",
    "A['dist_pars'] = {'s': s}\n",
    "B['dist_pars'] = {'s': s * 0.95}\n",
    "for g in [A, B]:\n",
    "    g['exact_dist'] = stats.zipfian(a=g['dist_pars']['s'], n=Npars, loc=-1)\n",
    "    g['data'] = g['exact_dist'].rvs(nsample)\n",
    "    g['post_pars'] = initial_params_dir(Npars)\n",
    "    g['post_pars'] = posterior_params_dir(g['data'], g['post_pars'])\n",
    "    g['post_nords'] = posterior_nords_dir_rvs(g['post_pars'], post_samp_len)\n",
    "    g['post_means'] = posterior_nords_mean_rvs(g['post_pars'], post_samp_len)\n",
    "    g['pi'] = [posterior_pi_mean_95pdi(i, g['post_pars']) for i in range(Npars)]\n",
    "\n",
    "x = np.arange(0, Npars)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x, y=A['exact_dist'].pmf(x), name='Exact distribution A',\n",
    "                        marker_color='black', opacity=0.2))\n",
    "fig.add_trace(go.Bar(x=x, y=B['exact_dist'].pmf(x), name='Exact distribution B',\n",
    "                        marker_color='black', opacity=0.8))\n",
    "fig.add_trace(go.Scatter(x=[A['exact_dist'].mean(), A['exact_dist'].mean()], \n",
    "                         y=[0, np.max(A['exact_dist'].pmf(x))*1.1],\n",
    "                         name='Exact mean A', \n",
    "                         mode='lines', line_dash='dash',\n",
    "                         line_color='black', opacity=0.3))\n",
    "fig.add_trace(go.Scatter(x=[B['exact_dist'].mean(), B['exact_dist'].mean()], \n",
    "                         y=[0, np.max(B['exact_dist'].pmf(x))*1.1],\n",
    "                         name='Exact mean B', \n",
    "                         mode='lines', line_dash='dash',\n",
    "                         line_color='black'))\n",
    "fig.add_trace(go.Scatter(x=x - 0.1, \n",
    "                         y=[p[0] for p in A['pi']],\n",
    "                         error_y=dict(type='data', symmetric=False, array=[p[2] - p[0] for p in A['pi']], arrayminus=[p[0] - p[1] for p in A['pi']]), \n",
    "                         name='$p_i, \\mbox{ A}$',\n",
    "                         line_color='black', opacity=0.3,\n",
    "                         mode='markers'\n",
    "                    ))\n",
    "fig.add_trace(go.Scatter(x=x + 0.1, \n",
    "                         y=[p[0] for p in B['pi']],\n",
    "                         error_y=dict(type='data', symmetric=False, array=[p[2] - p[0] for p in B['pi']], arrayminus=[p[0] - p[1] for p in B['pi']]), \n",
    "                         name='$p_i, \\mbox{ B}$',\n",
    "                         line_color='black',\n",
    "                         mode='markers'))\n",
    "fig.update_layout(title='Orders per User',\n",
    "                  xaxis_title='Orders',\n",
    "                  yaxis_title='Probability',\n",
    "                  xaxis_range=[-1, Npars+1-20],\n",
    "                  hovermode=\"x\",\n",
    "                  barmode=\"group\",\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch6_cmp_orig.png\", scale=2)\n",
    "#Exact order distributions, mean number of orders, and conversion estimates.\n",
    "\n",
    "x = np.arange(0, Npars)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[A['exact_dist'].mean(), A['exact_dist'].mean()], \n",
    "                         y=[0, np.max(A['exact_dist'].pmf(x))*1.1],\n",
    "                         name='Exact mean A', \n",
    "                         mode='lines', line_dash='dash',\n",
    "                         line_color='black', opacity=0.3))\n",
    "fig.add_trace(go.Scatter(x=[B['exact_dist'].mean(), B['exact_dist'].mean()], \n",
    "                         y=[0, np.max(B['exact_dist'].pmf(x))*1.1],\n",
    "                         name='Exact mean B', \n",
    "                         mode='lines', line_dash='dash',\n",
    "                         line_color='black'))\n",
    "fig.add_trace(go.Histogram(x=A['post_means'], histnorm='probability', name='$E[n], \\mbox{ A}$', \n",
    "                           marker_color='black', opacity=0.3, nbinsx=round(Nmax*2)))\n",
    "fig.add_trace(go.Histogram(x=B['post_means'], histnorm='probability', name='$E[n], \\mbox{ B}$', \n",
    "                           marker_color='black', nbinsx=round(Nmax*2)))\n",
    "fig.update_layout(title='Average Order Number',\n",
    "                  xaxis_title='Orders',\n",
    "                  yaxis_title='Probability',\n",
    "                  xaxis_range=[-1, Npars+1-20],\n",
    "                  hovermode=\"x\",\n",
    "                  barmode=\"group\",\n",
    "                  height=550)\n",
    "fig.show()\n",
    "#fig.write_image(\"./figs/en_ch6_cmp_means.png\", scale=2)\n",
    "#Estimated mean number of orders. Group B exceeds Group A with 90% probability.\n",
    "\n",
    "print(f\"P(E[n]_B > E[n]_A): {prob_pb_gt_pa_samples(A['post_means'], B['post_means'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72371d",
   "metadata": {},
   "source": [
    "The number of correctly identified \"better\" groups is tested across `nexps` experiments. In Group A, the number of orders per user follows a Zipf distribution with parameter `s`, while in Group B the parameter varies within $\\pm 5\\%$ of A. Groups are compared based on their mean number of orders. In each experiment, samples are added incrementally by `n_samp_step`, posterior parameters are updated, and the probability $P(E[n]_B > E[n]_A)$ is computed. The experiment stops when the probability that one group’s mean exceeds the other’s reaches `prob_stop`, or when the maximum number of samples `n_samp_max` is reached. The proportion of correctly identified groups, 0.94, is close to the expected threshold `prob_stop = 0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.DataFrame(columns=['A', 'B', 'best_exact', 'exp_samp_size', 'A_exp', 'B_exp', 'best_exp', 'p_best'])\n",
    "\n",
    "s = 1.5\n",
    "Nmax = 30\n",
    "Npars = Nmax + 1\n",
    "nexps = 100\n",
    "cmp['A'] = [s] * nexps\n",
    "cmp['B'] = s * (1 + stats.uniform.rvs(loc=-0.05, scale=0.1, size=nexps))\n",
    "\n",
    "n_samp_max = 200000\n",
    "n_samp_step = 5000\n",
    "\n",
    "prob_stop = 0.95\n",
    "for i in range(nexps):\n",
    "    s_a = cmp.at[i, 'A']\n",
    "    s_b = cmp.at[i, 'B']\n",
    "    exact_dist_a = stats.zipfian(a=s_a, n=Npars, loc=-1)\n",
    "    exact_dist_b = stats.zipfian(a=s_b, n=Npars, loc=-1)\n",
    "    cmp.at[i, 'best_exact'] = 'A' if exact_dist_a.mean() > exact_dist_b.mean() else 'B'\n",
    "    n_samp_total = 0\n",
    "    pars_a = initial_params_dir(Npars)\n",
    "    pars_b = initial_params_dir(Npars)\n",
    "    while n_samp_total < n_samp_max:\n",
    "        data_a = exact_dist_a.rvs(n_samp_step)\n",
    "        data_b = exact_dist_b.rvs(n_samp_step)\n",
    "        n_samp_total += n_samp_step\n",
    "        pars_a = posterior_params_dir(data_a, pars_a)\n",
    "        pars_b = posterior_params_dir(data_b, pars_b)\n",
    "        post_samp_len = 10000\n",
    "        post_means_a = posterior_nords_mean_rvs(pars_a, post_samp_len)\n",
    "        post_means_b = posterior_nords_mean_rvs(pars_b, post_samp_len)\n",
    "        pb_gt_pa = prob_pb_gt_pa_samples(post_means_a, post_means_b)\n",
    "        best_gr = 'B' if pb_gt_pa >= prob_stop else 'A' if (1 - pb_gt_pa) >= prob_stop else None\n",
    "        if best_gr:\n",
    "            cmp.at[i, 'A_exp'] = post_means_a.mean()\n",
    "            cmp.at[i, 'B_exp'] = post_means_b.mean()\n",
    "            cmp.at[i, 'exp_samp_size'] = n_samp_total\n",
    "            cmp.at[i, 'best_exp'] = best_gr\n",
    "            cmp.at[i, 'p_best'] = pb_gt_pa\n",
    "            break\n",
    "    print(f'done {i}: nsamp {n_samp_total}, best_gr {best_gr}, P(B>A) {pb_gt_pa}')\n",
    "\n",
    "cmp['correct'] = cmp['best_exact'] == cmp['best_exp']\n",
    "display(cmp.head(10))\n",
    "cor_guess = np.sum(cmp['correct'])\n",
    "print(f\"Nexp: {nexps}, Correct Guesses: {cor_guess}, Accuracy: {cor_guess / nexps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03324297",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed684a7d",
   "metadata": {},
   "source": [
    "Bayesian modeling has been applied to compare conversions, means using the central limit theorem, revenue per user, and orders per visitor. For each metric, a model distribution was proposed. The model parameters are defined using conjugate prior distributions, enabling the analytical construction of posterior distributions. The process demonstrated parameter estimation from a sample, comparison of two groups, and the evaluation of the proportion of correctly identified \"better\" groups in a series of experiments. The proportion aligns with expectations. Model validation was not discussed, but in practice, it is crucial to assess the model's applicability to specific situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bfd22",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114b95",
   "metadata": {},
   "source": [
    "[Apx] - [Bayesian A/B-Testing, Appendices](https://github.com/andrewbrdk/Bayesian-AB-Testing/blob/main/appendices), *GitHub*.  \n",
    "[BaseFal] - [Base Rate Fallacy](https://en.wikipedia.org/wiki/Base_rate_fallacy), *Wikipedia.*  \n",
    "[BernProc] - [Bernoulli Process](https://en.wikipedia.org/wiki/Bernoulli_process), *Wikipedia.*  \n",
    "[BerryEsseenTheorem] - [Berry-Esseen Theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem), *Wikipedia.*   \n",
    "[BetaDist] - [Beta Distribution](https://en.wikipedia.org/wiki/Beta_distribution), *Wikipedia.*     \n",
    "[BinomDist] - [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution), *Wikipedia.*  \n",
    "[CausalDAG] - [Causal Graph](https://en.wikipedia.org/wiki/Causal_graph), *Wikipedia.*    \n",
    "[CDF] - [Cumulative Distribution Function](https://en.wikipedia.org/wiki/Cumulative_distribution_function), *Wikipedia.*  \n",
    "[CLT] - [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), *Wikipedia.*    \n",
    "[ConjPrior] - [Conjugate Prior](https://en.wikipedia.org/wiki/Conjugate_prior), *Wikipedia.*   \n",
    "[DirDist] - [Dirichlet Distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution), *Wikipedia.*    \n",
    "[GammaDist] - [Gamma Distribution](https://en.wikipedia.org/wiki/Gamma_distribution), *Wikipedia.*     \n",
    "[LognormDist] - [Log-normal Distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), *Wikipedia.*    \n",
    "[LomaxDist] - [Lomax Distribution](https://en.wikipedia.org/wiki/Lomax_distribution), *Wikipedia.*     \n",
    "//[MicroExp] - R. Kohavi et al, [Online Experimentation at Microsoft](https://www.microsoft.com/en-us/research/publication/online-experimentation-at-microsoft/).  \n",
    "[MultiDist] - [Multinomial Distribution](https://en.wikipedia.org/wiki/Multinomial_distribution), *Wikipedia.*    \n",
    "[NormDist] - [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), *Wikipedia.*  \n",
    "[NormSum] - [Sum of Normally Distributed Random Variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables), *Wikipedia.*  \n",
    "[ParetoDist] - [Pareto Distribution](https://en.wikipedia.org/wiki/Pareto_distribution), *Wikipedia.*     \n",
    "[ProbConv] - [Convolution of Probability Distributions](https://en.wikipedia.org/wiki/Convolution_of_probability_distributions), *Wikipedia.*    \n",
    "[RandVarsConv] - [Convergence of Random Variables](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution), *Wikipedia.*   \n",
    "[SGBS] - B. Lambert, A Student’s Guide to Bayesian Statistics ([Textbook](https://www.amazon.co.uk/Students-Guide-Bayesian-Statistics/dp/1473916364), [Student Resources](https://study.sagepub.com/lambert)).     \n",
    "[SR] - R. McElreath, Statistical Rethinking: A Bayesian Course with Examples in R and STAN ([Textbook](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919), [Video Lectures](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus), [Course Materials](https://github.com/rmcelreath/stat_rethinking_2024)).   \n",
    "[SciPyBern] - [scipy.stats.bernoulli](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html), *SciPy Reference.*  \n",
    "[SciPyBeta] - [scipy.stats.beta](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html), *SciPy Reference.*   \n",
    "[SciPyBinom] - [scipy.stats.binom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html), *SciPy Reference.*   \n",
    "[SciPyDir] - [scipy.stats.dirichlet](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet.html), *SciPy Reference.*  \n",
    "[SciPyGamma] - [scipy.stats.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html), *SciPy Reference.*     \n",
    "[SciPyLognorm] - [scipy.stats.lognorm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html), *SciPy Reference.*      \n",
    "[SciPyLomax] - [scipy.stats.lomax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lomax.html), *SciPy Reference.*       \n",
    "[SciPyMult] - [scipy.stats.multinomial](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html), *SciPy Reference.*   \n",
    "[SciPyNorm] - [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), *SciPy Reference.*   \n",
    "[SciPyPareto] - [scipy.stats.pareto](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html), *SciPy Reference.*    \n",
    "[SciPyZipf] - [scipy.stats.zipf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipf.html), *SciPy Reference.*   \n",
    "[SciPyZipfian] - [scipy.stats.zipfian](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipfian.html), *SciPy Reference.*    \n",
    "[SubjProb] - [Probability Interpretations](https://en.wikipedia.org/wiki/Probability_interpretations#Subjectivism), *Wikipedia.*    \n",
    "[ZipfDist] - [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law#Formal_definition), *Wikipedia.*   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
